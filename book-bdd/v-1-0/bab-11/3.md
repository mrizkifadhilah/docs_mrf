# ğŸ“˜ SUB-BAB 11.3: MENJAMIN KEANDALAN: EXACTLY-ONCE PROCESSING

## ğŸ¯ Tujuan Pembelajaran
Setelah mempelajari sub-bab ini, kamu akan mampu:
1.  Menjelaskan tantangan **At-Least-Once Delivery** pada Kafka dan risiko pemrosesan ganda (*duplicate processing*).
2.  Menerapkan konsep **Idempotensi** dalam desain *Consumer* untuk menangani pesan duplikat dengan aman.
3.  Menggunakan **Unique Index** pada MongoDB sebagai mekanisme deduplikasi event yang efisien.

---

## ğŸ”— Context & Hook
Bayangkan kamu mentransfer uang Rp 1 Juta ke temanmu. Sinyal HP jelek. Aplikasi *loading* lama, lalu muncul "Error". Kamu tekan tombol "Kirim" lagi. Tiba-tiba muncul notifikasi: "Transfer Berhasil" **dua kali**. Saldomu terpotong Rp 2 Juta!

Ini adalah mimpi buruk dalam sistem terdistribusi. Apache Kafka secara *default* menjamin **At-Least-Once Delivery** (Setidaknya Sekali Sampai). Artinya, pesan *pasti* sampai, tapi **bisa jadi sampai dua kali** jika ada gangguan jaringan dan pengirim melakukan *retry*.

Tugasmu sebagai *engineer* adalah mengubah garansi "Setidaknya Sekali" menjadi **Exactly-Once** (Tepat Satu Kali). Bagaimana caranya agar tombol ditekan 100 kali pun, saldo hanya terpotong 1 kali? Jawabannya adalah **Idempotensi**.

---

## ğŸ’¡ Analogi: Cap Tangan di Konser
Bagaimana panitia konser membedakan orang yang baru datang dengan orang yang sudah masuk lalu keluar sebentar?
* **Tiket (Event):** Bisa dicopy atau dipalsukan.
* **Cap Tangan (Idempotency Token):** Saat pertama kali masuk, tanganmu dicap.
* Jika kamu mencoba masuk lagi, panitia melihat cap di tanganmu. "Oh, orang ini sudah masuk sebelumnya." Mereka membiarkanmu lewat tapi **tidak menghitungmu sebagai pengunjung baru**.
* Operasi mengecek cap ini bersifat **Idempotent**: Dicek 1 kali atau 1000 kali, statusmu tetap "Sudah Masuk".

---

## ğŸ“š Inti Materi

### 1. Masalah: At-Least-Once Delivery
Dalam sistem terdistribusi seperti Kafka, jika *Consumer* berhasil memproses pesan tapi gagal melapor (commit offset) ke Broker karena jaringan putus, Broker akan menganggap pesan belum terkirim. Akibatnya, Broker mengirim ulang pesan yang sama. Jika logikamu tidak siap, data akan terduplikasi (misal: saldo terpotong dua kali).

### 2. Solusi: Idempotensi (Idempotency)
Idempotensi adalah sifat operasi di mana menjalankannya berulang kali akan menghasilkan hasil yang sama seperti menjalankannya satu kali.
* **Operasi Non-Idempotent:** `saldo = saldo - 100` (Kalau dijalankan 2x, saldo berkurang 200).
* **Operasi Idempotent:** `saldo = 500` (Dijalankan 100x pun saldo tetap 500).

### 3. Implementasi dengan MongoDB: Unique Index
Bagaimana menerapkan idempotensi pada *Consumer* Kafka yang menyimpan data ke MongoDB?
1.  **Event ID:** Setiap event dari Kafka harus memiliki ID Unik (misal: UUID dari *Publisher*).
2.  **Deduplikasi:** *Consumer* menyimpan ID event yang sudah diproses ke dalam koleksi khusus atau field di dokumen tujuan.
3.  **Unique Index:** Manfaatkan fitur **Unique Index** pada MongoDB (Bab 5) pada field `event_id`.
    * Jika *Consumer* mencoba memproses pesan duplikat (Event ID sama), MongoDB akan menolak operasi *insert* tersebut dengan error `Duplicate Key`.
    * Aplikasi menangkap error tersebut dan mengabaikannya (karena berarti pesan sudah pernah diproses).

### 4. Transactional Consumer (Lanjutan)
Untuk jaminan yang lebih ketat, *Consumer* dapat menggunakan transaksi. Pembacaan offset dari Kafka dan penulisan *state* ke MongoDB dikoordinasikan dalam satu transaksi khusus untuk menjaga integritas *state* layanan.

---

## ğŸ“± Contoh Penerapan: Payment Service
1.  **Kafka:** Mengirim event `PaymentProcessed` dengan ID: `TX-999`.
2.  **Consumer:** Menerima pesan `TX-999`.
3.  **Logika:**
    * Coba Insert ke koleksi `processed_payments`: `{ _id: "TX-999", status: "DONE" }`.
    * **Skenario A (Baru):** Insert sukses. Lanjut update saldo user.
    * **Skenario B (Duplikat):** Insert gagal (Error `E11000 duplicate key`). Consumer tahu ini duplikat, langsung *stop* proses tanpa potong saldo lagi.
4.  **Hasil:** Saldo aman terjaga.

---

## ğŸ“– Mini-Glossary
* **At-Least-Once Delivery:** Jaminan bahwa pesan pasti sampai ke penerima, namun duplikasi mungkin terjadi.
* **Exactly-Once Processing:** Jaminan bahwa pesan diproses tepat satu kali, tidak kurang dan tidak lebih.
* **Idempotensi:** Kemampuan sistem untuk menangani operasi yang sama berulang kali tanpa mengubah hasil akhir.
* **Deduplikasi:** Proses mendeteksi dan membuang data duplikat.

---

## ğŸ“ Evaluasi Singkat

### 5 Soal Pilihan Ganda (HOTS)

1.  **Analisis:** Mengapa mengandalkan konfigurasi Kafka saja seringkali tidak cukup untuk menjamin *Exactly-Once* pada sisi aplikasi (database)?
    * a. Karena Kafka sering down.
* b. Karena Kafka hanya menjamin pengiriman; jika aplikasi *crash* setelah memproses data tapi sebelum memberi tahu Kafka (*commit offset*), Kafka akan mengirim ulang pesan tersebut (At-Least-Once).
    * c. Karena MongoDB tidak kompatibel dengan Kafka.
    * d. Karena jaringan internet lambat.
    * e. Karena disk penuh.

2.  **Konsep:** Apa peran **Unique Index** MongoDB dalam strategi Idempotensi pada *Consumer* Kafka?
    * a. Mempercepat pembacaan data.
* b. Menjadi "penjaga gawang" yang secara otomatis menolak operasi tulis (insert) jika mendeteksi *Event ID* yang sama sudah pernah disimpan sebelumnya.
    * c. Mengenkripsi data transaksi.
    * d. Menghapus data lama.
    * e. Mengurutkan pesan berdasarkan waktu.

3.  **Logika:** Manakah operasi di bawah ini yang bersifat **Idempotent**?
    * a. `UPDATE users SET points = points + 10`
    * b. `INSERT INTO logs (message) VALUES ('Error')` (tanpa ID unik)
    * c. `UPDATE users SET status = 'ACTIVE' WHERE id = 1`
    * d. `DELETE FROM orders` (tanpa where clause yang spesifik limit)
    * e. `http.post('/api/pay')`

4.  **Skenario:** Sebuah *Consumer* menerima pesan duplikat dan mencoba menyimpannya ke MongoDB. MongoDB melempar error `Duplicate Key`. Apa yang harus dilakukan kode aplikasi *Consumer*?
    * a. *Crash* dan restart.
    * b. Mencoba insert lagi terus menerus.
    * c. Menangkap error tersebut (Catch), menganggap pemrosesan sukses (karena memang sudah pernah sukses sebelumnya), dan melakukan *commit offset* ke Kafka.
    * d. Mengirim email ke admin.
    * e. Menghapus database.

5.  **Arsitektur:** Mengapa penting bagi *Consumer* untuk menyimpan ID Event dari Kafka ke dalam database MongoDB?
    * a. Agar bisa diaudit.
* b. Sebagai mekanisme deduplikasi; ID tersebut menjadi tanda pengenal untuk memeriksa apakah event yang sama datang kembali di masa depan (Idempotensi Check).
    * c. Agar hemat memori.
    * d. Karena Kafka menyuruhnya.
    * e. Agar data terlihat lebih banyak.

### Kunci Jawaban
1.  **b** (Masalah klasik *delivery guarantee* vs *processing guarantee*).
2.  **b** (Memanfaatkan fitur native database untuk menjamin integritas data).
3.  **c** (Mengubah status ke 'ACTIVE' berkali-kali hasilnya tetap 'ACTIVE').
4.  **c** (Sifat *fail-safe*: jika sudah ada, berarti tugas sudah selesai).
5.  **b** (Event ID adalah kunci utama untuk mengenali duplikat).

---

## ğŸš€ Mini Challenge
**Coding Logic (Pseudo-code):**
Tulis logika *Consumer* sederhana di kertas:
1.  Terima pesan `msg`.
2.  Coba: `db.processed_events.insertOne({ _id: msg.id })`.
3.  Jika Sukses: Lakukan `db.users.update(...)` (Logika Bisnis).
4.  Jika Error (Duplicate): Log "Pesan duplikat, abaikan".
5.  Akhir: Commit Offset ke Kafka.

---
*Bab 11 Selesai! Kamu sekarang memiliki sistem saraf yang kebal terhadap duplikasi. Ketik **"Lanjut"** untuk masuk ke **Bagian Penutup (Outro)**.*