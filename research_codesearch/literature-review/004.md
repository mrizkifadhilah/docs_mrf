---
title: Review Paper - ACD-SA untuk Cross-Domain Code Search
description: Rangkuman paper tentang model ACD-SA untuk pencarian kode lintas domain yang adaptif (Information and Software Technology, 2025).
head:
  - - meta
    - name: keywords
      content: Code Search, Cross-Domain, Adaptive Model, Self-Attention, fastText, LSTM
---

# 004 - An adaptive model for cross-domain code search

**Penulis:** **Mengge Fang** ᵃ, **Lie Wang** ᵃ, **Haize Hu** ᵃ,*

**Afiliasi:**
* ᵃ Guangxi Normal University, Department and Organization, school of computer science and engineering/ school of software, guilin, 541001, guangxi, China

**Kronologi:** Received: 24 July 2024 • Revised: 12 February 2025 • Accepted: 19 June 2025 • Available Online: 2 July 2025

<a href="https://www.scimagojr.com/journalsearch.php?q=18732&tip=sid&clean=0" target="_blank"><img src="https://www.scimagojr.com/journal_img.php?id=18732" alt="SCImago Journal & Country Rank" /></a>

| Resume Eksekutif |
| :--- |
| **Publikasi:**<br>• **Jurnal:** Information and Software Technology, Vol 186 (2025)<br>• **Penerbit:** Elsevier<br>• **Topik:** Adaptive Cross-Domain Code Search<br><br>**Masalah & Solusi:**<br>• **Masalah:** Model *deep learning* untuk *code search* memerlukan dataset besar dan waktu pelatihan yang lama. Model ini memiliki **adaptabilitas rendah** dan berkinerja buruk (*sub-optimal*) ketika diterapkan pada dataset baru atau domain spesifik (*Cross-Domain code search*), seringkali menghadapi masalah OOV (*Out of Vocabulary*).<br>• **Solusi:** Mengusulkan **ACD-SA** (*Adaptive Cross-Domain code search model based on Self-Attention*). Model ini mentransfer pola **gramatikal** yang dipelajari dari dataset besar ke domain target yang kecil, dikombinasikan dengan pemodelan *Self-Attention* untuk meningkatkan ekstraksi fitur dan korelasi kata.<br><br>**Contoh Penerapan:**<br>• **Pelatihan (Source Domain):** Dataset besar GitHub digunakan untuk melatih pola gramatikal melalui LSTM dan *Self-Attention*.<br>• **Aplikasi (Target Domain):** Model ACD-SA yang telah dilatih diterapkan langsung pada dataset domain spesifik (misal: Apache Lucene) untuk pencarian, tanpa pelatihan ulang yang ekstensif.<br><br>**Metodologi:**<br>• **Vektorisasi:** Menggunakan **fastText** untuk *word embedding* awal (mempertimbangkan morfologi kata).<br>• **Ekstraksi Fitur:** Menggunakan **Self-Attention** untuk menangkap korelasi kata-ke-kata dan informasi struktur kontekstual dalam vektor.<br>• **Pemodelan Gramatikal:** Matriks pencocokan kata (*Word Matching Matrix*) dibangun dari vektor fitur, yang kemudian diumpankan ke **LSTM** untuk mengekstrak pola gramatikal.<br><br>**Temuan Kunci:**<br>1. **Kinerja Terbaik:** ACD-SA secara konsisten mengungguli semua model *baseline* (CodeHow, DeepCS, BVAE, TOSS, AdaCS) pada semua metrik Hit@k dan MRR.<br>2. **Peningkatan MRR Signifikan:** MRR ACD-SA (**0.735**) menunjukkan peningkatan 18.36% dibandingkan AdaCS (**0.621**), model *cross-domain* sebelumnya.<br>3. **Struktur Kunci:** Ablasi menunjukkan bahwa kombinasi informasi **sintaksis** (dari data pelatihan) dan **semantik** (dari data target) adalah yang paling efektif.<br><br>**Kontribusi Utama:**<br>• Upaya pertama yang memperkenalkan model **Self-Attention** ke dalam penelitian *cross-domain code search*.<br>• Mengatasi keterbatasan adaptabilitas model *deep learning* tradisional untuk tugas *code search* domain spesifik.<br><br>**Dampak:**<br>• **Efisiensi Biaya/Waktu:** Mengurangi kebutuhan untuk mengumpulkan dataset besar dan melatih model dari awal untuk setiap tugas *code search* domain spesifik.<br><br>**Tautan (DOI):** 10.1016/j.infsof.2025.107827 |

## 1. Pendahuluan & Masalah

*Code search* (pencarian kode) adalah arah riset penting dalam rekayasa perangkat lunak, bertujuan meningkatkan efisiensi dan kualitas pengembangan. Model *deep learning* (DL) telah diadopsi luas sejak diperkenalkan oleh DeepCS, karena mampu menjembatani *semantic gap* antara bahasa alami dan kode. Namun, model DL konvensional sangat bergantung pada pelatihan dataset yang besar dan terikat pada domain tertentu (*fixed training dataset*). Ketika model ini digunakan untuk mencari kode pada dataset baru atau domain spesifik (*Cross-Domain*), kinerja seringkali menurun drastis karena masalah *Out of Vocabulary* (OOV) dan kurangnya adaptabilitas. Model *cross-domain* sebelumnya, AdaCS, telah mengidentifikasi bahwa pola gramatikal lebih *cross-domainable* daripada makna leksikal, namun proses ekstraksi gramatikalnya kurang akurat karena mengabaikan korelasi kata-ke-kata dan struktur kontekstual.

::: tip Solusi yang Diusulkan
Paper ini mengusulkan model **Adaptive Cross-Domain code search based on Self-Attention (ACD-SA)**. ACD-SA menggunakan *Self-Attention* untuk karakterisasi vektor yang lebih efektif, mengekstrak informasi gramatikal yang akurat dari dataset sumber (*source*) yang besar, dan menggabungkannya dengan informasi semantik dataset target yang spesifik.
:::

## 2. Metodologi

ACD-SA terdiri dari enam modul utama untuk mencapai pembelajaran dan transfer fitur secara efektif.

### A. Word-Embedding Learning
*   Menggunakan alat **fastText** alih-alih *word2vec*. fastText dipilih karena mampu mengatasi kehilangan informasi morfologi internal kata dengan mempertimbangkan *n-gram* karakter, sehingga memperkaya makna kata.

### B. Feature Extraction (Self-Attention)
*   **Tujuan:** Mengatasi batasan *fastText* yang mengabaikan hubungan intrinsik antar vektor kata dan kelengkapan kueri/fragmen kode.
*   **Mekanisme:** Menggunakan model **Self-Attention** untuk melakukan pembelajaran fitur pada vektor *fastText*. Ini efektif dalam mengekstrak informasi asosiasi kata-ke-kata dan struktur kontekstual, menghasilkan vektor fitur yang lebih akurat ($SV_{Tr-C}$ dan $SV_{Ta-C}$).

### C. Constructing the Word Matching Matrix
*   Matriks pencocokan kata ($m \times n$) dibangun antara vektor kata kueri ($V_q$) dan vektor kata kode ($V_c$).
*   Nilai setiap elemen $S_{i,j}$ dalam matriks adalah **kesamaan kosinus** antara vektor kata kode $i$ dan vektor kata kueri $j$.

$$S_{i,j}=cos(V_{ci},V_{qj})$$

### D. Grammar Learning
*   Vektor hubungan struktural timbal balik ($X$) dari matriks pencocokan kata diumpankan ke jaringan **LSTM** (dua lapisan).
*   **Tujuan:** Melatih dan mengekstrak **pola gramatikal** dari dataset pelatihan (GitHub). Pola gramatikal yang dihasilkan ini kemudian menjadi komponen inti yang *cross-domainable* dan ditransfer ke model domain target.

### E. Cross-Domain Search
*   Model akhir ACD-SA dibentuk dengan menggabungkan **Informasi Gramatikal** yang ditransfer dari data pelatihan dengan **Vektor Matriks Pencocokan Kata** yang dibuat dari data target.
*   Pencarian dilakukan dengan menghitung kesamaan kosinus antara kueri yang dimasukkan dan *Code Vector* yang dimodelkan oleh ACD-SA.

## 3. Detail Pengujian

### Dataset
*   **Dataset Pelatihan:** 77.920 pasang Kueri-Kode (Java) yang diperoleh dari GitHub. 20 sampel negatif acak disiapkan untuk setiap sampel positif.
*   **Dataset Target (Cross-Domain):** Tiga dataset domain spesifik yang lebih kecil: Apache Lucene (7.0.0), Apache POI (4.1.0), dan JFreeChart (1.0.19).

### Benchmark Model
*   **Information Retrieval:** CodeHow.
*   **Deep Learning:** DeepCS, BVAE, TOSS.
*   **Cross-Domain:** AdaCS (model *cross-domain* sebelumnya).

### Metrik Evaluasi
Dua metrik utama digunakan: **Hit@k** dan **MRR**.

*   **Hit@k:** Proporsi kueri yang berhasil menemukan kode yang benar di antara $k$ hasil teratas.

$$Hit @K = \frac{\sum_{q \in Q} (\text{Rank}(c_{q|q}) \le K)}{|Q|}$$

*   **MRR (Mean Reciprocal Ranking):** Rata-rata timbal balik dari peringkat hasil relevan pertama.

$$MRR=\frac{1}{|S|}\sum_{i=1}^{|S|}\frac{1}{rank_{i}}$$

## 4. Hasil Eksperimen

### Perbandingan Kinerja Cross-Domain Code Search

ACD-SA menunjukkan kinerja terbaik di semua metrik dibandingkan model *baseline* pada dataset target (rata-rata dari tiga domain).

| Model | Hit@1 | Hit@2 | Hit@3 | Hit@5 | Hit@10 | MRR |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| CodeHow | 0.377 | 0.472 | 0.509 | 0.567 | 0.698 | 0.479 |
| DeepCS | 0.268 | 0.366 | 0.430 | 0.523 | 0.659 | 0.386 |
| AdaCS | 0.486 | 0.598 | 0.675 | 0.772 | 0.885 | 0.621 |
| **ACD-SA** | **0.523** | **0.689** | **0.754** | **0.867** | **0.993** | **0.735** |

*   ACD-SA melampaui AdaCS (model *cross-domain* terbaik sebelumnya) dengan peningkatan MRR sebesar 18.36%.
*   Kinerja model tradisional (DeepCS, BVAE) sangat rendah, mengonfirmasi masalah adaptabilitas mereka dalam skenario *cross-domain*.

### Analisis Ablasi Struktur Model

Ablasi memverifikasi kontribusi masing-masing komponen:
*   **ACD-SA (Penuh):** Sintaks Pelatihan + Semantik Target.
*   **ACD-1:** Sintaks Pelatihan + Semantik Pelatihan.
*   **ACD-2:** Semantik Pelatihan + Semantik Target (tanpa Gramatikal/Sintaks).
*   **ACD-3:** Hanya Semantik Pelatihan (Model Tradisional).

| Model | MRR |
| :--- | :--- |
| **ACD-SA** | **0.735** |
| ACD-1 | 0.713 |
| ACD-2 | 0.709 |
| ACD-3 | 0.699 |

*   Hasil menunjukkan kombinasi **Sintaks Pelatihan + Semantik Target** (**ACD-SA**) menghasilkan efek terbaik, memvalidasi hipotesis bahwa pola gramatikal dapat ditransfer (sintaks *cross-domainable*), dan semantic dari domain target (data baru) diperlukan untuk kinerja optimal.

## 5. Kesimpulan

ACD-SA berhasil mengatasi masalah adaptabilitas model DL konvensional pada *cross-domain code search* dengan memadukan **fastText**, **Self-Attention** untuk ekstraksi fitur yang ditingkatkan, dan **LSTM** untuk pembelajaran pola gramatikal yang dapat ditransfer. Dengan menggunakan model yang dilatih pada dataset besar hanya sekali dan mengaplikasikannya pada domain-domain spesifik, model ini secara signifikan meningkatkan akurasi dan memecahkan hambatan waktu/biaya pelatihan.

::: info Dampak Praktis
ACD-SA menyediakan solusi yang lebih **praktis** dan **efisien** untuk kebutuhan *code search* domain spesifik di industri. Hal ini mengurangi ketergantungan pada koleksi dataset besar yang berulang dan pelatihan model dari awal untuk setiap domain baru, sekaligus mencapai akurasi pencarian yang superior.
:::