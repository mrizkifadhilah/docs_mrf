---
title: Review Paper - HFEDR Fitur Hierarkis dan Reorganisasi Data untuk Code Search
description: Rangkuman paper tentang ekstraksi fitur hierarkis dan reorganisasi data untuk code search (The Journal of Systems and Software, 2024).
head:
  - - meta
    - name: keywords
      content: AI in SE, Code search, Transformer-based architecture, Hierarchical features, Data reorganization, Contrastive training
---

# 035 - Hierarchical features extraction and data reorganization for code search
Tautan (DOI) [10.1016/j.jss.2023.111896](https://doi.org/10.1016/j.jss.2023.111896)

**Penulis:** **Fan Zhang** $^{a}$*, **Manman Peng** $^{a}$, **Yuanyuan Shen** $^{a}$, **Qiang Wu** $^{a}$

**Afiliasi:**
* $^a$ College of Computer Science and Electronic Engineering, Hunan University, Changsha, 410082, China

**Kronologi:** Received: 2 April 2023 • Revised: 7 September 2023 • Accepted: 2 November 2023 • Available Online: 9 November 2023

<a href="https://www.scimagojr.com/journalsearch.php?q=19309&tip=sid" target="_blank"><img src="https://www.scimagojr.com/journal_img.php?id=19309" alt="SCImago Journal & Country Rank" /></a>

| Resume Eksekutif |
| :--- |
| **Publikasi:**<br>• **Jurnal:** The Journal of Systems and Software 208 (2024) 111896<br>• **Topik:** Meningkatkan kinerja *code search* berbasis arsitektur Transformer dengan memanfaatkan fitur hirarkis model dan menambah jumlah data pelatihan melalui reorganisasi data. <br><br>**Masalah & Solusi:**<br>• **Masalah 1 (Mengabaikan Fitur Hierarkis):** Model *pre-training* berbasis Transformer (seperti GraphCodeBERT) menghasilkan 12 lapisan fitur, di mana lapisan bawah menangkap representasi **tingkat rendah** (fitur dasar) dan lapisan atas menangkap **tingkat tinggi** (fitur kompleks). Namun, pekerjaan yang ada mengabaikan pemanfaatan fitur hierarkis yang kaya ini.<br>• **Masalah 2 (Keterbatasan Data Pelatihan):** Metode yang ada tidak mempertimbangkan peningkatan jumlah data pelatihan secara artifisial selama proses pelatihan, padahal hal ini dapat meningkatkan kinerja model secara signifikan.<br>• **Solusi:** Mengusulkan **HFEDR** (*Hierarchical Features Extraction and Data Reorganization*), sebuah metode novel yang: (1) Mengekstrak **fitur hierarkis** (lapisan rendah dan tinggi dari GraphCodeBERT) untuk representasi semantik multi-pandangan. (2) Melakukan **reorganisasi data** pelatihan asli menjadi pasangan fitur hierarkis-tidak berkorelasi (*hierarchical-uncorrelated*) dan hierarkis-berkorelasi (*hierarchical-correlated*), yang secara efektif menambah data pelatihan. (3) Menggunakan **Contrastive Training** dengan *loss function* **CoSENT** untuk mengoptimalkan parameter model. (4) Menggabungkan **BERT-whitening** pada tahap *retrieval* untuk mengurangi anisotropi dan dimensi vektor. <br><br>**Contoh Penerapan:**<br>• Dievaluasi secara ekstensif pada *dataset* **CodeSearchNet** (Ruby, JavaScript, Go, Python, Java, PHP), **CoSQA** (skenario dunia nyata), dan **AdvTest** (uji generalisasi).<br><br>**Metodologi:**<br>• **Ekstraksi Fitur Hierarkis:** Menggunakan **GraphCodeBERT** sebagai *encoder*. Fitur tingkat rendah diambil dari **3 lapisan pertama** (Layer 1, 2, 3) dan fitur tingkat tinggi dari **3 lapisan terakhir** (Layer 10, 11, 12).<br>• **Reorganisasi Data (2 Tahap):** (1) **Hierarchical-Uncorrelated Feature Pairs (Tahap 1):** Mengatur ulang pasangan fitur positif/negatif asli yang sudah tidak berurutan dan memiliki korelasi rendah. (2) **Hierarchical-Correlated Feature Pairs (Tahap 2):** Menggunakan setiap fitur kueri yang cocok (*matched query feature*) dari Tahap 1, dan memasangkannya dengan semua fitur kode yang *tidak cocok* (*unmatched code features*) dalam lapisan yang sama (seluruh *batch*) untuk membuat pasangan fitur negatif tambahan yang berkorelasi secara hierarkis.<br>• **Contrastive Training:** Menggunakan **CoSENT** (*loss function*) pada fitur gabungan dari semua 6 lapisan (akumulasi *loss*) untuk memastikan kesamaan pasangan positif lebih besar daripada pasangan negatif.<br>$$ \text{loss\_layer} \propto \log \left( 1+\sum_{\langle i,j\rangle \in \text{Pos}, \langle k,l\rangle \in \text{Neg}} e^{\tau (\cos(q_i,c_j)-\cos(q_k,c_l))} \right) $$<br>• **Code Retrieval:** Menerapkan **BERT-whitening** pada fitur hierarkis untuk reduksi dimensi (menjadi 512) dan mengurangi anisotropi, kemudian menghitung skor kecocokan dengan penjumlahan *cosine similarity* di semua lapisan yang dipilih: $score=\sum_{\text{layer}\_i\in L} \cos(Q_{\text{LowFea}_{\text{layer}\_i}}, C_{\text{LowFea}_{\text{layer}\_i}})$.<br><br>**Temuan Kunci:**<br>1. **Kinerja SOTA:** HFEDR mengungguli semua *baseline* SOTA (termasuk GraphCodeBERT, Unixcoder, MoCoCS) pada CodeSearchNet (AVG MRR $\mathbf{0.802}$), CoSQA (MRR $\mathbf{0.752}$), dan AdvTest (MRR $\mathbf{0.445}$), membuktikan efektivitas dan kemampuan generalisasi yang baik.<br>2. **Kontribusi Modul:** Semua komponen HFEDR (Reorganisasi data, Ekstraksi Fitur Hierarkis, BERT-whitening) secara positif meningkatkan kinerja. Modul **Hierarchical-Correlated Pairs** adalah yang paling kritis (penurunan kinerja terbesar jika dihapus: $\text{AVG MRR} \downarrow 0.036$).<br>3. **Model-Free:** Komponen-komponen yang diusulkan terbukti *model-free*, menunjukkan efektivitas ketika diterapkan pada arsitektur *pre-training* lain (misalnya, **CodeBERT**).<br><br>**Kontribusi Utama:**<br>• Pekerjaan pertama yang menyelidiki dan memanfaatkan **fitur hierarkis** dari model Transformer bimodal untuk *code search*.<br>• Mengusulkan metode **HFEDR** dengan strategi **reorganisasi data** dua tahap untuk meningkatkan data pelatihan dan kinerja model.<br>• Mengintegrasikan **BERT-whitening** pada tahap *retrieval* untuk efisiensi ruang dan kinerja yang lebih baik.<br><br>**Dampak:**<br>• HFEDR menetapkan *state-of-the-art* baru dalam *code search*, menyediakan metode yang lebih akurat untuk membantu pengembang meningkatkan efisiensi dengan memanfaatkan fitur dalam model Transformer yang sebelumnya diabaikan dan mengoptimalkan penggunaan data pelatihan. |

## 1. Pendahuluan & Masalah

*Code search* adalah tugas penting dalam rekayasa perangkat lunak (SE) yang bertujuan menemukan *snippet* kode yang relevan berdasarkan kueri bahasa alami (NL), membantu pengembang meningkatkan efisiensi dan memanfaatkan kode berkualitas tinggi. Upaya terbaru telah didominasi oleh model *pre-training* berbasis arsitektur Transformer (seperti CodeBERT dan GraphCodeBERT), yang secara signifikan meningkatkan kinerja dibandingkan metode DL sebelumnya (LSTM, Attention).

Namun, model-model Transformer ini memiliki dua kekurangan utama yang diabaikan oleh pekerjaan yang ada:

1.  **Mengabaikan Fitur Hierarkis:** Arsitektur Transformer menghasilkan fitur berlapis, di mana lapisan yang lebih rendah menangkap representasi **tingkat rendah** (dasar) dan lapisan yang lebih tinggi menangkap representasi **tingkat tinggi** (kompleks). Fitur hierarkis ini, yang intuitif untuk pemahaman semantik, tidak dimanfaatkan secara efektif.
2.  **Keterbatasan Data Pelatihan:** Meskipun model dilatih dengan data dalam jumlah besar, metode yang ada gagal untuk **meningkatkan jumlah data pelatihan** lebih lanjut (misalnya, dengan membangun pasangan sampel tambahan) selama fase pelatihan untuk meningkatkan kinerja model secara lebih baik.

::: tip Solusi yang Diusulkan
Kami mengusulkan **HFEDR** (*Hierarchical Features Extraction and Data Reorganization*), sebuah metode yang mengatasi masalah ini dengan (1) mengekstrak **fitur hierarkis** (tingkat rendah dan tingkat tinggi dari GraphCodeBERT) untuk representasi semantik multi-pandangan, (2) **mereorganisasi data** pelatihan asli menjadi pasangan fitur hierarkis-tidak berkorelasi dan berkorelasi untuk meningkatkan data pelatihan, dan (3) menggunakan **Contrastive Training** dengan *loss function* **CoSENT** untuk optimasi. Selain itu, **BERT-whitening** digunakan pada tahap *retrieval* untuk efisiensi dan kinerja.
:::

## 2. Metodologi

Kerangka kerja HFEDR terdiri dari empat langkah utama: ekstraksi fitur hierarkis, reorganisasi data dalam dua tahap, *contrastive training*, dan *code retrieval* dengan BERT-whitening.

### A. Extracting Hierarchical Features

HFEDR menggunakan **GraphCodeBERT**, model SOTA berbasis Transformer, sebagai *encoder*. Berdasarkan penelitian yang menunjukkan bahwa lapisan rendah menangkap fitur tingkat rendah dan lapisan tinggi menangkap fitur tingkat tinggi, HFEDR memilih **tiga lapisan pertama** (Layer 1, 2, 3) sebagai **fitur tingkat rendah** dan **tiga lapisan terakhir** (Layer 10, 11, 12) sebagai **fitur tingkat tinggi** untuk mencapai representasi semantik multi-pandangan.

### B. Data Reorganization (Two Stages)

Tujuan reorganisasi data adalah untuk menghasilkan lebih banyak data pelatihan dari *batch* asli untuk membangun hubungan semantik yang lebih baik antara kueri dan kode.

1.  **Hierarchical-Uncorrelated Feature Pairs (Tahap 1):** Pada tahap ini, *batch* data pelatihan asli (pasangan positif dan negatif yang dikocok secara acak) di-encode, dan fitur hierarkisnya diekstrak. Pasangan fitur berlapis ini disebut "tidak berkorelasi" karena pasangan positif dan negatif dalam setiap lapisan umumnya memiliki elemen yang kurang tumpang tindih.

2.  **Hierarchical-Correlated Feature Pairs (Tahap 2):** Untuk setiap pasangan fitur positif `<query, code, 1>` dalam setiap lapisan dari Tahap 1, HFEDR membuat pasangan fitur negatif tambahan. Fitur kueri positif tersebut ($query_l$) dipasangkan dengan semua fitur kode yang *tidak cocok* ($code_l$) dalam *batch* yang sama, menciptakan pasangan negatif yang berkorelasi. Strategi ini secara signifikan meningkatkan jumlah data pelatihan.

### C. Contrastive Training

HFEDR menggunakan *loss function* **CoSENT** (Contrastive Sentence) untuk mengoptimalkan parameter model, yang dikenal memiliki konvergensi lebih cepat dan hasil yang lebih baik dalam tugas kesamaan semantik.

CoSENT bertujuan agar kesamaan pasangan fitur positif $\langle i, j\rangle \in \text{Pos}$ lebih besar daripada pasangan negatif $\langle k, l\rangle \in \text{Neg}$, yaitu $\cos(q_i, c_j) > \cos(q_k, c_l)$. *Loss* dihitung untuk setiap lapisan, dan total *loss* diakumulasikan untuk memperbarui parameter:
$$ \text{loss\_all} = \sum_{\text{layer}\in \{1,2,3,10,11,12\}} \text{loss\_layer} $$

### D. Code Retrieval

Pada tahap *retrieval*, **BERT-whitening** diterapkan pada fitur hierarkis untuk mengurangi **anisotropi** (fitur yang dikelompokkan terlalu dekat) dan **dimensi** vektor (menjadi 512), yang menghasilkan kinerja yang lebih baik dan menghemat ruang penyimpanan.

Skor kecocokan dihitung sebagai jumlah *cosine similarity* di seluruh lapisan yang diekstrak:
$$ \text{score} = \sum_{\text{layer}\_i\in L} \cos(Q_{\text{LowFea}_{\text{layer}\_i}}, C_{\text{LowFea}_{\text{layer}\_i}}) $$

## 3. Detail Pengujian

### Dataset
*   **CodeSearchNet:** Digunakan untuk pengujian utama pada enam bahasa: **Ruby, JavaScript, Go, Python, Java, dan PHP**. Tugasnya adalah mengambil jawaban dari 1000 kandidat.
*   **CoSQA:** Digunakan untuk menguji kinerja dalam skenario **dunia nyata**.
*   **AdvTest:** Digunakan untuk menguji **generalisasi** model (dengan mengganti nama fungsi/variabel dengan penanda khusus).

### Baseline
Model SOTA yang digunakan untuk perbandingan: ROBERTa, RoBERTa(code), CodeBERT, **GraphCodeBERT**, CodeT5, SPT-Code, Plbart, **Unixcoder**, MoCoCS, dan SYNCOBERT.

### Metrik Evaluasi
Kinerja diukur menggunakan metrik kuantitatif populer:

*   **Mean Reciprocal Rank (MRR):**
    $$ \text{MRR} = \frac{1}{Q} \sum_{i=1}^{Q} \frac{1}{\text{rank}_i} $$
    Di mana $Q$ adalah jumlah kueri, dan $\text{rank}_i$ adalah posisi kode *snippet* yang benar. MRR yang lebih tinggi menunjukkan kinerja yang lebih baik.

## 4. Hasil Eksperimen

### RQ1: Efektivitas HFEDR

| Model | CoSQA (Python) | AdvTest (Python) | CodeSearchNet (AVG) |
| :--- | :--- | :--- | :--- |
| GraphCodeBERT | 0.684 | 0.352 | 0.774 |
| Unixcoder | 0.701 | 0.413 | 0.783 |
| MoCoCS | - | - | 0.782 |
| **HFEDR** | **0.752** | **0.445** | **0.802** |

*   HFEDR mengungguli semua *baseline* SOTA di ketiga *dataset*, termasuk GraphCodeBERT dan Unixcoder, membuktikan kerangka kerjanya efektif dalam *code search*. Peningkatan pada *dataset* CoSQA dan AdvTest menunjukkan kemampuan **generalisasi yang kuat** dan kinerja yang baik dalam **skenario dunia nyata**.

### RQ2: Kontribusi Komponen Kunci (Studi Ablasi)

Studi ablasi menunjukkan kontribusi setiap komponen HFEDR terhadap kinerja rata-rata (AVG MRR):

| Varian | AVG MRR | Penurunan dari HFEDR | Komponen |
| :--- | :--- | :--- | :--- |
| **HFEDR** | **0.802** | - | Penuh |
| w/o whitening | 0.787 | $\downarrow 0.015$ | Menghapus BERT-whitening |
| w/o Uncor\_Pairs | 0.778 | $\downarrow 0.024$ | Menghapus Reorganisasi Tahap 1 |
| **w/o Cor\_Pairs** | 0.766 | $\downarrow 0.036$ | Menghapus Reorganisasi Tahap 2 |
| w/o Multi\_Lay | 0.782 | $\downarrow 0.020$ | Hanya menggunakan Layer-12 |

*   **Hierarchical-Correlated Pairs (w/o Cor\_Pairs)** adalah komponen paling kritis. Penghapusan Tahap 2 Reorganisasi Data menyebabkan penurunan kinerja rata-rata terbesar ($\downarrow 0.036$), mengindikasikan bahwa penambahan data pelatihan yang cermat selama pelatihan sangat efektif.
*   **Hierarchical Features (w/o Multi\_Lay)** juga penting ($\downarrow 0.020$), memvalidasi perlunya mengekstrak fitur dari berbagai lapisan Transformer.
*   Komponen **BERT-whitening** dan **Hierarchical-Uncorrelated Pairs** juga memberikan kontribusi positif.

### RQ6: Komponen *Model-Free*

HFEDR (CodeBERT) mengungguli *baseline* CodeBERT asli ($\text{AVG MRR } \mathbf{0.785}$ vs $0.760$) dan semua varian ablasinya. Ini membuktikan bahwa komponen yang diusulkan (ekstraksi fitur hierarkis, reorganisasi data, dan BERT-whitening) **bersifat *model-free*** dan dapat meningkatkan kinerja pada arsitektur *pre-training* berbasis Transformer lainnya.

## 5. Kesimpulan

Paper ini mengusulkan **HFEDR**, sebuah solusi novel yang meningkatkan kinerja *code search* dengan mengatasi keterbatasan model Transformer yang ada, yaitu dengan memanfaatkan fitur hierarkis model dan mereorganisasi data pelatihan secara artifisial.

Secara spesifik, HFEDR mengekstrak fitur tingkat rendah dan tinggi dari GraphCodeBERT, mengatur data menjadi pasangan fitur hierarkis-tidak berkorelasi dan berkorelasi, dan menggunakan *contrastive training* (CoSENT) untuk optimasi. **BERT-whitening** diterapkan pada tahap *retrieval* untuk efisiensi.

Hasil eksperimen yang ekstensif menunjukkan bahwa HFEDR mencapai kinerja *state-of-the-art* pada CodeSearchNet, CoSQA, dan AdvTest. Kontribusi terbesar berasal dari modul reorganisasi data berkorelasi, yang secara efektif meningkatkan jumlah data pelatihan. Komponen HFEDR juga terbukti *model-free* dan dapat diterapkan pada model *pre-training* lainnya.

::: info Dampak Praktis
HFEDR menetapkan standar baru untuk *code search* berbasis Transformer dengan menunjukkan pentingnya fitur berlapis dan teknik augmentasi data selama pelatihan. Metode ini memiliki dampak praktis langsung dalam meningkatkan akurasi *code search*, sehingga sangat membantu para pengembang. Pekerjaan di masa depan akan berfokus pada pengurangan konsumsi waktu pencarian dan penggabungan pengetahuan eksternal seperti *git commit logs*.
:::