---
title: Review Paper - MSARN-CS untuk Peningkatan Akurasi Code Search
description: Rangkuman paper tentang Model Jaringan Residual Self-Attention Multilayer untuk Code Search (Concurrency and Computation Practice and Experience, 2024).
head:
  - - meta
    - name: keywords
      content: code search, self-attention, residual network, deep learning, semantic correlation, MSARN-CS
---

# 053 - Multilayer self-attention residual network for code search
Tautan (DOI) [https://doi.org/10.1002/cpe.7650]

**Penulis:** **Haize Hu** $^{1,2}$, **Jianxun Liu** $^{1,2}$*, **Xiangping Zhang** $^{1,2}$

**Afiliasi:**
* $^{1}$ School of Computer Science and Technology, Hunan University of Science and Technology, Xiangtan, China
* $^{2}$ Hunan Provincial Key Lab for Services Computing and Novel Software Technology, Hunan University of Science and Technology, Xiangtan, China

**Kronologi:** Received: 8 August 2022 • Revised: 6 December 2022 • Accepted: 25 January 2023

<a href="https://www.scimagojr.com/journalsearch.php?q=27871&tip=sid" target="_blank"><img src="https://www.scimagojr.com/journal_img.php?id=27871" alt="SCImago Journal & Country Rank" /></a>

| Resume Eksekutif |
| :--- |
| **Publikasi:**<br>• **Jurnal:** Concurrency and Computation: Practice and Experience 35, 9 (2024)<br>• **Topik:** Meningkatkan akurasi *code search* dengan mengatasi masalah hilangnya integritas kode dan informasi semantik antar-komponen dalam model berbasis *self-attention* sebelumnya.<br><br>**Masalah & Solusi:**<br>• **Masalah 1 (Integritas Kode):** Model *code search* sebelumnya (misalnya, SAN-CS) yang memecah kode menjadi komponen (API, Nama Metode, Tokens) dan hanya menganalisis bobot internal setiap komponen, **mengabaikan hubungan semantik dan integritas kode** secara keseluruhan di antara komponen-komponen tersebut.<br>• **Masalah 2 (Hilangnya Informasi):** Proses pelatihan jaringan saraf, terutama dengan *self-attention*, dapat menyebabkan **hilangnya informasi fitur** dari urutan kode.<br>• **Solusi:** Mengusulkan model **MSARN-CS** (*Multilayer Self-Attention Residual Network for Code Search*). Model ini memperkenalkan: (1) **Lapisan *Joint Embedding Attention* Multilayer** untuk menangkap korelasi semantik antar komponen kode (API, Nama, Tokens) dan (2) **Jaringan Residual** untuk mengkompensasi hilangnya informasi fitur selama proses *self-attention* dan *joint embedding*.<br><br>**Contoh Penerapan:**<br>• Model diuji pada tugas *code search* Java menggunakan dataset besar yang dikumpulkan oleh Gu et al. (18 juta data pelatihan).<br>• MSARN-CS berhasil menghasilkan fragmen kode yang relevan dengan kueri, bahkan pada kata kunci yang menghubungkan komponen kode yang berbeda (misalnya, "add url to classpath"), yang gagal ditangkap oleh *baseline* SAN-CS.<br><br>**Metodologi:**<br>• **Arsitektur Inti:** Berbasis jaringan *self-attention* untuk mengekstrak bobot internal dari setiap komponen kode (API, Nama, Tokens) dan deskripsi (Des).<br>• **Joint Embedding Multilayer:** Menerapkan **perhatian *embedding* berpasangan** (*two-by-two embedding*) antara ketiga komponen kode ($Nama \leftrightarrow API$, $Nama \leftrightarrow Tokens$, $API \leftrightarrow Tokens$) untuk membangun **korelasi semantik dan struktural antar-komponen**. Hasil *embedding* berpasangan ini kemudian disatukan (*superimposed*) menjadi representasi yang lebih kaya: $V_{n}$, $V_{a}$, $V_{t}$.<br>• **Jaringan Residual (Residual Network):**
    *   **Level 1 (Kode):** Menambahkan vektor *embedding* awal ($V_{API}$, $V_{Names}$, $V_{Tokens}$) ke hasil *joint embedding* berpasangan (JAAPI, JANames, JATokens) untuk mengkompensasi hilangnya informasi di awal.
    *   **Level 2 (Kode-Kueri):** Menambahkan vektor kode awal ($V_{code}$) dan vektor deskripsi awal ($V_{Des}$) ke hasil *joint embedding* utamanya, menghasilkan $V'_{c}$ dan $V'_{D}$.<br>• **Optimasi:** Menggunakan *Minimum Ranking Loss Degree* (Triplet Loss) untuk pelatihan. $\mathcal{L}(\theta)=\sum_{(c,d^{+},d^{-})}max(0,\xi-cos(c,d^{+})+cos(c,d^{-}))$.<br><br>**Temuan Kunci:**<br>1. **Kinerja Superior:** MSARN-CS melampaui semua *baseline* (DeepCS, CRLCS, SAN-CS-, dan SAN-CS) di semua metrik. Peningkatan $\mathbf{3.88\%}$ MRR dan $\mathbf{3.30\%}$ NDCG dibandingkan model *self-attention* terbaik (SAN-CS).<br>2. **Dampak Joint Embedding:** *Joint embedding* antara komponen kode secara signifikan meningkatkan akurasi. Penggabungan antara **Token dan Nama Metode** (CS-NT) memiliki dampak terbesar.<br>3. **Kompensasi Residual:** Jaringan Residual memiliki dampak besar pada hasil model. Penghapusan Residual **Kode** menyebabkan penurunan kinerja yang lebih besar dibandingkan Residual Kueri, menekankan pentingnya mempertahankan integritas fitur kode.<br><br>**Kontribusi Utama:**<br>• Mengusulkan model **MSARN-CS**, yang merupakan yang pertama menggabungkan *Multilayer Joint Attention Embedding* dan *Residual Network* dalam tugas *code search*.<br>• Berhasil mengatasi masalah hilangnya integritas kode dengan menangkap korelasi semantik antar komponen kode.<br>• Membuktikan bahwa Jaringan Residual secara efektif mengkompensasi hilangnya informasi fitur selama pelatihan jaringan *self-attention*.<br><br>**Dampak:**<br>• **Peningkatan Akurasi:** MSARN-CS secara signifikan meningkatkan akurasi *code search*, memungkinkan pengembang menemukan *snippet* kode yang paling relevan dengan cepat dan akurat, mengurangi waktu pengembangan. |

## 1. Pendahuluan & Masalah

Para pengembang perangkat lunak menghabiskan sebagian besar waktu mereka (lebih dari 19%) untuk mencari, memodifikasi, dan menggunakan kembali *snippet* kode yang ada dari repositori *open source*. Oleh karena itu, menciptakan alat *code search* yang cepat dan akurat adalah tantangan utama dalam pengembangan perangkat lunak modern.

Awalnya, pencarian kode mengandalkan metode *Information Retrieval* (IR), tetapi metode ini memiliki dua kelemahan fatal: (1) memperlakukan bahasa kode dan bahasa alami secara konsisten, mengabaikan **kesenjangan semantik** di antara keduanya, dan (2) tidak efektif dalam menghilangkan *noise* yang tidak relevan.

Pendekatan *Deep Learning* (DL) yang dimulai dengan DeepCS (Gu et al., 2018) berusaha memecahkan masalah ini dengan memetakan kode dan kueri ke dalam ruang vektor yang sama. Namun, model berbasis *Long Short-Term Memory* (LSTM) seperti DeepCS rentan terhadap kegagalan dalam menangkap korelasi semantik internal. Meskipun model yang lebih baru, seperti SAN-CS (Fang et al., 2021) yang menggunakan *Self-Attention*, telah meningkatkan akurasi secara signifikan dengan mengekstraksi bobot fitur internal, model tersebut masih memiliki kekurangan:

1.  **Mengabaikan Korelasi Antar-Komponen:** SAN-CS hanya berfokus pada hubungan internal dalam setiap komponen kode (API, Nama, Tokens) tetapi **mengabaikan hubungan bobot di antara komponen-komponen tersebut**, yang penting untuk integritas semantik kode secara keseluruhan.
2.  **Rentannya Hilangnya Informasi:** Proses pelatihan jaringan saraf dapat menyebabkan **hilangnya informasi fitur** penting.

::: tip Solusi yang Diusulkan
Untuk mengatasi kekurangan ini, kami mengusulkan model **MSARN-CS** (*Multilayer Self-Attention Residual Network for Code Search*). MSARN-CS menggunakan (1) *Joint Attention Embedding* Multilayer untuk secara eksplisit menangkap hubungan semantik antara komponen-komponen kode, dan (2) **Jaringan Residual** untuk mengkompensasi hilangnya informasi selama pelatihan jaringan.
:::

## 2. Metodologi

MSARN-CS memperluas arsitektur berbasis *Self-Attention* dengan menambahkan lapisan *embedding* berpasangan (*two-by-two embedding*) dan koneksi residual.

### A. Arsitektur MSARN-CS

Model menerima dua input: **Fragmen Kode** (terdiri dari API, Nama Metode, dan Tokens) dan **Deskripsi** (Des).

1.  **Langkah 1 (Embedding Awal):** Keempat urutan (API, Nama, Tokens, Des) diubah menjadi vektor awal ($V_{API}$, $V_{Names}$, $V_{Tokens}$, $V_{Des}$) melalui *embedding* kata.
2.  **Langkah 2 (Self-Attention):** Vektor kode awal diumpankan ke jaringan *Self-Attention* untuk mendapatkan representasi berbobot internal ($A_{API}$, $A_{Names}$, $A_{Tokens}$).
3.  **Langkah 3 (Joint Embedding Multilayer):** Ini adalah lapisan kunci. *Attention* *joint embedding* diterapkan secara berpasangan antara $A_{API}$, $A_{Names}$, dan $A_{Tokens}$ untuk mempelajari korelasi semantik. Hasil *embedding* berpasangan kemudian disatukan ($V_{n1}$, $V_{n2}$, dst.) untuk membentuk vektor yang kaya semantik ($V_{n}$, $V_{a}$, $V_{t}$).
4.  **Langkah 4 (Residual Network Level 1):** Vektor *joint embedding* kode ($V_{n}$, $V_{a}$, $V_{t}$) disuperimposisikan dengan vektor *embedding* awal ($V_{Names}$, $V_{API}$, $V_{Tokens}$) untuk mengkompensasi potensi hilangnya informasi di awal:
$$V_{nc}=V_{n}+V_{Names}$$
$$V_{ac}=V_{a}+V_{API}$$
$$V_{tc}=V_{t}+V_{Tokens}$$
5.  **Langkah 5 (Concat & Joint Embedding Utama):** Vektor komponen kode yang diperkaya residual disatukan (*concatenate*) untuk mendapatkan $V_{code}$. Kemudian $V_{code}$ dan vektor Deskripsi yang telah diproses (*AVDes*) menjalani *Joint Attention* lagi untuk mendapatkan $JV_{Code}$ dan $JVA_{Des}$.
6.  **Langkah 6 (Residual Network Level 2):** Vektor *joint embedding* utama disuperimposisikan dengan vektor kode/deskripsi sebelum *joint embedding* untuk mempertahankan informasi:
$$V'_{c}=V_{c}+V_{code}$$
$$V'_{D}=V_{D}+V_{Des}$$

### B. Similarity dan Training
Vektor akhir kode ($V'_{c}$) dan deskripsi ($V'_{D}$) dipulihkan menggunakan *Average Pooling* (dipilih karena lebih efektif dalam mengekstrak fitur internal dibandingkan *Max Pooling* setelah *Self-Attention*).

Kesamaan diukur menggunakan **Cosine Similarity** ($cos(c,d)$). Pelatihan model menggunakan **Minimum Ranking Loss Degree (Triplet Loss)**:
$$\mathcal{L}(\theta)=\sum_{(c,d^{+},d^{-})}max(0,\xi-cos(c,d^{+})+cos(c,d^{-}))$$
Di mana $\xi$ adalah konstanta batas (*margin*), $d^{+}$ adalah deskripsi positif, dan $d^{-}$ adalah deskripsi negatif.

## 3. Detail Pengujian

### Dataset
*   **Data Pelatihan:** Dataset Java dari GitHub (Gu et al., 2018), terdiri dari $\mathbf{18.233.872}$ data pelatihan, $100.000$ validasi, dan $10.000$ tes.

### Baseline
Model dibandingkan dengan $\mathbf{4}$ *baseline* utama:
*   **DeepCS:** Model berbasis LSTM awal.
*   **CRLCS:** Model berbasis CNN (*Co-attentive Representation Learning*).
*   **SAN-CS-:** Varian SAN-CS tanpa *joint embedding* antara kode dan kueri.
*   **SAN-CS:** Model *Self-Attention* lengkap.

### Metrik Evaluasi
Tiga metrik utama digunakan untuk mengukur kinerja *code search*:
*   **Recall@k ($R@k$):** Persentase keberhasilan (jawaban benar ada di Top-k).
$$Recall@k =\frac{1}{|Q|}\sum_{j=1}^{|Q|}\epsilon$$
*   **Mean Reciprocal Rank (MRR):** Mengukur posisi urutan hasil yang benar.
$$MRR=\frac{1}{|Q|}\sum_{j=1}^{|Q|}\frac{1}{Index_{Qj}}$$
*   **Normalized Discounted Cumulative Gain (NDCG):** Mengukur relevansi hasil.
$$NDCG=\frac{1}{|Q|}\sum_{j=1}^{k}\frac{2^{r_{j}}-1}{log_{2}(1+j)}$$

## 4. Hasil Eksperimen

### RQ1. Perbandingan Kinerja Model
MSARN-CS menunjukkan akurasi yang lebih unggul secara konsisten.

| Model | Recall@1 | Recall@5 | Recall@10 | MRR | NDCG |
| :--- | :--- | :--- | :--- | :--- | :--- |
| DeepCS | 0.4752 | 0.7610 | 0.8633 | 0.6169 | 0.6169 |
| CRLCS | 0.5491 | 0.7132 | 0.7824 | 0.5354 | 0.5926 |
| SAN-CS | 0.9310 | 0.9560 | 0.9620 | 0.9080 | 0.9210 |
| **MSARN-CS** | **0.9547** | **0.9716** | **0.9835** | **0.9432** | **0.9514** |

*   **Peningkatan Kinerja:** MSARN-CS meningkatkan $\mathbf{3.88\%}$ MRR dan $\mathbf{3.30\%}$ NDCG dibandingkan dengan *baseline* terbaik (SAN-CS).
*   **Alasan:** MSARN-CS mampu menangkap **korelasi antar-komponen** (misalnya, kueri "add url to classpath" memerlukan korelasi kata kerja 'add' dengan 'url' dan 'classpath'), yang gagal ditangkap oleh SAN-CS.

### RQ2. Dampak Joint Embedding Multilayer (Ablasi)
Simulasi ablasi menunjukkan pentingnya *embedding* berpasangan:

| Model | Recall@1 | MRR | Keterangan |
| :--- | :--- | :--- | :--- |
| **MSARN-CS** | **0.9547** | **0.9432** | Semua *Joint Embedding* |
| CS-NT | 0.9423 | 0.9232 | Hanya Names $\leftrightarrow$ Tokens |
| CS-NA | 0.9409 | 0.9211 | Hanya Names $\leftrightarrow$ API |
| CS-AT | 0.9387 | 0.9201 | Hanya API $\leftrightarrow$ Tokens |

*   Hasil menunjukkan bahwa *Joint Embedding* antara **Names dan Tokens** (CS-NT) memiliki dampak terbesar terhadap fitur kode secara keseluruhan.
*   API memiliki dampak paling kecil, mungkin karena urutan API umumnya pendek dan membawa lebih sedikit fitur efektif dalam konteks arsitektur ini.

### RQ3. Dampak Jaringan Residual (Ablasi)

| Model | Recall@1 | MRR | Keterangan |
| :--- | :--- | :--- | :--- |
| **MSARN-CS** | **0.9547** | **0.9432** | Semua Residual |
| CS-code | 0.9236 | 0.9012 | Residual Kode Dihapus (Penurunan $\mathbf{4.45\%}$ MRR) |
| CS-query | 0.9501 | 0.9370 | Residual Kueri Dihapus (Penurunan $0.66\%$ MRR) |
| CS-code-query | 0.9012 | 0.9078 | Semua Residual Dihapus |

*   **Residual sangat penting:** Penghapusan **Residual Kode** menyebabkan penurunan kinerja yang jauh lebih besar daripada penghapusan Residual Kueri. Hal ini menunjukkan bahwa Jaringan Residual secara efektif mengkompensasi hilangnya informasi fitur kode yang lebih kompleks selama *joint embedding*.

## 5. Kesimpulan

Model **MSARN-CS** yang diusulkan berhasil mengatasi keterbatasan model *code search* berbasis *self-attention* sebelumnya dengan memperkenalkan **Joint Embedding Multilayer** dan **Jaringan Residual**. Peningkatan ini memungkinkan model untuk secara akurat menangkap korelasi semantik antara komponen-komponen kode (Nama, API, Tokens) dan secara efektif mempertahankan informasi fitur yang seharusnya hilang selama pelatihan. MSARN-CS mencapai kinerja *state-of-the-art*, yang membuktikan efektivitas metode arsitektur yang diusulkan.

::: info Dampak Praktis
Penggunaan MSARN-CS dapat secara signifikan meningkatkan **efisiensi pemrograman** dengan memberikan hasil *code search* yang lebih akurat (lebih dari $95\%$ *Recall@1*). Jaringan Residual membuktikan bahwa model DL dapat ditingkatkan melalui mekanisme kompensasi untuk memelihara integritas fitur pada data yang kompleks seperti kode sumber.
:::