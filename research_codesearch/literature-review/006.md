---
title: Review Paper - NACS untuk Deep Code Search Agnostik Penamaan
description: Rangkuman paper tentang metode NACS berbasis Contrastive Multi-View Learning untuk Deep Code Search yang Agnostik Penamaan (ACM transactions on knowledge discovery from data, 2025).
head:
  - - meta
    - name: keywords
      content: Code search, Contrastive Multi-View Learning, Abstract Syntax Tree, Graph Neural Network, Naming-Agnostic, GIN
---

# 006 - Deep Code Search with Naming-Agnostic Contrastive Multi-View Learning
[https://doi.org/10.1145/3737878]

**Penulis:** **Jiadong Feng** ᵃ, **Wei Li** ᵇ, **Suhuang Wu** ᵃ, **Zhao Wei** ᶜ, **Yong Xu** ᶜ, **Juhong Wang** ᶜ, **Hui Li** ᵃ*

**Afiliasi:**
* ᵃ Key Laboratory of Multimedia Trusted Perception and Efficient Computing, Ministry of Education of China, Xiamen University, Xiamen, China
* ᵇ School of Electronic and Computer Engineering, Peking University, Shenzhen, China
* ᶜ Tencent, Shenzhen, China

**Kronologi:** Received: 18 October 2022 • Revised: 6 November 2024 • Accepted: 8 May 2025 • Available Online: July 2025

<a href="https://www.scimagojr.com/journalsearch.php?q=5800173377&tip=sid&clean=0" target="_blank"><img src="https://www.scimagojr.com/journal_img.php?id=5800173377" alt="SCImago Journal & Country Rank" /></a>

| Resume Eksekutif |
| :--- |
| **Publikasi:**<br>• **Jurnal:** ACM Transactions on Knowledge Discovery from Data (TKDD), Vol 19, Isu 6, Artikel 116 (2025)<br>• **Penerbit:** ACM<br>• **Topik:** Deep Code Search yang Agnostik terhadap Konvensi Penamaan (Naming-Agnostic)<br><br>**Masalah & Solusi:**<br>• **Masalah:** Metode *deep learning* untuk *code search* saat ini sangat bergantung pada korespondensi variabel eksplisit. Namun, pengembang menggunakan konvensi penamaan yang berbeda-beda (*naming issue*), membuat variabel yang sama memiliki nama yang berbeda di potongan kode yang berbeda, sehingga menurunkan performa model. Masalah ini sangat kurang dieksploitasi dalam pemodelan **Abstract Syntax Tree (AST)**.<br>• **Solusi:** Mengusulkan metode **NACS** (*Naming-Agnostic Code Search*) berbasis *Contrastive Multi-View Learning*. NACS **melepaskan informasi yang terikat pada nama variabel** dari AST, fokus menangkap properti intrinsik dari struktur AST saja, dan menggunakan *multi-view learning* (Graph View dan Path View) untuk memperkuat representasi kode.<br><br>**Contoh Penerapan:**<br>• **Skenario:** Mencari potongan kode yang melakukan fungsi yang sama, meskipun nama variabel seperti `lst` dan `l` telah diubah secara radikal di antara implementasi tersebut.<br>• **Fokus NACS:** Mengabaikan blok nama variabel (`Name:ix`, `Name:item`) dalam AST dan berkonsentrasi pada node struktural (`FunctionDef`, `If`, `Assign`, dll.) untuk memastikan kedua potongan kode dianggap identik secara fungsional dan struktural.<br><br>**Metodologi:**<br>• **Arsitektur:** Menggabungkan *Graph Topology Encoder* (berbasis **GIN**) untuk *Graph View* dan **Bi-LSTM** untuk *Path View* (jalur Root-to-Leaf dalam AST), dilatih melalui *Contrastive Multi-View Learning*.<br>• **Augmentasi Data:** Menggunakan strategi **Program Transformation (PT)** tingkat semantik (misalnya, *Insert Dead Code*, *Swap Independent Statements*) dan **Structure Transformation (ST)** tingkat sintaksis (misalnya, *Subtree Dropping*, *Feature Shuffling*) untuk menghasilkan pasangan positif.<br>• **Fungsi Rugi:** Menggunakan *InfoNCE loss* ($\mathcal{L}$) sebagai tujuan gabungan dari *Graph Contrastive Loss* ($\mathcal{L}_g$), *Path Contrastive Loss* ($\mathcal{L}_p$), dan *Query Loss* ($\mathcal{L}_q$).<br><br>**Temuan Kunci:**<br>1. **Agnostik Penamaan:** Kinerja NACS hampir tidak terpengaruh oleh perubahan nama variabel pada dataset **CoSQA-Var**, sementara *baseline* lain mengalami penurunan signifikan (hingga 42.11% pada GraphCodeBERT).<br>2. **Kinerja Superior:** NACS mencapai skor MRR tertinggi secara konsisten di semua dataset (*CodeSearchNet-Python/Java*, *CoSQA*, *CoSQA-Var*).<br>3. **Fusion Multi-View:** Kombinasi *Graph View* dan *Path View* menghasilkan kinerja lebih baik daripada menggunakan salah satu komponen secara terpisah, membuktikan sinergi *multi-view*.<br><br>**Kontribusi Utama:**<br>• Mengusulkan NACS, yang pertama kali mengatasi masalah *naming issue* saat memodelkan AST untuk *code search*.<br>• Mendesain *Contrastive Multi-View Learning* untuk memperkuat pemahaman struktur AST tanpa terikat pada nama variabel eksplisit.<br>• Mendemonstrasikan bahwa ide NACS dapat diadaptasi untuk meningkatkan ketahanan (*robustness*) model *code search* yang sudah ada terhadap masalah penamaan. |

## 1. Pendahuluan & Masalah

Pengembangan perangkat lunak adalah tugas berulang, di mana pengembang sering kali menggunakan kembali atau mendapatkan inspirasi dari implementasi yang sudah ada. Oleh karena itu, *code search* (pencarian potongan kode yang relevan berdasarkan niat pengembang) menjadi sangat penting. Meskipun teknik *deep learning* telah meningkatkan kualitas *code search*, sebagian besar metode yang ada gagal mempertimbangkan dampak dari perbedaan gaya penamaan (*naming convention*).

Masalah penamaan (*naming issue*) timbul karena pengembang mungkin tidak mengikuti konvensi penamaan yang sama, sehingga variabel yang sama dapat memiliki nama yang berbeda di potongan kode yang berbeda. Hal ini menjadi penghalang bagi metode *deep learning* karena banyak yang mengandalkan korespondensi entitas eksplisit (misalnya, variabel yang sama selalu diwakili oleh vektor yang sama). Meskipun beberapa pekerjaan telah mencoba mengatasi ini dengan memodelkan *dataflow graph* (GraphCodeBERT), masalah ini **belum dieksploitasi** ketika memodelkan **Abstract Syntax Tree (AST)**, representasi struktural sintaksis kode yang umum digunakan. AST masih menyandikan nama node yang berisi nama variabel, menyebabkan model yang mengandalkan AST menderita masalah penamaan.

::: tip Solusi yang Diusulkan
Untuk mengatasi tantangan ini, kami mengusulkan **NACS** (*Naming-Agnostic Code Search*), metode *code search* berbasis *Contrastive Multi-View Learning*. NACS dirancang untuk **melepaskan informasi yang terikat pada nama variabel** dari AST, dan fokus menangkap properti intrinsik murni dari struktur AST. Metode ini diperkuat melalui *multi-view learning* yang memodelkan AST dalam *Graph View* dan *Path View*.
:::

## 2. Metodologi

NACS terdiri dari tiga langkah utama: Augmentasi Data, *Contrastive Multi-View Learning* (Pre-training), dan Peningkatan *Code Search* (Fine-tuning).

### A. Augmentasi Data
Augmentasi data dilakukan pada AST untuk menghasilkan sampel positif yang secara fungsional setara tetapi memiliki perbedaan sintaksis atau semantik.
*   **Strategi Program Transformation (PT) - Tingkat Semantik:** Tidak mengubah semantik asli. Contoh: *Insert Dead Code Statement*, *Swap Independent Statements*, dan *Change Loop Statements* (misalnya, `for-loop` menjadi `while-loop`).
*   **Strategi Structure Transformation (ST) - Tingkat Sintaksis:** Menghasilkan data yang meningkatkan pemahaman struktural. Contoh: *Subtree Dropping* (menghapus *subtree* secara acak dengan probabilitas terbalik dari jumlah node) dan *Feature Shuffling* (mengacak fitur node).

### B. Contrastive Multi-View Code Representation Learning
Komponen inti NACS ini dirancang agnostik terhadap penamaan.
*   **Agnostik Penamaan:** NACS menghilangkan informasi yang terikat pada nama variabel dari AST input. Representasi node AST hanya menggunakan tipe node ($b_{s}^{type}$) dan derajatnya ($b_{s}^{degree}$), serta *laplacian eigenvectors* ($b_{s}^{lapla}$), untuk memastikan model hanya menangkap properti intrinsik struktural.
$$b_{s}=b_{s}^{type}\oplus b_{s}^{degree}\oplus b_{s}^{lapla}$$
*   **Graph-View Modeling:** AST dimodelkan sebagai graf, dan **Graph Isomorphism Network (GIN)** digunakan sebagai *Graph Topology Encoder* (GT-Encoder) untuk menghasilkan representasi graf ($r_g$). Pelatihan menggunakan *AST graph discrimination task* dengan *contrastive loss* ($\mathcal{L}_g$).
*   **Path-View Modeling:** AST dimodelkan sebagai sekumpulan jalur *Root-to-Leaf*. Representasi jalur ($h_{c,i}$) diekstrak menggunakan **Bi-LSTM**, dan representasi AST final ($w_c$) diperoleh melalui *mean pooling* dari semua jalur. Pelatihan menggunakan *cross-view contrastive loss* ($\mathcal{L}_p$), di mana representasi *Path View* dikontraskan dengan representasi *Graph View*.

### C. Query Encoding dan Fungsi Rugi Total
*   **Query Encoding:** Kueri ($b_i$) di-*embed* menggunakan Bi-LSTM ($s_i$). Pelatihan query menggunakan *InfoNCE loss* ($\mathcal{L}_q$), di mana representasi kueri dikontraskan dengan representasi *AST graph* dari potongan kode *ground-truth*.
*   **Objective Total:** Fungsi rugi keseluruhan untuk pre-training adalah:
$$\mathcal{L}=\mathcal{L}_{g}+\mathcal{L}_{p}+\mathcal{L}_{q}$$

### D. Fine-Tune dan Search
*   **Representasi Akhir:** Representasi kode akhir menggabungkan dua sumber: representasi *code-token* dari CodeBERT ($h_{c}^{code}$) dan representasi AST dari NACS *Graph Encoder* ($h_{c}^{AST}$).
*   **Matching Score:** Skor pencocokan gabungan dihitung selama fine-tuning dan pencarian:
$$score=score_{1}(q,c)+\lambda\cdot score_{2}(q,c)$$
di mana $score_1$ berasal dari CodeBERT dan $score_2$ dari NACS, dengan $\lambda$ sebagai parameter penyeimbang (optimal $\lambda=0.0001$).

## 3. Detail Pengujian

### Dataset
*   **CodeSearchNet-Python** (0.5 M fungsi)
*   **CodeSearchNet-Java** (0.5 M fungsi)
*   **CoSQA** (20.604 pasangan kueri-kode berlabel)
*   **CoSQA-Var:** Dataset baru yang dibuat dengan mengganti nama variabel secara acak di dalam CoSQA, digunakan khusus untuk menguji ketahanan *naming-agnostic*.

### Baseline Model
DeepCS, OCOR, CSRS, CoCLR, CodeBERT, GraphCodeBERT, dan DGMS.

### Metrik Evaluasi
Metrik utama yang digunakan adalah **Mean Reciprocal Rank (MRR)**:
$$MRR=\frac{1}{|Q|}\sum_{i=1}^{|Q|}\frac{1}{rank_{i}}$$
Di mana $|Q|$ adalah jumlah kueri, dan $rank_{i}$ adalah peringkat potongan kode *ground-truth* untuk kueri ke-$i$.

## 4. Hasil Eksperimen

### Analisis Kinerja Keseluruhan (RQ1)
NACS secara konsisten mencapai skor MRR tertinggi di keempat dataset.

| Model | CodeSearchNet-Python | CodeSearchNet-Java | CoSQA | COSQA-Var |
| :--- | :--- | :--- | :--- | :--- |
| CodeBERT | 0.665 | 0.667 | 0.652 | 0.558 |
| GraphCodeBERT | 0.694 | 0.689 | 0.648 | 0.456 |
| CoCLR | 0.637 | 0.631 | 0.647 | 0.515 |
| **NACS** | **0.701** | **0.705** | **0.708** | **0.704** |

**Temuan Kunci (Agnostik Penamaan):**
*   Kinerja semua *baseline* **menurun secara signifikan** pada dataset **CoSQA-Var** (GraphCodeBERT turun 42.11%, CoCLR turun 20.40%), menunjukkan kerentanan mereka terhadap masalah penamaan.
*   Kinerja **NACS hampir tidak terpengaruh** oleh penggantian nama variabel (MRR 0.708 pada CoSQA vs 0.704 pada CoSQA-Var), memverifikasi efektivitas desain *naming-agnostic*.

### Kontribusi Komponen (RQ2)
Studi ablasi menunjukkan kontribusi setiap komponen NACS pada dataset CoSQA:

| Varian NACS | MRR Score |
| :--- | :--- |
| $NACS_{a}$ (CodeBERT) | 0.652 |
| $NACS_{g}$ (tanpa $\mathcal{L}_g$) | 0.682 |
| $NACS_{p}$ (tanpa $\mathcal{L}_p$) | 0.687 |
| **NACS (Lg + Lp + Lq)** | **0.708** |

*   **Analisis:** Kinerja NACS (0.708) lebih unggul daripada varian yang hanya menggunakan salah satu *contrastive loss* ($NACS_g$ atau $NACS_p$), membuktikan bahwa *Graph-View Contrast* dan *Cross-View Contrast* bekerja sama secara efektif untuk meningkatkan kualitas *code search*.

### Peningkatan Model Lain (RQ3)
NACS dapat diadaptasi untuk meningkatkan model lain (misalnya, CodeBERT dapat diganti dengan DeepCS).

| Metode | CoSQA | CoSQA-Var | Penurunan Persentase |
| :--- | :--- | :--- | :--- |
| DeepCS | 0.472 | 0.433 | $\downarrow 8.26\%$ |
| DeepCS* (Enhanced by NACS) | 0.488 | 0.473 | $\downarrow 3.07\%$ |
| GraphCodeBERT | 0.648 | 0.456 | $\downarrow 42.11\%$ |
| GraphCodeBERT* (Enhanced by NACS) | 0.691 | 0.675 | $\downarrow 2.32\%$ |

*   **Analisis:** Metode yang ditingkatkan oleh NACS (*superscript* *) menunjukkan **penurunan kinerja yang jauh lebih kecil** pada CoSQA-Var dibandingkan versi aslinya. Ini menyimpulkan bahwa NACS dapat meningkatkan ketahanan (*robustness*) model *code search* yang ada terhadap masalah penamaan.

## 5. Kesimpulan

Paper ini memperkenalkan **NACS**, metode *code search* agnostik penamaan pertama yang secara eksplisit memodelkan AST tanpa terikat pada nama variabel, menggunakan *Contrastive Multi-View Learning* (Graph dan Path View). NACS berhasil mengatasi masalah penurunan kinerja yang disebabkan oleh perbedaan konvensi penamaan dalam implementasi kode. Hasil eksperimen ekstensif menunjukkan kinerja NACS yang superior dibandingkan *baseline* di berbagai dataset dan membuktikan ketahanannya terhadap masalah penamaan, bahkan ketika digunakan untuk meningkatkan model *code search* yang sudah ada.

::: info Dampak Praktis
NACS meningkatkan kualitas *code search* secara signifikan dengan memastikan bahwa fungsionalitas dan struktur kode dipahami secara intrinsik, terlepas dari gaya penamaan variabel yang digunakan. Hal ini sangat krusial untuk meningkatkan produktivitas pengembang, mengurangi beban kognitif, dan mempercepat proses penggunaan kembali kode.
:::