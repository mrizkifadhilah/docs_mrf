---
title: Review Paper - Peningkatan Bersamaan Generasi dan Pencarian Kode via GAN
description: Rangkuman paper tentang CoCoGAN, metode Generative Adversarial Network (GAN) untuk meningkatkan Code Generation dan Code Search secara mutual (Proc. ACM Program. Lang., 2023).
head:
  - - meta
    - name: keywords
      content: code generation, code search, generative adversarial network, GAN, adversarial training, CodeBLEU, MRR
---

# 046 - Two Birds with One Stone: Boosting Code Generation and Code Search via a Generative Adversarial Network
Tautan (DOI) [https://doi.org/10.1145/3622815]

**Penulis:** **Shangwen Wang** ᵃ, **Bo Lin** ᵃ, **Zhensu Sun** ᵇ, **Ming Wen** ᶜ*, **Yepang Liu** ᵈ*, **Yan Lei** ᵉ, **Xiaoguang Mao** ᵃ

**Afiliasi:**
* ᵃ National University of Defense Technology, China
* ᵇ Singapore Management University, Singapore
* ᶜ School of Cyber Science and Engineering, Huazhong University of Science and Technology, China
* ᵈ Research Institute of Trustworthy Autonoumous Systems, Department of Computer Science and Engineering, Southern University of Science and Technology, China
* ᵉ Chongqing University, China

**Kronologi:** Received: 2023-04-14 • Revised: 2023-08-27 • Accepted: 2023-08-27 • Available Online: Oktober 2023

<a href="https://www.scimagojr.com/journalsearch.php?q=21101020042&tip=sid" target="_blank"><img src="https://www.scimagojr.com/journal_img.php?id=21101020042" alt="SCImago Journal & Country Rank" /></a>

| Resume Eksekutif |
| :--- |
| **Publikasi:**<br>• **Jurnal:** Proceedings of the ACM on Programming Languages (PACMPL), Vol. 7, No. OOPSLA2, Article 239 (2023)<br>• **Topik:** Mengatasi keterbatasan model *Code Generation* dan *Code Search* melalui kerangka pelatihan permusuhan (*adversarial training*) menggunakan *Generative Adversarial Network* (GAN).<br><br>**Masalah & Solusi:**<br>• **Masalah 1 (Code Generation):** Model CG menderita masalah *exposure bias* saat inferensi, menghasilkan kode yang tidak menyerupai kode yang ditulis manusia dan seringkali tidak semantik benar.<br>• **Masalah 2 (Code Search):** Model CS terbatas karena kuantitas data pelatihan yang kurang, menyulitkan pemahaman semantik yang akurat saat diterapkan pada korpus kandidat besar di dunia nyata.<br>• **Wawasan & Solusi:** *Code generation* (CG) dan *Code search* (CS) memiliki kekuatan yang saling melengkapi. Mengusulkan **CoCoGAN**—pendekatan yang menggabungkan CG (sebagai **Generator, G**) dan CS (sebagai **Diskriminator, D**) dalam kerangka GAN untuk peningkatan mutual melalui pelatihan permusuhan.<br><br>**Contoh Penerapan:**<br>• Menggunakan **NatGen** (Generator) dan **GraphCodeBERT** (Diskriminator) pada *dataset* Python skala besar. Peningkatan **32%** pada skor **CodeBLEU** untuk CG dan **12%** pada **MRR** untuk CS, dibandingkan kinerja model aslinya.<br><br>**Metodologi:**<br>• **Kerangka CoCoGAN:** Memperlakukan model CG sebagai $G$ (bertujuan menipu $D$ dengan menghasilkan kode seperti manusia) dan model CS sebagai $D$ (bertujuan membedakan kode yang dihasilkan dari kode nyata/oracle, memberikan umpan balik kualitas ke $G$).<br>• **Fitur Kustomisasi GAN:**<br>  1. **Data Berpasangan & Panduan Kueri:** Mengatasi input *unpairwised* GAN. $G$ menerima kueri $q$, $D$ menerima pasangan $<o, q>$ (kode *oracle* dan kueri yang cocok) dan menggunakan $q$ sebagai panduan tambahan untuk diskriminasi.<br>  2. ***Sequence-Level Policy Gradient*:** Mengatasi masalah gradien yang tidak dapat dibedakan pada data diskrit (token kode). Pelatihan $G$ dimodelkan sebagai *Reinforcement Learning* (RL) dengan *policy gradient* [Yu et al. 2017], di mana *reward* dihitung berdasarkan seluruh urutan token yang dihasilkan, $D(\tilde{x}^i, q^i)$.<br>  3. **Pengulangan *Confusing Exemplars* (*Lifelong Learning*):** Mengatasi *catastrophic forgetting* pada $D$ (CS) saat dilatih pada tugas biner. *Confusing exemplars* (kode dari set pelatihan yang secara leksikal mirip tetapi tidak relevan secara semantik dengan $q$) dimasukkan ke $D$ untuk mempertahankan kemampuan membedakan semantik kode manusia.<br><br>**Temuan Kunci:**<br>1. **Peningkatan CG Signifikan:** Peningkatan CodeBLEU hingga **39%** (CodeT5 + GraphCodeBERT, Python) dan CrystalBLEU hingga **82%** (NatGen + GraphCodeBERT, Python). Peningkatan terbesar ada pada skor **Syntax Match (SM)**, menunjukkan $G$ menjadi lebih kompeten dalam menghasilkan sintaks dan struktur kode yang benar.<br>2. **Peningkatan CS Konsisten:** MRR meningkat sekitar **10% hingga 12%** di sebagian besar pengaturan, dan *SuccessRate@k* juga meningkat secara konsisten.<br>3. **Efisiensi:** Waktu pelatihan permusuhan sebanding dengan total waktu *fine-tuning* $G$ dan $D$, menunjukkan bahwa CoCoGAN efisien.<br>4. **Sensitivitas:** Strategi *Lifelong Learning* (Pengulangan *Confusing Exemplars*) **penting**, mencegah penurunan kinerja CS lebih dari 5% dan membantu mencapai kinerja optimal.<br><br>**Kontribusi Utama:**<br>• Mengidentifikasi kekuatan saling melengkapi antara *Code Generation* dan *Code Search*, membuka arah baru dalam menggabungkan kedua teknik tersebut.<br>• Mengusulkan **CoCoGAN**, jaringan permusuhan yang memanfaatkan GAN untuk peningkatan kinerja mutual $G$ dan $D$.<br>• Eksperimen ekstensif di delapan pengaturan berbeda (2 generator x 2 diskriminator x 2 bahasa) menunjukkan peningkatan kinerja yang konsisten.<br><br>**Dampak:**<br>• Menyediakan kerangka pelatihan baru yang efektif untuk meningkatkan kemampuan model DL dalam menafsirkan niat pengembang menjadi kode yang semantik benar, dengan potensi penerapan pada LLM seperti ChatGPT yang juga menghadapi masalah *exposure bias*. |

## 1. Pendahuluan & Masalah

Sistem pemberi rekomendasi di bidang rekayasa perangkat lunak memainkan peran penting dalam meningkatkan produktivitas pengembang. Salah satu tujuan utamanya adalah secara efektif menerjemahkan niat pengembang, yang biasanya berupa kueri bahasa alami, menjadi kode sumber. Penelitian telah mengusulkan dua jenis pendekatan: **Pencarian Kode (*Code Search*, CS)**, yang mengambil *snippet* kode yang relevan dari *codebase* yang sudah ada, dan **Generasi Kode (*Code Generation*, CG)**, yang menghasilkan *snippet* kode dari awal menggunakan model terjemahan mesin saraf.

Meskipun terdapat upaya yang signifikan, efektivitas teknik CG dan CS yang ada masih terbatas. Model CG sering menghasilkan kode yang tidak semantik benar karena masalah *exposure bias* selama inferensi, sedangkan model CS mengalami kesulitan dalam membedakan semantik kode secara akurat dalam korpus kandidat besar karena data pelatihan yang terbatas.

::: tip Solusi yang Diusulkan
Kami mengusulkan **CoCoGAN** (*Code generation and Code search via a Generative Adversarial Network*), kerangka kerja yang memanfaatkan kekuatan saling melengkapi dari CG dan CS melalui pelatihan permusuhan GAN. CG berfungsi sebagai **Generator (G)**, menghasilkan kode, dan CS berfungsi sebagai **Diskriminator (D)**, mengevaluasi kualitas kode yang dihasilkan dan memberikan *reward* umpan balik, sekaligus menggunakan kode yang dihasilkan $G$ sebagai data pelatihan tambahan untuk memperkuat pemahaman semantik $D$.
:::

## 2. Metodologi

CoCoGAN mengadaptasi kerangka GAN tradisional untuk tugas diskrit dan terpandu spesifik dalam rekayasa perangkat lunak.

### A. Data Berpasangan & Panduan Kueri untuk Mengatasi Tantangan \#1
GAN tradisional bekerja dengan input *unpairwised* (misalnya, vektor acak), tetapi CG membutuhkan kode yang memenuhi fungsionalitas tertentu.
1.  **Strategi Data Berpasangan:** $G$ menerima kueri $q$ ($p_{query}(q)$) dan $D$ menerima pasangan kode *oracle* $o$ yang cocok secara semantik dengan kueri $q$, yaitu $<o, q>$.
2.  **Panduan Kueri:** Kueri $q$ juga diumpankan ke $D$ (Diskriminator/CS) untuk memandu proses diskriminasi. Hal ini memaksa $G$ untuk menghasilkan kode $G(q)$ yang tidak hanya terlihat realistis tetapi juga memenuhi fungsionalitas yang dimaksudkan oleh $q$.

### B. Sequence-Level Policy Gradient untuk Mengatasi Tantangan \#2
Karena kode terdiri dari token diskrit (non-kontinu), gradien kerugian $G$ ($D(G(q), q)$) tidak dapat dihitung secara langsung.
1.  **Reinforcement Learning (RL):** $G$ diperlakukan sebagai agen RL.
2.  ***Policy Gradient*** **dan *Sequence-Level Reward***: *Policy gradient* [Yu et al. 2017] digunakan untuk memperbarui parameter $G$. Umpan balik $D$ digunakan sebagai sinyal *reward* yang mengukur kemungkinan kode yang dihasilkan $\tilde{x}^i$ menipu $D$. Untuk mengatasi masalah *program aliasing* (implementasi yang berbeda), *reward* dihitung berdasarkan seluruh urutan token yang dihasilkan (*sequence-level*) daripada per langkah *decoding*.
    $$\nabla\overline{\mathbb{R}}_{\theta_{g}}=\frac{1}{m}\sum_{i=1}^{m}D(\tilde{x}^{i},q^{i})\cdot\nabla P_{\theta_{g}}(\tilde{x}^{i}|q^{i})$$
    di mana $D(\tilde{x}^{i},q^{i})$ adalah *reward* dan $P_{\theta_{g}}(\tilde{x}^{i}|q^{i})$ adalah probabilitas $G$ menghasilkan $\tilde{x}^i$ diberikan $q^i$.

### C. Replaying Confusing Exemplars untuk Mengatasi Tantangan \#3
Pelatihan permusuhan dapat menyebabkan *catastrophic forgetting* pada $D$ (yang merupakan CS), membuatnya melupakan pengetahuan yang dipelajari sebelumnya tentang membedakan semantik kode manusia.
1.  ***Lifelong Learning* Berbasis Pengulangan:** Teknik ini digunakan untuk mempertahankan kemampuan CS.
2.  ***Confusing Exemplars*** **(Replayed Samples):** Selain membedakan kode *oracle* $o$ dari kode yang dihasilkan $G(q)$, $D$ juga dilatih untuk membedakan $o$ dari $l$ kode *human-written* lain ($\{ir^1, ..., ir^l\}$) yang dipilih dari set pelatihan. Pemilihan didasarkan pada **kesamaan token tingkat Jaccard** dengan kode *oracle*, karena kode ini secara leksikal mirip tetapi semantik tidak relevan dan mudah membingungkan CS.
3.  **Fungsi Kerugian Akhir $D$:** Tujuan pelatihan $D$ adalah memaksimalkan:
    $$ \max_{D} \mathbb{E}_{q\sim p_{query}(q)} \left[ \log D(o, q) - \log D(G(q), q) - \sum_{i=1}^{l} \log D(ir^i, q) \right] $$

## 3. Detail Pengujian

### Dataset
**CodeSearchNet (CSN)** *benchmark* digunakan: **CSN-Python** (412K data pelatihan) dan **CSN-Java** (394K data pelatihan).

### Generator dan Diskriminator
*   **Generator (G):** **CodeT5** [Wang et al. 2021] dan **NatGen** [Chakraborty et al. 2022] (dibangun di atas CodeT5).
*   **Diskriminator (D):** **CodeBERT** [Feng et al. 2020] dan **GraphCodeBERT** [Guo et al. 2021] (menggabungkan informasi struktur kode).
Total 8 kombinasi eksperimen (2 bahasa $\times$ 2 generator $\times$ 2 diskriminator).

### Metrik
#### Code Generation (G)
1.  **CodeBLEU** [Ren et al. 2020]: Metrik holistik yang menggabungkan kecocokan token (TM), kecocokan sintaks (SM), dan kecocokan *dataflow* (DM).
2.  **CrystalBLEU** [Eghbali dan Pradel 2022]: Varian BLEU yang mengurangi bobot *n-gram* yang sering muncul untuk menilai kesamaan semantik secara lebih akurat.

#### Code Search (D)
1.  **Mean Reciprocal Rank (MRR)** [Lv et al. 2015]: Rata-rata *reciprocal rank* kode *oracle* dalam hasil.
    $$MRR=\frac{1}{|Q|}\sum_{q\in Q}\frac{1}{Rank_{q}}$$
2.  **Success Rate@k (SR@k)** [Gu et al. 2018]: Persentase kueri di mana kode *oracle* muncul dalam $k$ hasil teratas ($k=1, 5, 10$).

## 4. Hasil Eksperimen

### RQ1: Efektivitas Generasi Kode

| Generator | Diskriminator | Dataset | CodeBLEU (Awal) | CodeBLEU (CoCoGAN) | Peningkatan |
| :--- | :--- | :--- | :--- | :--- | :--- |
| CodeT5 | GraphCodeBERT | Python | 21.2% | 29.4% | $\uparrow$ 39% |
| NatGen | GraphCodeBERT | Python | 21.6% | 28.4% | $\uparrow$ 32% |
| NatGen | GraphCodeBERT | Java | 20.8% | 27.0% | $\uparrow$ 30% |

*   **Peningkatan CodeBLEU dan CrystalBLEU:** Peningkatan CodeBLEU hingga $\sim$40% dan CrystalBLEU hingga $\sim$80% diamati pada Python. Peningkatan CrystalBLEU lebih besar daripada TM, menunjukkan $G$ menghasilkan token yang lebih berbobot semantik.
*   **Perbaikan SM:** Peningkatan terbesar ada pada **Syntax Match (SM)** (misalnya, CodeT5 $\uparrow$ 126% pada Python), menunjukkan $G$ menjadi jauh lebih kompeten dalam menghasilkan sintaks dan struktur kode yang benar (mengatasi *code smells*).

### RQ2: Efektivitas Pencarian Kode

| Code Searcher | Generator | Dataset | MRR (Awal) | MRR (CoCoGAN) | Peningkatan |
| :--- | :--- | :--- | :--- | :--- | :--- |
| GraphCodeBERT | NatGen | Python | 0.682 | 0.763 | $\uparrow$ 12% |
| GraphCodeBERT | NatGen | Java | 0.534 | 0.589 | $\uparrow$ 10% |
| CodeBERT | NatGen | Python | 0.669 | 0.721 | $\uparrow$ 8% |

*   **Peningkatan MRR:** Semua model CS menunjukkan peningkatan MRR sekitar $\mathbf{10\%}$ hingga $\mathbf{12\%}$.
*   **SR@1:** GraphCodeBERT (dengan NatGen) meningkatkan SR@1 dari 57.7% menjadi 67.8% pada Python, yang berarti kode *oracle* berada di posisi teratas untuk $\sim$70% kasus.
*   **Pilihan Model:** Kinerja terbaik dicapai ketika menggunakan teknik dengan performa tinggi dari kedua jenis (misalnya, NatGen $\leftrightarrow$ GraphCodeBERT), yang menunjukkan bahwa KAPE optimal melibatkan teknik yang canggih.

### RQ3: Efisiensi

Waktu pelatihan permusuhan CoCoGAN secara keseluruhan kira-kira **sama dengan total waktu *fine-tuning* generator dan diskriminator secara terpisah** (misalnya, Python: NatGen (18h) + CodeBERT (6h) $\approx$ 24h, CoCoGAN $\approx$ 26h), yang dianggap berskala wajar untuk tugas *one-time*.

### RQ4: Analisis Sensitivitas (Lifelong Learning)

Mengeluarkan strategi *Lifelong Learning* (*Replaying Confusing Exemplars*) menghasilkan **penurunan kinerja lebih dari 5%** pada MRR CS. Namun, kinerja yang menurun ini masih lebih baik daripada model CS yang tidak dilatih secara permusuhan. Ini membuktikan bahwa: (1) Pelatihan permusuhan memang memperkuat CS (pemahaman semantik), dan (2) Strategi *lifelong learning* diperlukan untuk mencegah *catastrophic forgetting* dan mencapai kinerja optimal.

## 5. Kesimpulan

CoCoGAN berhasil membuktikan hipotesis bahwa *Code Generation* dan *Code Search* dapat saling meningkatkan melalui kerangka pelatihan permusuhan GAN. Generator dipaksa menghasilkan kode yang lebih mirip manusia, sementara Diskriminator (CS) dipaksa untuk menangkap perbedaan semantik yang halus berkat kode yang dihasilkan oleh $G$.

::: info Dampak Praktis
CoCoGAN menyediakan kerangka kerja yang efektif dan efisien untuk meningkatkan kemampuan model DL dalam menghasilkan kode yang **semantik benar** dan meningkatkan akurasi *code search*. Pendekatan ini relevan untuk mengatasi masalah *exposure bias* pada model yang menghasilkan urutan diskrit, termasuk *Large Language Models* (LLMs), dan dapat mendorong pengembangan perangkat lunak yang lebih efisien dan andal.
:::