---
title: Review Paper - Model Penyelarasan Gabungan untuk Pencarian Kode
description: Rangkuman paper tentang model penyelarasan gabungan untuk pencarian kode (IEICE TRANS. INF. & SYST, 2024).
head:
  - - meta
    - name: keywords
      content: code search, deep learning, code analysis, cross-modal attention, intra-modal attention, bi-directional ranking loss, multi-encoder
---

# 032 - A Combined Alignment Model for Code Search
Tautan (DOI) [10.1587/transinf.2023MPP0002](https://doi.org/10.1587/transinf.2023MPP0002)

**Penulis:** **Juntong HONG** $^{a}$*, **Eunjong CHOI** $^{b}$, **Osamu MIZUNO** $^{b}$

**Afiliasi:**
* $^a$ Graduate School of Science and Technology, Kyoto Institute of Technology, Kyoto-shi, 606-8585 Japan
* $^b$ Faculty of Information and Human Sciences, Kyoto Institute of Technology, Kyoto-shi, 606-8585 Japan

**Kronologi:** Received: April 28, 2023 • Revised: September 20, 2023 • Accepted: December 15, 2023 • Publicized: December 15, 2023

<a href="https://www.scimagojr.com/journalsearch.php?q=15109&tip=sid" target="_blank"><img src="https://www.scimagojr.com/journal_img.php?id=15109" alt="SCImago Journal & Country Rank" /></a>

| Resume Eksekutif |
| :--- |
| **Publikasi:**<br>• **Jurnal:** IEICE TRANS. INF. & SYST, VOL.E107-D, NO.3 (March 2024), 257–267<br>• **Topik:** Peningkatan kinerja dan efisiensi *code search* dengan mengurangi ketergantungan pada representasi *multi-field* dan *multi-encoder*, serta mengatasi kehilangan informasi penyelarasan kata dalam matriks relasi.<br><br>**Masalah & Solusi:**<br>• **Masalah 1 (Kompleksitas & Sumber Daya):** Model *multi-encoder* sebelumnya (*DeepCS, TabCS, FcarCS*) menggunakan beberapa *encoder* untuk memproses *multi-field* kode (Nama Metode, Token, API, dan terkadang AST), yang meningkatkan jumlah parameter, biaya komputasi, dan meningkatkan risiko OOV (*Out-of-Vocabulary*).<br>• **Masalah 2 (Kehilangan Informasi):** Pemanfaatan matriks relasi kueri-kode yang hanya mengandalkan operasi *max-pooling* untuk mendapatkan bobot atensi akan **mengabaikan** dan **menghilangkan** informasi penyelarasan kata yang esensial, secara keliru menyaring fitur penting.<br>• **Solusi:** Mengusulkan **Model Penyelarasan Gabungan** (*Combined Alignment Model*). (1) **Efisiensi:** Menggabungkan tiga *field* kode dasar (Nama Metode, Token, API) menjadi satu urutan dan menggunakan **satu *encoder* tunggal**. (2) **Penyelarasan:** Menggabungkan atensi **intra-modal** dan **cross-modal** untuk penetapan bobot yang komprehensif. **Cross-modal Attention** menggunakan vektor yang dapat dilatih (*trainable vectors*) untuk transformasi matriks relasi, menghindari *max-pooling* dan kehilangan informasi.<br><br>**Contoh Penerapan:**<br>• Dievaluasi pada dua *dataset* populer: **CSN** (*Code Search Net*) dan **Hu et al.'s**. Keduanya mengandung pasangan kueri-kode yang diproses dengan empat *field* kode.<br><br>**Metodologi:**<br>• **Word Embedding:** Menggabungkan $C_{name}, C_{token}, C_{api}$ menjadi satu urutan, menggunakan **satu lapisan *embedding*** ($E_c$) untuk representasi kode $C$.<br>• **Relation Matrix:** Dihitung menggunakan $a(C,Q) = \tanh(CW_{relation}Q^T)$, menghubungkan kata-kata kode dan kueri.<br>• **Cross-Modal Attention:** Mentransformasi matriks relasi ($a(C,Q)$) baris dan kolom demi baris menggunakan **vektor yang dapat dilatih** ($W_3, W_4$), bukan *max-pooling*, untuk mendapatkan fitur berbobot $a(C,Q)^{cq}$ dan $a(C,Q)^{qc}$.<br>• **Intra-Modal Attention:** Mengeksplorasi dua jenis: *Self-based Attention* (SA) dan *Distance-based Attention* (DA). **DA** secara khusus **menggunakan kembali Matriks Relasi** yang sudah di-normalisasi $l_2$ (menggabungkan informasi modalitas lain).<br>• **Aggregation:** Menggabungkan fitur berbobot *cross-modal* dan *intra-modal* ($fa(Q), fa(C)$), lalu menggunakannya untuk memberi bobot pada *embedding* kode/kueri akhir $\tilde{C}, \tilde{Q}$ melalui lapisan *softmax* dan vektor yang dapat dilatih ($W_{hv}, W_{hq}$).<br>• **Optimasi:** Menggunakan **Bi-directional Ranking Loss** untuk memaksimalkan jarak antara pasangan negatif dan memaksimalkan perbedaan antara pasangan positif dan negatif.<br><br>**Temuan Kunci:**<br>1. **Kinerja SOTA:** Mencapai $\mathbf{0.614}$ Top@1 pada CSN (mengalahkan TabCS $12.2\%$) dan $\mathbf{0.687}$ Top@1 pada Hu et al.'s (mengalahkan FcarCS $9.3\%$).<br>2. **Efisiensi Arsitektur:** Terbukti bahwa arsitektur model yang dirancang dengan baik, menggunakan **lebih sedikit *field*** (3 *field* vs 4 *field*) dan **satu *encoder***, dapat mengungguli model kompleks yang mengandalkan penumpukan *field*.<br>3. **Analisis Komponen:** **Cross-modal Attention** menyumbang paling besar pada kinerja. Integrasi **Intra-modal Attention** (terutama DA pada Hu et al.'s) lebih lanjut meningkatkan akurasi.<br>4. **Pentingnya Token:** *Field* **Token** ($T$) menyumbang paling besar pada akurasi di antara eksperimen *single-field* dan *two-field*.<br><br>**Kontribusi Utama:**<br>• Mengusulkan arsitektur model *code search* yang efisien, hanya menggunakan tiga *field* kode dasar dan satu *encoder* terpadu.<br>• Merancang mekanisme *cross-modal attention* baru yang menggunakan vektor yang dapat dilatih alih-alih *max-pooling* untuk mempertahankan informasi penyelarasan kata.<br>• Menggabungkan *cross-modal* dan *intra-modal attention* untuk representasi akhir kode/kueri yang lebih komprehensif.<br><br>**Dampak:**<br>• Menyediakan model *code search* berkinerja tinggi yang **lebih ringan** dan **lebih efisien** secara komputasi (membutuhkan lebih sedikit parameter) dibandingkan model *multi-encoder* sebelumnya, meningkatkan aksesibilitas dan kecepatan pencarian kode. |

## 1. Pendahuluan & Masalah

Pencarian kode (*code search*) adalah tugas mendasar dalam rekayasa perangkat lunak untuk mengambil kode yang paling relevan dari repositori berdasarkan kueri bahasa alami. Meskipun model *deep learning* telah meningkatkan kinerja secara signifikan, mereka menghadapi beberapa tantangan yang terkait dengan kompleksitas arsitektur dan kehilangan informasi.

Studi-studi *multi-encoder* sebelumnya (*DeepCS, TabCS, FcarCS*) berupaya meningkatkan kinerja dengan:

1.  Mengurai kode menjadi beberapa *field* (Nama Metode, Token, API, AST) untuk meningkatkan token unik dan pembedaan kode.
2.  Menggunakan *encoder* yang berbeda (seperti LSTM atau CNN) untuk setiap *field* kode.
3.  Memanfaatkan matriks relasi untuk menghubungkan kode dan kueri.

Pendekatan ini menghasilkan perbaikan, tetapi menimbulkan masalah serius:

*   **Peningkatan Biaya Komputasi:** Penggunaan *multi-encoder* untuk *multi-field* menghasilkan lebih banyak parameter model dan meningkatkan biaya komputasi serta waktu pelatihan.
*   **Kerugian Informasi Penyelarasan:** Matriks relasi yang hanya diubah melalui operasi *max-pooling* (baris dan kolom demi baris) mengabaikan dan menghilangkan informasi penyelarasan kata yang esensial, yang berpotensi menyaring fitur penting secara keliru.

::: tip Solusi yang Diusulkan
Diusulkan **Model Penyelarasan Gabungan** yang menyederhanakan arsitektur dan meningkatkan mekanisme atensi. Model ini menggabungkan hanya **tiga *field* kode dasar** menjadi satu urutan dan menggunakan **satu *encoder***. Peningkatan penyelarasan dicapai dengan: (1) Mengubah matriks relasi menggunakan **vektor yang dapat dilatih** (bukan *max-pooling*). (2) Menggabungkan atensi **cross-modal** (untuk penyelarasan akurat) dan atensi **intra-modal** (untuk menyoroti kata-kata penting) untuk representasi akhir yang komprehensif.
:::

## 2. Metodologi

Model yang diusulkan ini terdiri dari lima modul utama yang berfokus pada penyederhanaan *encoding* dan penggabungan atensi.

### A. Word Embedding & Relation Matrix

1.  **Code Embedding yang Disederhanakan:** Hanya menggunakan tiga *field* kode ($C_{name}, C_{token}, C_{api}$). *Field*-*field* ini digabungkan menjadi satu urutan, dan kemudian diumpankan ke **satu lapisan *embedding* tunggal** ($E_c$).
    $$ C=E_c(C_{name} + C_{token} + C_{api}) $$
    Kueri $Q$ juga di-*embed* menjadi $Q \in \mathbb{R}^{m \times dim}$.
2.  **Relation Matrix:** Matriks relasi $a(C,Q)$ dihitung untuk menjembatani kesenjangan semantik antara kode dan kueri.
    $$ a(C,Q)=\tanh(CW_{relation}Q^T) $$

### B. Combined Attention Mechanism

Model ini menggabungkan *Cross-Modal* dan *Intra-Modal Attention* untuk mendapatkan bobot atensi yang lebih kaya informasi.

1.  **Cross-Modal Attention (Vektor yang Dapat Dilatih):** Matriks relasi $a(C,Q)$ ditransformasikan secara linier (bukan *max-pooling*) menggunakan dua variabel yang dapat dipelajari ($W_3$ dan $W_4$) untuk mendapatkan fitur berbobot *cross-modal* $a(C,Q)^{cq}$ (untuk kode) dan $a(C,Q)^{qc}$ (untuk kueri).
    $$ a(C,Q)^{cq}=C^TW_3a(C,Q) $$
    Pendekatan ini memitigasi kehilangan fitur penting yang disebabkan oleh *max-pooling*.
2.  **Intra-Modal Attention:** Memungkinkan model menyoroti kata-kata penting dalam modalitasnya sendiri. Dua mekanisme dieksplorasi:
    *   ***Self-based Attention* (SA):** Berfokus pada fitur modalitas itu sendiri.
        $$ a(C)=C^TW_5C $$
    *   ***Distance-based Attention* (DA):** **Menggunakan kembali Matriks Relasi** $a(C,Q)$ dengan normalisasi $l_2$ (baris dan kolom demi baris) dan pengkuadratan. DA secara eksplisit memasukkan informasi modalitas lain ke dalam fitur *intra-modal*, menciptakan efek *key and lock hole*.

### C. Aggregation & Optimization

1.  **Aggregation:** Fitur *cross-modal* dan *intra-modal* digabungkan ($fa(Q)$ dan $fa(C)$) melalui penjumlahan. Kemudian, fitur gabungan ini digunakan untuk memberikan bobot pada *embedding* akhir ($\tilde{C}, \tilde{Q}$) melalui lapisan *softmax* dan vektor yang dapat dilatih ($W_{hv}, W_{hq}$).
    $$ fa(Q)=\tanh(a(C,Q)^{cq}+Q^q) $$
    $$ \tilde{C}=a_vC $$
2.  **Optimization:** Model dilatih menggunakan **Bi-directional Ranking Loss** untuk secara bersamaan memaksimalkan jarak antara pasangan kode/kueri negatif ($\hat{c}, \hat{q}$) dan memaksimalkan kedekatan pasangan positif $(C, Q)$.
    $$ \text{loss}(C, Q) = \max[0, A-S(c, q) + S(\hat{c}, q)] + \max[0, A-S(c, q) + S(c, \hat{q})] $$

## 3. Detail Pengujian

### Dataset
*   **CSN (Code Search Net):** 394.497 pasangan pelatihan, 10.000 pasangan pengujian.
*   **Hu et al.'s:** 475.463 pasangan pelatihan, 10.000 pasangan pengujian.

### Baseline
**DeepCS, CARLCS-CNN, CARLCS-TS, UNIF, TabCS** (dengan dan tanpa AST), dan **FcarCS** (dengan dan tanpa AST). Model ini membandingkan diri secara langsung dengan model *multi-encoder* terbaik dari studi-studi sebelumnya.

### Metrik Evaluasi
*   **Top@K:** Persentase keberhasilan, di mana $K$ adalah 1, 5, atau 10.
    $$ \text{Top}@K=\frac{1}{Q}\sum_{q=1}^{Q}\begin{cases}1&\text{Rank}_{\sigma} \le K\\ 0&\text{Rank}_{\sigma}>K\end{cases} $$
*   **MRR (Mean Reciprocal Rank):** Rata-rata kebalikan peringkat.
    $$ \text{MRR} = \frac{1}{Q}\sum_{q=1}^{Q}\frac{1}{\text{Rank}_{c_q}} $$

## 4. Hasil Eksperimen

### RQ1. Perbandingan Kinerja
Model yang diusulkan mencapai kinerja tertinggi di kedua *dataset*, melampaui semua *baseline* yang menggunakan tiga *field* kode maupun empat *field* kode (termasuk AST).

| Models (CSN) | Top@1 | Top@5 | Top@10 | MRR |
| :--- | :--- | :--- | :--- | :--- |
| TabCS | 0.547 | 0.683 | 0.748 | 0.539 |
| TabCS w/o AST | 0.504 | 0.654 | 0.724 | 0.504 |
| **Ours** | **0.614** | **0.783** | **0.822** | **0.686** |

*   **CSN:** Model Kami mengungguli TabCS (model terbaik sebelumnya dengan 4 *field*) sebesar **12.2%** pada Top@1 dan **27.3%** pada MRR. Mengungguli TabCS w/o AST (3 *field*) sebesar **21.8%** pada Top@1.

| Models (Hu et al.'s) | Top@1 | Top@5 | Top@10 | MRR |
| :--- | :--- | :--- | :--- | :--- |
| FcarCS | 0.628 | 0.683 | 0.748 | 0.613 |
| FcarCS w/o AST | 0.609 | 0.757 | 0.823 | 0.593 |
| **Ours** | **0.687** | **0.850** | **0.886** | **0.733** |

*   **Hu et al.'s:** Model Kami mengungguli FcarCS (model terbaik sebelumnya dengan 4 *field*) sebesar **9.3%** pada Top@1 dan **19.5%** pada MRR.

### Analisis Kontribusi Fitur Kode
*   **Single Field:** $Ours(T)$ (Token) menunjukkan akurasi tertinggi di antara konfigurasi *single-field* di kedua *dataset*.
*   **Implikasi:** Hasil ini menunjukkan bahwa **desain model yang ditingkatkan** (terutama mekanisme atensi) memiliki ruang yang lebih besar untuk meningkatkan kinerja daripada sekadar menumpuk *field* kode tambahan (seperti AST) dan *multi-encoder*.

### RQ2. Kontribusi Komponen Atensi (Ablation Study)
Eksperimen ablasi pada CSN dan Hu et al.'s menunjukkan kontribusi setiap komponen.

| Models (CSN) | Top@1 | MRR | Keterangan |
| :--- | :--- | :--- | :--- |
| w/o intra-modal | 0.607 | 0.682 | Hanya Cross-modal (Vektor yang Dapat Dilatih) |
| Full Model SA | 0.614 | 0.686 | + Self-Attention |
| Full Model DA | 0.612 | 0.684 | + Distance-based Attention |

| Models (Hu et al.'s) | Top@1 | MRR | Keterangan |
| :--- | :--- | :--- | :--- |
| w/o intra-modal | 0.671 | 0.722 | Hanya Cross-modal (Vektor yang Dapat Dilatih) |
| Full Model SA | 0.675 | 0.724 | + Self-Attention |
| Full Model DA | **0.687** | **0.733** | + Distance-based Attention |

*   **Kontribusi Cross-Modal:** Model yang hanya menggunakan *cross-modal attention* yang ditingkatkan (w/o intra-modal) sudah mengungguli semua *baseline*, menunjukkan bahwa mekanisme transformasi matriks relasi yang baru sangat efektif.
*   **Kontribusi Intra-Modal:** Penggabungan *intra-modal attention* lebih lanjut meningkatkan kinerja. Pada *dataset* CSN, SA bekerja sedikit lebih baik, sementara pada *dataset* Hu et al.'s, **DA** menunjukkan keuntungan yang lebih signifikan ($\uparrow 1\%$ pada Top@1 dibandingkan SA). Hal ini dikaitkan dengan kemampuan DA untuk memanfaatkan informasi *cross-modal* dari matriks relasi, yang lebih efektif pada *dataset* yang memiliki lebih banyak token unik (Hu et al.'s).

## 5. Kesimpulan

Studi ini berhasil mengusulkan **Model Penyelarasan Gabungan** untuk *code search* yang mengatasi masalah kompleksitas dan kehilangan informasi pada model *multi-encoder* sebelumnya. Dengan menyederhanakan *encoding* kode menjadi satu urutan dari tiga *field* dasar dan memperkenalkan kombinasi atensi **cross-modal** (menggunakan vektor yang dapat dilatih) dan **intra-modal**, model mencapai kinerja *state-of-the-art* pada dua *dataset* utama. Model yang diusulkan ini membuktikan bahwa arsitektur yang efisien dan mekanisme atensi yang dirancang dengan cermat lebih unggul daripada penumpukan *field* dan *encoder* yang boros sumber daya.

::: info Dampak Praktis
Model ini menawarkan peningkatan signifikan dalam **efisiensi komputasi** dan **akurasi** untuk tugas *code search*. Penggunaan *field* yang lebih sedikit dan satu *encoder* membuat model ini lebih ringan, lebih mudah dilatih, dan diterapkan di lingkungan produksi dibandingkan model *multi-encoder* yang besar, sekaligus memberikan hasil pencarian yang lebih akurat.
:::