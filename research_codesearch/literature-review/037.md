---
title: Review Paper - HFEDR Fitur Hierarkis dan Reorganisasi Data untuk Code Search
description: Rangkuman paper tentang ekstraksi fitur hierarkis dan reorganisasi data untuk code search (The Journal of Systems and Software, 2024).
head:
  - - meta
    - name: keywords
      content: AI in SE, Code search, Transformer-based architecture, Hierarchical features, Data reorganization, Contrastive training
---

# 037 - Hierarchical features extraction and data reorganization for code search
Tautan (DOI) [10.1016/j.jss.2023.111896](https://doi.org/10.1016/j.jss.2023.111896)

**Penulis:** **Fan Zhang** $^{a*}$, **Manman Peng** $^{a}$, **Yuanyuan Shen** $^{a}$, **Qiang Wu** $^{a}$

**Afiliasi:**
* $^a$ College of Computer Science and Electronic Engineering, Hunan University, Changsha, 410082, China

**Kronologi:** Received: 2 April 2023 • Revised: 7 September 2023 • Accepted: 2 November 2023 • Available Online: 9 November 2023

<a href="https://www.scimagojr.com/journalsearch.php?q=19309&tip=sid" target="_blank"><img src="https://www.scimagojr.com/journal_img.php?id=19309" alt="SCImago Journal & Country Rank" /></a>

| Resume Eksekutif |
| :--- |
| **Publikasi:**<br>• **Jurnal:** The Journal of Systems and Software 208 (2024) 111896<br>• **Topik:** Meningkatkan kinerja *code search* berbasis arsitektur Transformer dengan memanfaatkan fitur hirarkis model dan menambah jumlah data pelatihan melalui reorganisasi data.<br><br>**Masalah & Solusi:**<br>• **Masalah 1 (Fitur Hierarkis Diabaikan):** Model Transformer (seperti GraphCodeBERT) menghasilkan fitur berlapis, di mana lapisan rendah menangkap representasi **tingkat rendah** dan lapisan tinggi menangkap **tingkat tinggi**. Pemanfaatan fitur ini diabaikan.<br>• **Masalah 2 (Keterbatasan Data):** Metode yang ada tidak mempertimbangkan peningkatan jumlah data pelatihan secara artifisial (pembuatan pasangan sampel tambahan) selama pelatihan.<br>• **Solusi:** Mengusulkan **HFEDR** (*Hierarchical Features Extraction and Data Reorganization*): (1) Mengekstrak **fitur hierarkis** (3 lapisan pertama dan 3 lapisan terakhir dari GraphCodeBERT) untuk representasi semantik multi-pandangan. (2) Melakukan **reorganisasi data** menjadi pasangan fitur *hierarchical-uncorrelated* dan *hierarchical-correlated*, yang secara efektif menambah data pelatihan. (3) Menggunakan **Contrastive Training** dengan *loss function* **CoSENT**. (4) Mengintegrasikan **BERT-whitening** pada tahap *retrieval* untuk reduksi dimensi dan mengurangi anisotropi.<br><br>**Contoh Penerapan:**<br>• Dievaluasi pada *dataset* **CodeSearchNet** (Ruby, JavaScript, Go, Python, Java, PHP), **CoSQA** (skenario dunia nyata), dan **AdvTest** (uji generalisasi).<br><br>**Metodologi:**<br>• **Ekstraksi Fitur:** Menggunakan **GraphCodeBERT** untuk mengambil fitur dari 3 lapisan pertama (tingkat rendah) dan 3 lapisan terakhir (tingkat tinggi).<br>• **Reorganisasi Data:** Tahap 1: Mengatur pasangan fitur asli (hierarkis-tidak berkorelasi). Tahap 2: Untuk setiap pasangan positif, membuat pasangan negatif tambahan dengan memasangkan fitur kueri positif tersebut dengan semua fitur kode *unmatched* di *batch* yang sama (hierarkis-berkorelasi).<br>• **Contrastive Training:** Menggunakan *loss function* **CoSENT** untuk mengoptimalkan model dengan akumulasi *loss* dari semua lapisan yang dipilih.<br>$$ \text{loss\_layer} = \log \left( 1+\sum_{\langle i,j\rangle \in \text{Pos}, \langle k,l\rangle \in \text{Neg}} e^{\tau (\cos(q_i,c_j)-\cos(q_k,c_l))} \right) $$<br>• **Retrieval:** Menerapkan **BERT-whitening** dan menghitung skor kecocokan dengan penjumlahan *cosine similarity* di semua lapisan yang dipilih: $score=\sum_{\text{layer}\_i\in L} \cos(Q_{\text{LowFea}_{\text{layer}\_i}}, C_{\text{LowFea}_{\text{layer}\_i}})$.<br><br>**Temuan Kunci:**<br>1. **Kinerja SOTA:** HFEDR mencapai kinerja SOTA pada semua *dataset* (AVG MRR $0.802$), mengungguli *baseline* seperti GraphCodeBERT dan Unixcoder.<br>2. **Kontribusi Modul:** Semua komponen (Reorganisasi data, Fitur Hierarkis, BERT-whitening) secara positif berkontribusi. Modul **Hierarchical-Correlated Pairs** adalah yang paling penting.<br>3. **Model-Free:** Komponen HFEDR terbukti efektif dan *model-free* saat diuji dengan arsitektur **CodeBERT**.<br><br>**Kontribusi Utama:**<br>• Pekerjaan pertama yang memanfaatkan **fitur hierarkis** dari model Transformer bimodal untuk *code search*.<br>• Mengusulkan metode **reorganisasi data** dua tahap untuk meningkatkan data pelatihan.<br>• Mengintegrasikan **BERT-whitening** untuk kinerja dan efisiensi *retrieval*.<br><br>**Dampak:**<br>• HFEDR meningkatkan akurasi *code search* secara signifikan, memberikan representasi semantik yang lebih kaya dan memanfaatkan sumber daya model Transformer yang sebelumnya diabaikan. |

## 1. Pendahuluan & Masalah

*Code search* adalah tugas mendasar dalam pengembangan perangkat lunak yang bertujuan mengambil *snippet* kode yang relevan berdasarkan kueri bahasa alami. Pendekatan SOTA saat ini sangat bergantung pada model *pre-training* berbasis Transformer (misalnya, CodeBERT dan GraphCodeBERT), yang telah meningkatkan kinerja secara substansial dibandingkan metode *deep learning* sebelumnya seperti LSTM dan Attention.

Meskipun model Transformer berhasil, terdapat dua keterbatasan utama dalam penelitian yang ada:

1.  **Pengabaian Fitur Hierarkis:** Model Transformer (seperti GraphCodeBERT) menghasilkan 12 lapisan fitur. Lapisan yang lebih rendah menangkap representasi **tingkat rendah** (fitur dasar), sementara lapisan yang lebih tinggi menangkap representasi **tingkat tinggi** (semantik kompleks). Penelitian yang ada gagal memanfaatkan sepenuhnya fitur hierarkis yang kaya dan komprehensif ini.
2.  **Kurangnya Peningkatan Data Pelatihan:** Metode yang ada tidak secara proaktif meningkatkan jumlah data pelatihan selama fase pelatihan untuk lebih meningkatkan kinerja model, padahal teknik konstruksi sampel tambahan di dalam *batch* adalah metode yang menjanjikan.

::: tip Solusi yang Diusulkan
Kami mengusulkan **HFEDR** (*Hierarchical Features Extraction and Data Reorganization*), sebuah metode baru yang mengeksploitasi fitur hierarkis model Transformer dan mereorganisasi data pelatihan asli. Secara spesifik, HFEDR mengekstrak fitur tingkat rendah dan tinggi dari GraphCodeBERT untuk representasi semantik multi-pandangan, dan kemudian mereorganisasi data pelatihan menjadi pasangan fitur *hierarchical-uncorrelated* dan *hierarchical-correlated* untuk meningkatkan jumlah data secara efektif.
:::

## 2. Metodologi

Kerangka kerja HFEDR dirancang untuk memanfaatkan kedalaman model Transformer dan memaksimalkan penggunaan data.

### A. Extracting Hierarchical Features
HFEDR menggunakan **GraphCodeBERT** sebagai *encoder*. Untuk mencapai representasi semantik multi-pandangan yang komprehensif:
*   **Fitur Tingkat Rendah (Low-level features):** Diambil dari **tiga lapisan pertama** (Layer 1, 2, 3) dari keluaran Transformer. Fitur ini menggambarkan informasi dasar.
*   **Fitur Tingkat Tinggi (High-level features):** Diambil dari **tiga lapisan terakhir** (Layer 10, 11, 12) dari keluaran Transformer. Fitur ini mengandung makna semantik yang lebih kompleks.

### B. Data Reorganization (Dua Tahap)
Strategi ini dirancang untuk meningkatkan jumlah data pelatihan dan memungkinkan model mempelajari hubungan semantik yang lebih baik.

1.  **Hierarchical-Uncorrelated Feature Pairs (Tahap 1):** Fitur hierarkis (dari 6 lapisan yang dipilih) diekstrak dari *batch* data pelatihan asli, yang terdiri dari pasangan positif (*matched*) dan negatif (*unmatched*) yang sudah diacak dan memiliki korelasi rendah.

2.  **Hierarchical-Correlated Feature Pairs (Tahap 2):** Untuk setiap lapisan, dari setiap pasangan fitur positif `<query, code, 1>` di Tahap 1, fitur kueri ($query_l$) dipasangkan dengan semua fitur kode yang *tidak cocok* ($code_l$) dalam *batch* yang sama untuk membuat pasangan fitur negatif baru. Ini secara signifikan meningkatkan jumlah pasangan pelatihan yang berkorelasi secara hierarkis dalam setiap *batch*.

### C. Contrastive Training
HFEDR menggunakan *loss function* **CoSENT** untuk mengoptimalkan parameter model. *Loss* dihitung untuk setiap lapisan, dan total *loss* adalah akumulasi dari *loss* di semua 6 lapisan yang dipilih. Tujuannya adalah memastikan *cosine similarity* dari pasangan positif ($\cos(q_i, c_j)$) lebih besar daripada pasangan negatif ($\cos(q_k, c_l)$).

Rumus untuk menghitung *loss* per lapisan adalah:
$$ \text{loss\_layer} = \log \left( 1+\sum_{\langle i,j\rangle \in \text{Pos}, \langle k,l\rangle \in \text{Neg}} e^{\tau (\cos(q_i,c_j)-\cos(q_k,c_l))} \right) $$
Di mana $\tau$ adalah *hyperparameter* (ditetapkan $\tau=20$). Total *loss* adalah $\text{loss\_all} = \sum_{\text{layer}\in \{1,2,3,10,11,12\}} \text{loss\_layer}$.

### D. Code Retrieval
Untuk mengurangi anisotropi dan dimensi vektor (menghemat ruang penyimpanan dan mempercepat *retrieval*), operasi **BERT-whitening** diterapkan pada fitur hierarkis yang diekstrak. Dimensi dikurangi menjadi 512. Skor kecocokan akhir dihitung sebagai penjumlahan *cosine similarity* dari vektor berdimensi rendah di semua lapisan yang dipilih:
$$ \text{score}=\sum_{\text{layer}\_i\in L} \cos(Q_{\text{LowFea}_{\text{layer}\_i}}, C_{\text{LowFea}_{\text{layer}\_i}}) $$

## 3. Detail Pengujian

### Dataset
*   **CodeSearchNet:** *Dataset* utama, mencakup enam bahasa pemrograman (Ruby, JavaScript, Go, Python, Java, PHP). Tugasnya adalah mengambil jawaban dari 1000 kandidat.
*   **CoSQA:** Untuk menguji kinerja dalam skenario dunia nyata.
*   **AdvTest:** Untuk menguji kemampuan generalisasi model.

### Baseline
Model SOTA yang dibandingkan meliputi ROBERTa, CodeBERT, **GraphCodeBERT**, CodeT5, SPT-Code, Plbart, **Unixcoder**, MoCoCS, dan SYNCOBERT.

### Metrik Evaluasi
Kinerja diukur menggunakan metrik kuantitatif utama:

*   **Mean Reciprocal Rank (MRR):**
    $$ \text{MRR} = \frac{1}{Q} \sum_{i=1}^{Q} \frac{1}{\text{rank}_i} $$
    Di mana $Q$ adalah jumlah kueri, dan $\text{rank}_i$ adalah posisi kode *snippet* yang benar dalam daftar hasil yang diberi peringkat. Skor MRR yang lebih tinggi menunjukkan kinerja yang lebih baik.

## 4. Hasil Eksperimen

### Efektivitas HFEDR (RQ1)
HFEDR mencapai kinerja *state-of-the-art* (SOTA) pada CodeSearchNet (AVG MRR $\mathbf{0.802}$) dibandingkan dengan semua *baseline*.

| Model | CoSQA (Python) | AdvTest (Python) | CodeSearchNet (AVG) |
| :--- | :--- | :--- | :--- |
| GraphCodeBERT | 0.684 | 0.352 | 0.774 |
| Unixcoder | 0.701 | 0.413 | 0.783 |
| MoCoCS | - | - | 0.782 |
| **HFEDR** | **0.752** | **0.445** | **0.802** |

### Kontribusi Komponen Kunci (RQ2)
Studi ablasi mengkonfirmasi bahwa setiap komponen berkontribusi pada peningkatan kinerja.

| Varian | AVG MRR | Penurunan dari HFEDR |
| :--- | :--- | :--- |
| **HFEDR** | **0.802** | - |
| w/o whitening | 0.787 | $\downarrow 0.015$ |
| w/o Uncor\_Pairs | 0.778 | $\downarrow 0.024$ |
| w/o Cor\_Pairs | 0.766 | $\downarrow 0.036$ |
| w/o Multi\_Lay | 0.782 | $\downarrow 0.020$ |

Komponen **Hierarchical-Correlated Pairs (w/o Cor\_Pairs)** menunjukkan penurunan terbesar dalam kinerja rata-rata, membuktikan bahwa reorganisasi data selama pelatihan adalah modul yang paling kritis.

### Dampak Pemilihan Lapisan (RQ4)
Membandingkan berbagai kombinasi lapisan menunjukkan bahwa menggunakan **tiga lapisan pertama dan tiga lapisan terakhir (First-Last-Three)** memberikan *trade-off* terbaik antara kinerja dan sumber daya komputasi (MRR $0.802$), yang lebih baik daripada hanya menggunakan satu lapisan terakhir (MRR $0.782$) atau kombinasi lapisan yang lebih sedikit.

### Komponen Model-Free (RQ6)
Mengganti GraphCodeBERT dengan **CodeBERT** dan melakukan studi ablasi yang sama menunjukkan bahwa HFEDR(CodeBERT) mengungguli CodeBERT asli (MRR $0.785$ vs $0.760$). Hal ini mengonfirmasi bahwa komponen HFEDR bersifat **model-free** dan dapat meningkatkan kinerja pada arsitektur Transformer lainnya.

## 5. Kesimpulan

Paper ini memperkenalkan **HFEDR**, sebuah metode baru untuk meningkatkan kinerja *code search* dengan memanfaatkan fitur hierarkis model Transformer dan reorganisasi data pelatihan. Dengan mengekstrak fitur tingkat rendah dan tinggi dan secara artifisial meningkatkan data pelatihan melalui pasangan fitur berkorelasi, HFEDR berhasil mencapai kinerja *state-of-the-art* pada *dataset* skala besar seperti CodeSearchNet.

Analisis mendalam memvalidasi kontribusi setiap komponen, terutama modul reorganisasi data berkorelasi, dan membuktikan efektivitas metode ini lintas arsitektur model (*model-free*).

::: info Dampak Praktis
HFEDR memberikan representasi semantik yang lebih kaya dan komprehensif untuk kode dan kueri, menghasilkan akurasi *code search* yang lebih tinggi. Ini secara langsung membantu pengembang dalam merekayasa perangkat lunak, meskipun ada tantangan di masa depan yang perlu diatasi terkait dengan konsumsi waktu dan penanganan kode/kueri yang panjang.
:::