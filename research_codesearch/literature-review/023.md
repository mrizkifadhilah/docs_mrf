---
title: Review Paper - xCoFormer untuk Code Retrieval yang Efisien dan Skalabel
description: Rangkuman paper tentang Model Representation Learning xCoFormer untuk Code Retrieval (Neurocomputing, 2024).
head:
  - - meta
    - name: keywords
      content: Code retrieval, Representation learning, Language models, xCoFormer, N-pair loss, TaG-Training
---

# 023 - On Representation Learning-based Methods for Effective, Efficient, and Scalable Code Retrieval
Tautan (DOI) [10.1016/j.neucom.2024.128172](https://doi.org/10.1016/j.neucom.2024.128172)

**Penulis:** **Celso França** ¹*, **Rennan C. Lima** ¹, **Claudio Andrade** ¹, **Washington Cunha** ¹, **Pedro O.S. Vaz de Melo** ¹'², **Berthier Ribeiro-Neto** ¹'², **Leonardo Rocha** ¹, **Rodrygo L.T. Santos** ¹, **Adriana Silvina Pagano** ¹, **Marcos André Gonçalves** ¹

**Afiliasi:**
* ¹ Federal University of Minas Gerais (UFMG), Belo Horizonte, MG, Brazil
* ² Google, Belo Horizonte, MG, Brazil

**Kronologi:** Received: 29 June 2023 • Revised: 23 March 2024 • Accepted: 30 June 2024 • Available Online: 6 July 2024

<a href="https://www.scimagojr.com/journalsearch.php?q=24807&tip=iss" target="_blank"><img src="https://www.scimagojr.com/journal_img.php?id=24807" alt="SCImago Journal & Country Rank" /></a>

| Resume Eksekutif |
| :--- |
| **Publikasi:**<br>• **Jurnal:** Neurocomputing 600 (2024) 128172<br>• **Topik:** Mengembangkan model *representation learning* untuk *Code Retrieval* yang memenuhi persyaratan **Efektivitas (Effectiveness - CRR-2)**, **Efisiensi (Efficiency - CRR-1)**, dan **Skalabilitas (Scalability - CRR-3)** secara simultan.<br><br>**Masalah & Solusi:**<br>• **Masalah:** Model *code retrieval* berbasis Transformer yang paling efektif (misalnya, CodeBERT dengan arsitektur *cross-encoder*) tidak efisien dan tidak skalabel untuk repositori kode besar karena memerlukan komputasi *forward pass* untuk setiap pasangan kueri-kode pada waktu pencarian (membutuhkan waktu hingga 50 jam untuk repositori 40 juta elemen).<br>• **Solusi:** Mengusulkan **xCoFormer** (*Explainable Co-Transformer*), model *representation learning* berbasis arsitektur **bi-encoder** yang dilatih secara interaktif menggunakan metode **TaG-Training** dan fungsi *loss* **N-Pair**.<br><br>**Contoh Penerapan:**<br>• **Mesin Pencari Kode *End-to-End*:** xCoFormer dirancang sebagai solusi mesin pencari yang dapat mengindeks repositori kode besar dan menjawab kueri dengan cepat (milidetik) dengan akurasi tinggi.<br><br>**Metodologi:**<br>• **Arsitektur:** Menggunakan arsitektur **Bi-Encoder** (Query Encoder dan Code Encoder, keduanya Transformer) untuk menghasilkan representasi vektor independen, memungkinkan *indexing* kode sebelumnya (memenuhi CRR-1 & CRR-3).<br>• **TaG-Training (*Turn-based Bargaining Game*):** Metode pelatihan interaktif di mana Query Encoder dan Code Encoder dioptimalkan secara bergantian dalam langkah pelatihan yang berurutan, meniru permainan bergantian dua pemain untuk mencapai konvergensi yang lebih efisien (meningkatkan CRR-1).<br>• **Fungsi *Loss***: Menggunakan **N-Pair Loss** (adaptasi dari *triplet loss* yang memungkinkan perbandingan bersama dengan $N-1$ sampel negatif dalam satu langkah pembaruan), yang lebih cocok untuk tugas peringkat IR (meningkatkan CRR-1 & CRR-2).<br>• **Modularitas:** Model dapat menggunakan model bahasa spesifik domain seperti UniXcoder sebagai *encoder* untuk peningkatan efektivitas lebih lanjut.<br><br>**Temuan Kunci:**<br>1. **Efisiensi dan Skalabilitas:** xCoFormer adalah $\mathbf{6}$ urutan besaran (**ribuan kali**) lebih cepat daripada CodeBERT pada waktu pencarian, menunjukkan kompleksitas logaritmik ($O(\log n)$) seiring peningkatan indeks, dan merupakan satu-satunya model yang sepenuhnya memenuhi CRR-1, CRR-2, dan CRR-3.<br>2. **Tradeoff Terbaik:** xCoFormer memberikan *cost-effectiveness tradeoff* terbaik: hampir seefektif CodeBERT (perbedaan MRR@10 $\approx 2\%$) tetapi dengan kecepatan pencarian yang sangat unggul.<br>3. **Keunggulan N-Pair Loss:** Substitusi N-Pair Loss dengan *triplet loss* menyebabkan penurunan efektivitas yang signifikan, membuktikan bahwa N-Pair Loss sangat penting untuk tugas *code retrieval* (penurunan efektivitas mencapai hingga $49.79\%$ di MRR@1).<br>4. **TaG-Training Efisien:** TaG-Training menghasilkan kecepatan pelatihan ($Speed-up$) hingga $\mathbf{1.5}$ kali lipat tanpa perbedaan statistik yang signifikan dalam efektivitas, dibandingkan pelatihan tanpa TaG-Training.<br>5. **Modularitas:** Integrasi dengan UniXcoder (sebagai *encoder*) menghasilkan peningkatan efektivitas lebih dari $\mathbf{33\%}$ dibandingkan penggunaan BERT/RoBERTa.<br><br>**Kontribusi Utama:**<br>• Mengembangkan **xCoFormer**, model *code retrieval* berbasis *representation learning* pertama yang memenuhi CRR-1, CRR-2, dan CRR-3 secara simultan.<br>• Memperkenalkan **TaG-Training**, metode pelatihan interaktif untuk *bi-encoder* yang meningkatkan efisiensi pelatihan.<br>• Mengadaptasi dan menunjukkan superioritas **N-Pair Loss** untuk tugas *code retrieval* dibandingkan *contrastive* atau *triplet loss*.<br><br>**Dampak:**<br>• Menyediakan solusi *code retrieval* yang **praktis** dan **dapat diterapkan** pada repositori kode skala besar, memecahkan hambatan efisiensi dan skalabilitas yang dihadapi oleh model *cross-encoder* yang sangat efektif. |

## 1. Pendahuluan & Masalah

Kebutuhan akan *code retrieval* (pencarian *snippet* kode yang relevan dengan kueri programmer) semakin penting seiring dengan meluasnya perangkat lunak. Tugas ini menantang karena harus menjembatani **kesenjangan semantik** antara maksud tingkat tinggi dalam bahasa alami (kueri) dan detail implementasi tingkat rendah dalam kode sumber. Pendekatan modern memanfaatkan model *representation learning* yang menggunakan dua *encoder* (satu untuk kueri dan satu untuk kode) untuk memproyeksikan kedua input ke dalam ruang vektor bersama.

Model berbasis Transformer, seperti **CodeBERT**, telah menunjukkan efektivitas tertinggi. Namun, CodeBERT menggunakan arsitektur **cross-encoder** yang memerlukan input pasangan (kueri, kode) dan tidak dapat menghasilkan representasi independen. Akibatnya, pada waktu pencarian (*search time*), model harus melakukan *forward pass* untuk setiap elemen koleksi kode, menjadikannya **tidak praktis** dan **tidak skalabel** untuk repositori yang besar (membutuhkan waktu lebih dari 50 jam untuk 40 juta elemen).

Oleh karena itu, solusi *code retrieval* yang praktis harus memenuhi setidaknya tiga Persyaratan *Code Retrieval* (CRRs):
*   **Efisiensi (CRR-1):** Memproses kueri dengan sumber daya dan waktu minimal.
*   **Efektivitas (CRR-2):** Kode yang relevan muncul di peringkat atas.
*   **Skalabilitas (CRR-3):** Mampu menangani repositori kode yang terus bertambah besar.

::: tip Solusi yang Diusulkan
Paper ini memperkenalkan **xCoFormer** (*Explainable Co-Transformer*), yang diklaim sebagai model *representation learning* pertama untuk *code retrieval* yang sepenuhnya memenuhi CRR-1, CRR-2, dan CRR-3, serta sebagian memenuhi CRR-4 (Explainability). Ini dicapai melalui arsitektur **bi-encoder** yang efisien, metode pelatihan **TaG-Training** yang interaktif, dan penggunaan fungsi *loss* **N-pair** yang terspesialisasi.
:::

## 2. Metodologi

Arsitektur xCoFormer dirancang sebagai kerangka kerja logis untuk merepresentasikan kueri dan kode dalam ruang vektor yang sama, memungkinkan proses IR dua langkah yang efisien.

### A. Arsitektur Bi-Encoder Berbasis Transformer

xCoFormer menggunakan arsitektur **bi-encoder** (Query Encoder dan Code Encoder), di mana kedua *encoder* adalah Transformer (12 *layer*, 768 dimensi, 12 *head* *self-attention*). Arsitektur ini memungkinkan:
1.  **Indexing:** Kode dienkode oleh Code Encoder dan diindeks sebelumnya.
2.  **Searching:** Query Encoder merepresentasikan kueri tunggal, dan kecocokan ditemukan melalui pencarian kesamaan (*similarity search*) dengan kompleksitas logaritmik ($O(\log n)$), memenuhi CRR-1 dan CRR-3.

### B. Learning Objective: N-Pair Loss

Untuk mengoptimalkan *encoder* agar menempatkan vektor kueri dekat dengan vektor kode yang relevan ($\mathbf{g}_i$) dan jauh dari vektor kode yang tidak relevan ($\mathbf{g}_j$), xCoFormer menggunakan adaptasi dari **N-pair Loss** yang menggeneralisasi *triplet loss*.
$$ \mathcal{L}(\mathbf{q}_i, \mathbf{c}_i, \{\mathbf{c}_j\}_{j \ne i}) = -\log\frac{\exp(\mathbf{f}_i^T\mathbf{g}_i)}{\exp(\mathbf{f}_i^T\mathbf{g}_i) + \sum_{j \ne i}^N \exp(\mathbf{f}_i^T\mathbf{g}_j)} $$
Di mana $\mathbf{f}_i = f(\mathbf{q}_i;\theta)$ dan $\mathbf{g}_i = g(\mathbf{c}_i;\sigma)$ adalah fungsi *embedding* kueri dan kode. Keunggulan N-pair Loss adalah ia menarik sampel positif sambil mendorong $(\mathbf{N}-1)$ sampel negatif sekaligus dalam satu langkah pembaruan, menjadikannya lebih efisien dan efektif daripada *triplet loss* untuk tugas peringkat dengan banyak item tidak relevan.

### C. TaG-Training (*Turn-based Bargaining Game Training*)

TaG-Training adalah strategi pelatihan kooperatif baru di mana parameter Query Encoder dan Code Encoder dioptimalkan secara bergantian (*alternating steps*) dan bukan secara serentak.
*   **Proses:** Pada langkah $t$, hanya Query Encoder yang dioptimalkan. Pada langkah $t+1$, hanya Code Encoder yang dioptimalkan.
*   **Tujuan:** Mencegah osilasi parameter dan konvergensi sub-optimal yang terjadi ketika kedua *encoder* diperbarui secara bersamaan (misalnya, *Code Encoder* bergerak mendekati posisi kueri saat ini, tetapi pada saat yang sama *Query Encoder* telah memindahkan posisi kuerinya ke tempat lain).
*   **Dampak:** Mempercepat konvergensi dan mengurangi jumlah pembaruan parameter, sehingga meningkatkan Efisiensi Pelatihan (CRR-1).

### D. Explainability (CRR-4)

xCoFormer memanfaatkan mekanisme *attention* Transformer untuk mencapai *explainability* sebagian. Skor *attention* secara intrinsik mencerminkan pentingnya *token* input. Dengan menganalisis pola *attention* yang dilatih bersama (*co-trained*), model dapat memberikan "petunjuk" tentang mengapa kode tertentu diambil untuk kueri tertentu (misalnya, menyorot kata kunci dalam kueri dan kata kunci terkait dalam tanda tangan atau variabel kode).

## 3. Detail Pengujian

### Dataset
Tiga dataset *desc-code pair* digunakan, dibagi menjadi 5-folds untuk *cross-validation* yang ketat:
*   **Javascript (Small):** 138,625 sampel (CodeXGLUE).
*   **Python (Med):** 447,791 sampel (CodeXGLUE).
*   **Java (Large):** 2,149,121 sampel.

### Baseline Models
Model *representation learning* terbaru digunakan sebagai *baseline*: **1D-CNN**, **NBOW**, **Bi-GRU**, **Self-Att**, **S-BERT** (*bi-encoder* berbasis BERT), dan **CodeBERT** (*cross-encoder*).

### Metrik Evaluasi
**Recall@k** dan **Mean Reciprocal Rank (MRR@k)** digunakan, di mana $k \in \{1, 5, 10\}$. Skenario yang sangat menantang digunakan: hanya satu kode berpasangan yang dianggap relevan untuk setiap kueri.
$$ \mathbf{MRR} = \frac{1}{|Q|}\sum_{j=1}^{|Q|}\frac{1}{Rank_{j}} $$
Di mana $Rank_j$ adalah posisi peringkat kode yang paling relevan untuk kueri ke-$j$.

## 4. Hasil Eksperimen

### RQ1 & RQ2: Efisiensi Pencarian dan Skalabilitas

| Model | Efektivitas (MRR@10, Java) | Waktu Pencarian (s, Log Scale) | Kompleksitas Skala |
| :--- | :--- | :--- | :--- |
| **xCoFormer** | **0.371** | $\mathbf{10^{-2}}$ | $\mathbf{O(\log n)}$ |
| CodeBERT | 0.391 | $10^3$ | $O(n)$ (Linier) |
| S-BERT | 0.268 | $10^{-2}$ | $O(\log n)$ |
| Bi-GRU | 0.219 | $10^{-3}$ | $O(\log n)$ |

**Analisis:** xCoFormer adalah model kualitas tertinggi (MRR@10 tertinggi di antara *bi-encoder*). Kompleksitas skalanya logaritmik, menjadikannya sangat skalabel. CodeBERT, meskipun sedikit lebih efektif, memiliki kompleksitas linier, menjadikannya **tidak skalabel** dan **tidak efisien** untuk repositori besar. xCoFormer $\mathbf{6}$ urutan besaran ($\approx 10^6$) lebih cepat daripada CodeBERT pada waktu pencarian, memberikan *cost-effectiveness tradeoff* terbaik.

### RQ3: Efektivitas Model

CodeBERT tetap menjadi yang paling efektif (MRR@10 tertinggi), tetapi xCoFormer sangat kompetitif.
*   **Java (Large):** CodeBERT 0.391 vs. xCoFormer 0.371.
*   **Python (Med):** CodeBERT 0.558 vs. xCoFormer 0.510.

Perbedaan efektivitas sekitar $\mathbf{2\%}$ hingga $\mathbf{5\%}$ ini tidak mencerminkan perbedaan praktis yang substansial. Analisis CDF (Cumulative Distribution Function) menunjukkan bahwa $P(rCP\le3) \approx 0.8$ pada CodeBERT dan xCoFormer, yang berarti pencari rata-rata hanya perlu memeriksa hingga posisi ke-3 untuk menemukan kode yang relevan.

### RQ6: Peringkat Pendekatan Berdasarkan CRRs

Menggunakan *Multi-Attribute Utility Theory* (MAUT), xCoFormer adalah yang terbaik dalam memenuhi semua persyaratan secara bersamaan:

| Peringkat | Uniform-weighted Ranking | Effectiveness-biased Ranking |
| :--- | :--- | :--- |
| **#1** | **xCoFormer** | **xCoFormer** |
| #2 | S-BERT | S-BERT |
| #3 | Self-Att | Self-Att |
| #4 | CodeBERT | CodeBERT |

xCoFormer adalah satu-satunya model yang sepenuhnya memenuhi CRR-1 (Efisiensi), CRR-2 (Efektivitas), dan CRR-3 (Skalabilitas) sambil sebagian memenuhi CRR-4 (Explainability).

### RQ7: Efisiensi Pelatihan (Ablasi)

| Komponen Dihapus | Dampak pada MRR@1 (Java) | Dampak pada Waktu Pelatihan (Java) |
| :--- | :--- | :--- |
| **N-pair Loss** | Menurun $18.8\%$ (Menjadi $25.7$) | - |
| **- TaG-Training** | Menurun $1.6\%$ (Menjadi $30.0$) | Menurun $33\%$ (Waktu $1.5\times$ Lebih Lama) |

**Analisis:** N-pair Loss adalah komponen paling penting untuk efektivitas. TaG-Training memberikan *speedup* pelatihan rata-rata $\mathbf{1.5\times}$ tanpa kerugian efektivitas yang signifikan, membuktikan bahwa ia mengoptimalkan konvergensi secara efisien.

### RQ8: Modularitas Encoder (Model Bahasa Kode)

| Encoder xCoFormer (Java) | MRR@1 | Peningkatan vs. BERT |
| :--- | :--- | :--- |
| **UniXcoder** | **36.1** | **$\approx 12.5\%$** |
| CodeBERT | 34.9 | $\approx 8.7\%$ |
| BERT | 32.1 | — |
| **UniXcoder (Python)** | **51.6** | **$\approx 38.7\%$** |

**Analisis:** Menggunakan model bahasa spesifik domain (UniXcoder) sebagai *encoder* modular di dalam xCoFormer secara signifikan meningkatkan efektivitas (rata-rata lebih dari $\mathbf{33\%}$ di Python dan JavaScript) dibandingkan model tujuan umum (BERT/RoBERTa).

## 5. Kesimpulan

xCoFormer adalah solusi *code retrieval* novel yang dirancang untuk menjadi **efektif, efisien, skalabel,** dan sebagian **dapat dijelaskan**. Ini adalah solusi *code retrieval* berbasis *representation learning* pertama yang diklaim memenuhi ketiga persyaratan utama (CRR-1, CRR-2, CRR-3) secara simultan, memecahkan masalah efisiensi dan skalabilitas yang dihadapi oleh model *cross-encoder* yang paling efektif.

Inovasi utamanya adalah arsitektur *bi-encoder* yang dikombinasikan dengan pelatihan interaktif **TaG-Training** (untuk efisiensi pelatihan) dan fungsi *loss* **N-pair** (untuk efektivitas). xCoFormer menunjukkan *cost-effectiveness tradeoff* terbaik, berkinerja seefektif CodeBERT tetapi **ribuan kali lebih cepat** pada waktu pencarian.

::: info Dampak Praktis
Dampak praktis xCoFormer adalah memungkinkan pengembangan mesin pencari kode *end-to-end* yang **praktis dan skalabel** untuk repositori kode yang sangat besar. Dengan kompleksitas pencarian logaritmik, xCoFormer dapat menyediakan jawaban dalam milidetik untuk kueri pengembang, secara signifikan meningkatkan efisiensi pengembangan perangkat lunak dalam skenario dunia nyata.
:::