---
title: Review Paper - Inisialisasi Model Terspesialisasi dan Optimasi Arsitektur untuk Few-Shot Code Search
description: Rangkuman paper tentang Kerangka Few-Shot Code Search Dua Tahap SMIAO (Information and Software Technology, 2025).
head:
  - - meta
    - name: keywords
      content: Code search, Code understanding, Information retrieval, Neural architecture search, Al for software engineering
---

# 015 - Specialized model initialization and architecture optimization for few-shot code search
[https://doi.org/10.1016/j.infsof.2024.107571]

**Penulis:** **Fan Zhang** ᵃ'ᵇ, **Qiang Wu** ᵃ, **Manman Peng** ᵃ, **Yuanyuan Shen** ᶜ

**Afiliasi:**
* ᵃ College of Computer Science and Electronic Engineering, Hunan University, Changsha, 410082, China
* ᵇ Hunan Provincial Key Laboratory of Blockchain Infrastructure and Application, Hunan University, Changsha, 410082, China
* ᶜ School of Data Science and Technology, North University of China, Taiyuan, 030051, China

**Kronologi:** Received: 8 March 2024 • Revised: 16 August 2024 • Accepted: 1 September 2024 • Available Online: 4 September 2024

<a href="https://www.scimagojr.com/journalsearch.php?q=18732&tip=sid" target="_blank"><img src="https://www.scimagojr.com/journal_img.php?id=18732" alt="SCImago Journal & Country Rank" /></a>

| Resume Eksekutif |
| :--- |
| **Publikasi:**<br>• **Jurnal:** Information and Software Technology, Vol 177 (2025), Art. 107571<br>• **Topik:** Peningkatan kinerja *Code Search* untuk bahasa pemrograman spesifik domain (*domain-specific*) dengan data pelatihan terbatas (*Few-Shot*).<br><br>**Masalah & Solusi:**<br>• **Masalah:** Metode *Code Search* yang ada tidak efektif untuk bahasa *domain-specific* (misalnya, Solidity, SQL) karena **keterbatasan data pelatihan** (*few-shot*). Metode SOTA (*MAML/CDCS*) adalah *model-agnostic* dan tidak dirancang khusus untuk *code search*, serta rentan terhadap *overfitting* karena ketidakseimbangan antara parameter yang dapat dilatih dan jumlah data yang sedikit.<br>• **Solusi:** Mengusulkan **SMIAO** (*Specialized Model Initialization and Architecture Optimization*), sebuah kerangka *few-shot code search* **dua tahap** yang mengatasi masalah data terbatas dan *overfitting*.<br><br>**Contoh Penerapan:**<br>• **Code Search Few-Shot pada DSL:** SMIAO diuji pada **SQL** dan **Solidity** untuk simulasi skenario *few-shot* dengan ukuran data bervariasi (100 hingga set penuh).<br>• **Kinerja Unggul:** SMIAO melampaui SOTA CDCS secara signifikan: $\mathbf{+7.07\%}$ MRR dan $\mathbf{+9.38\%}$ Recall@1 pada SQL, serta $\mathbf{+5.42\%}$ MRR dan $\mathbf{+8.37\%}$ Recall@1 pada Solidity (Rata-rata).<br><br>**Metodologi:**<br>• **Tahap 1 (Inisialisasi Model Terspesialisasi):** (a) Mengidentifikasi set data bahasa *mainstream* yang paling dekat secara semantik dengan bahasa target *few-shot* menggunakan **Wasserstein Distance**. (b) Memperkaya set data yang terdekat dengan **Hard Samples** (kueri yang serupa tetapi tidak cocok) untuk meningkatkan diskriminasi. (c) Melatih model **Adapter-GraphCodeBERT** menggunakan set data yang menantang ini untuk mendapatkan parameter terinisialisasi yang baik (*well-initialized*).<br>• **Tahap 2 (Optimasi Arsitektur):** (a) Merancang *search space* untuk Adapter-GraphCodeBERT yang terinisialisasi. (b) Menggunakan teknik **Neural Architecture Search (NAS) One-Shot** untuk mengoptimalkan posisi dan kuantitas modul *Adapter* di dalam lapisan GraphCodeBERT. Ini mencegah *overfitting* dan meningkatkan kinerja arsitektur.<br><br>**Temuan Kunci:**<br>1. **Kinerja SOTA:** SMIAO mencapai kinerja tertinggi di semua skenario *few-shot* (termasuk *zero-shot*), membuktikan efektivitas inisialisasi yang disesuaikan dan NAS.<br>2. **Peran Wasserstein Distance:** Menggunakan set data yang paling dekat secara semantik (*Java* untuk SQL/Solidity) penting, memberikan peningkatan kinerja yang signifikan dibandingkan yang terjauh.<br>3. **Keunggulan NAS:** Optimasi NAS meningkatkan kinerja hingga $\mathbf{4.17\%}$ MRR (dibandingkan *worst subnet*) dan mengatasi *overfitting* dengan membatasi parameter yang dapat dilatih.<br><br>**Kontribusi Utama:**<br>• Mengusulkan kerangka SMIAO dua tahap untuk *few-shot code search*.<br>• Metode inisialisasi model terspesialisasi yang menggunakan **Wasserstein Distance** dan **Hard Samples**.<br>• Menggunakan teknik **NAS One-Shot** untuk mengoptimalkan arsitektur *Adapter-GraphCodeBERT* untuk skenario *few-shot*.<br><br>**Dampak:**<br>• Meningkatkan secara drastis kinerja *code search* di bahasa *domain-specific* dengan data terbatas, membuat tugas-tugas *AI for software engineering* lebih layak dan efektif. |

## 1. Pendahuluan & Masalah

*Code search* adalah tugas fundamental dalam rekayasa perangkat lunak, yang bertujuan untuk mencocokkan kueri bahasa alami dengan potongan kode yang relevan. Keberhasilannya meningkatkan efisiensi pengembang dan kinerja tugas-tugas cerdas kode lainnya (*code intelligent tasks*) seperti pembuatan kode dan perbaikan program.

Upaya-upaya sebelumnya didominasi oleh teknik *deep learning* (misalnya, LSTM, Transformer, *pre-training models* seperti CodeBERT/GraphCodeBERT) yang sukses di bahasa pemrograman *mainstream* yang memiliki **data pelatihan yang melimpah**.

Namun, *code search* menjadi tantangan signifikan untuk **bahasa pemrograman spesifik domain** (*DSL*) seperti Solidity dan SQL, di mana data pelatihan yang berlabel sangat terbatas (*few-shot*). Pendekatan *few-shot* SOTA sebelumnya, **CDCS**, menggunakan *meta-learning* MAML, yang memiliki dua keterbatasan utama:
1.  **Generalisitas MAML:** MAML adalah metode inisialisasi *model-agnostic* yang tidak dirancang spesifik untuk *code search*.
2.  **Overfitting:** *Fine-tuning* seluruh parameter model pada data *few-shot* yang terbatas rentan terhadap *overfitting*, mengurangi kemampuan generalisasi model dalam skenario dunia nyata.

::: tip Solusi yang Diusulkan
Kami mengusulkan **SMIAO** (*Specialized Model Initialization and Architecture Optimization*), kerangka *few-shot code search* **dua tahap**. Tahap pertama fokus pada **Inisialisasi Model Terspesialisasi** (mengidentifikasi set data terdekat secara semantik dan menggunakan *hard samples* untuk melatih *Adapter-GraphCodeBERT*). Tahap kedua berfokus pada **Optimasi Arsitektur** menggunakan **Neural Architecture Search (NAS) One-Shot** untuk menyesuaikan posisi dan kuantitas modul *Adapter* agar sesuai dengan data *few-shot* target, secara efektif mencegah *overfitting* dan meningkatkan kinerja.
:::

## 2. Metodologi

Kerangka SMIAO terdiri dari dua tahap utama: inisialisasi model terspesialisasi dan optimasi arsitektur NAS.

### A. Specialized Model Initialization (Tahap 1)

Tujuan tahap ini adalah memperoleh parameter terinisialisasi yang baik ($W_{init}$) untuk model target.

1.  **Mengidentifikasi PL Terdekat:** Kami menggunakan **Wasserstein Distance (WD)** untuk mengukur jarak semantik antara set data bahasa target *few-shot* ($T$) dengan set data bahasa *mainstream* ($S$) dari CodeSearchNet. Set data yang paling kecil WD-nya dianggap paling dekat secara semantik.

$$W(T_{sem}, S_{sem}) = \inf_{\gamma \in \mathcal{F}(T_{sem}, S_{sem})} \int_{\mathcal{T} \times \mathcal{S}} t\_sem \cdot s\_sem \, d\gamma(t\_sem, s\_sem)$$

2.  **Memperkenalkan Hard Samples:** Set data bahasa *mainstream* terdekat diperkaya dengan *hard samples*. *Hard samples* dibuat dengan mengambil pasangan kueri-kode yang cocok (*D\_match*), lalu menemukan **kueri terdekat** (*query\_near*) yang berbeda dari kueri aslinya (menggunakan pencarian *nearest neighbour*). Pasangan $\langle query\_near, code, 0\rangle$ (tidak cocok) yang dihasilkan menjadi *hard samples* yang sulit dibedakan oleh model.
3.  **Initialization Training:** Model **Adapter-GraphCodeBERT** (*Adapter modules* disisipkan ke dalam setiap lapisan *Transformer*) dilatih menggunakan set data *D\_Hard* (gabungan *D\_match* dan *D\_unmatch*). Pada tahap ini, **semua parameter** (GraphCodeBERT dan Adapter) dilatih untuk mendapatkan inisialisasi yang optimal.

### B. NAS for Architecture Optimization (Tahap 2)

Tahap ini menggunakan NAS untuk mengoptimalkan penempatan modul *Adapter* yang kecil di dalam *GraphCodeBERT* yang telah diinisialisasi. Modul *Adapter* memiliki parameter yang sedikit, sehingga *fine-tuning* hanya pada *Adapter* dapat mencegah *overfitting*.

1.  **Constructing Supernet:** *Supernet* mencakup semua arsitektur yang mungkin. Untuk setiap lapisan *GraphCodeBERT* ($G_i$), ada dua pilihan: memasukkan *Adapter* ($A_i$) atau tidak. *Search space* total adalah $2^{12}$ (untuk 12 lapisan *GraphCodeBERT*).

$$S=\bigoplus_{i=1}^{12} \{G_{i}(x), A_{i}(G_{i}(x))\}$$

2.  **Training Supernet:** Menggunakan teknik **One-Shot** yang efisien. Hanya *supernet* yang dilatih sekali dengan mengambil sampel *subnet* secara acak di setiap langkah pelatihan. Selama pelatihan ini, parameter *GraphCodeBERT* ($W_{init\_GBT}$) **dibekukan**, sementara hanya parameter *Adapter* ($W_{sub}$) yang dilatih/diperbarui.
3.  **Finding an Optimal Subnet:** Setelah pelatihan *supernet*, *random search* dilakukan pada *validation set* untuk mengidentifikasi arsitektur *subnet* terbaik ($N_{best}$) yang memiliki kinerja tertinggi.
4.  **Retraining the Optimal Subnet:** *Subnet* terbaik diinstansiasi dengan parameter yang diwariskan dari Tahap 1, dan di-*retrain* menggunakan set data *few-shot* target ($D_{train}$). *Retraining* ini memastikan model disesuaikan secara optimal untuk tugas *few-shot* spesifik.

## 3. Detail Pengujian

### Dataset
Eksperimen dilakukan pada set data publik Few-Shot Code Search [21] yang mencakup bahasa **SQL** dan **Solidity**. Berbagai ukuran set pelatihan digunakan untuk mensimulasikan skenario *few-shot* (misalnya, 100, 500, 1000 sampel).

### Baseline
*   **CDCS:** SOTA Few-Shot (berbasis MAML).
*   **ROBERTa:** Model *pre-trained* NLP.
*   **CodeBERT, GraphCodeBERT, UniXcoder:** Model *pre-trained* kode.

### Metrik Evaluasi
*   **Mean Reciprocal Rank (MRR):**
$$MRR=\frac{1}{Q}\sum_{i=1}^{Q}\frac{1}{rank_{i}}$$
*   **Recall@1:** Mengukur frekuensi hasil yang relevan muncul di posisi pertama.

## 4. Hasil Eksperimen

### RQ1: Efektivitas SMIAO

| Language | Metrik | CDCS (AVG) | SMIAO (AVG) | Peningkatan SMIAO |
| :--- | :--- | :--- | :--- | :--- |
| **SQL** | MRR | 0.792 | **0.848** | $\mathbf{+7.07\%}$ |
| | Recall@1 | 0.703 | **0.769** | $\mathbf{+9.38\%}$ |
| **Solidity** | MRR | 0.663 | **0.699** | $\mathbf{+5.42\%}$ |
| | Recall@1 | 0.573 | **0.621** | $\mathbf{+8.37\%}$ |

**Analisis:** SMIAO secara konsisten mencapai kinerja SOTA di semua skenario *few-shot* dan *zero-shot*, membuktikan bahwa inisialisasi model terspesialisasi dan optimasi arsitektur NAS secara signifikan lebih efektif daripada *meta-learning* (CDCS) dan *fine-tuning* langsung.

### RQ2: Kontribusi Komponen Kunci (MRR Rata-Rata)

| Komponen Dihilangkan | SQL (Penurunan MRR vs SMIAO) | Solidity (Penurunan MRR vs SMIAO) |
| :--- | :--- | :--- |
| **w/o NAS optimizing** | 2.66% (0.826 vs 0.848) | 2.79% (0.680 vs 0.699) |
| **w/o hard samples** | 4.95% (0.808 vs 0.848) | 7.20% (0.652 vs 0.699) |
| **w/o closest datasets** | 2.91% (0.824 vs 0.848) | 3.55% (0.675 vs 0.699) |

**Analisis:**
*   **Hard Samples** adalah komponen inisialisasi paling penting, yang meningkatkan diskriminasi model secara substansial.
*   Penggunaan **Closest Datasets** yang diukur dengan **Wasserstein Distance** juga krusial untuk inisialisasi yang efektif.
*   **NAS Optimizing** memberikan peningkatan kinerja yang stabil dan diperlukan untuk mengatasi *overfitting* yang disebabkan oleh arsitektur yang tidak seimbang (dibandingkan *worst subnet*, SMIAO meningkat hingga 4.17% MRR).

### RQ4: Efisiensi Waktu

Meskipun menggunakan NAS, SMIAO dirancang untuk efisien waktu (menggunakan One-Shot). Waktu komputasi dibagi rata-rata: Inisialisasi Model (54.70%), Pelatihan Supernet (29.79%), Pencarian Subnet (6.26%), dan *Retraining* (9.25%). Waktu total NAS optimasi (*Supernet Training* + *Searching* + *Retraining*) dapat diterima (maksimum $\approx 2.67$ jam GPU) dan seimbang dengan manfaat kinerja yang diberikan.

### RQ5 & RQ6: Kompatibilitas Model dan Adapter

*   **Arsitektur Model:** SMIAO lebih kompatibel dengan model berarsitektur **Transformer Encoder** (seperti GraphCodeBERT dan CodeBERT) dibandingkan model *autoregressive* (GPT-2, CodeGPT), karena tugas *code retrieval* sangat bergantung pada *semantic embedding* yang lebih baik ditangkap oleh *encoder*.
*   **Teknologi Adapter:** SMIAO kompatibel dengan berbagai teknologi *adapter* (Bottleneck, LoRA, Compacter, IA$^3$), yang membuktikan bahwa peningkatan kinerja SMIAO didorong oleh strategi dua tahapnya, bukan hanya jenis *adapter* tertentu.

## 5. Kesimpulan

Paper ini berhasil menyajikan **SMIAO**, kerangka kerja *few-shot code search* dua tahap yang terdiri dari **Inisialisasi Model Terspesialisasi** (menggunakan Wasserstein Distance dan Hard Samples) dan **Optimasi Arsitektur NAS** (*One-Shot*). SMIAO secara signifikan mengatasi masalah *overfitting* dan keterbatasan data pada bahasa pemrograman spesifik domain. Hasilnya membuktikan SMIAO melampaui metode SOTA sebelumnya (CDCS) dengan margin yang substansial, menjadikan *code search* di lingkungan data terbatas menjadi lebih andal dan berkinerja tinggi.

::: info Dampak Praktis
SMIAO memberikan metodologi *fine-tuning* yang terstruktur dan terbukti efektif untuk model *pre-trained* dalam skenario *few-shot*. Keberhasilannya membuka jalan bagi pengembangan tugas-tugas *AI for software engineering* lainnya (*code generation*, *defect detection*) di domain spesifik, yang mana data pelatihan berlabel sangat langka dan mahal untuk diperoleh.
:::