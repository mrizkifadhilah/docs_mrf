---
title: Review Paper - deGraphCS untuk Neural Code Search
description: Rangkuman paper tentang deGraphCS Embedding Variable-based Flow Graph untuk Neural Code Search (ACM Transactions on Software Engineering and Methodology, 2023).
head:
  - - meta
    - name: keywords
      content: Intermediate representation, graph neural networks, code search, deep learning, LLVM IR
---

# 057 - deGraphCS: Embedding Variable-based Flow Graph for Neural Code Search
Tautan (DOI) [https://doi.org/10.1145/3546066]

**Penulis:** **Chen Zeng** $^{1}$, **Yue Yu** $^{1*}$, **Shanshan Li** $^{1*}$, **Xin Xia** $^{2}$, **Zhiming Wang** $^{1}$, **Mingyang Geng** $^{1}$, **Linxiao Bai** $^{1}$, **Wei Dong** $^{1}$, **Xiangke Liao** $^{1}$

**Afiliasi:**
* $^{1}$ School of Computer, National University of Defense Technology, China
* $^{2}$ College of Computer Science and Technology, Zhejiang University, China

**Kronologi:** Received: 22 December 2020 • Revised: 7 May 2022 • Accepted: 20 May 2022 • Available Online: March 2023

<a href="https://www.scimagojr.com/journalsearch.php?q=18121&tip=sid" target="_blank"><img src="https://www.scimagojr.com/journal_img.php?id=18121" alt="SCImago Journal & Country Rank" /></a>

| Resume Eksekutif |
| :--- |
| **Publikasi:**<br>• **Jurnal:** ACM Transactions on Software Engineering and Methodology 32, 2 (March 2023)<br>• **Topik:** Peningkatan akurasi *code search* dengan merepresentasikan kode sumber sebagai *variable-based flow graph* berbasis *Intermediate Representation* (IR) untuk menangkap semantik mendalam.<br><br>**Masalah & Solusi:**<br>• **Masalah 1 (Kesenjangan Sintaks-Semantik):** Representasi kode tradisional (seperti Token atau AST/S-CFG) gagal menangkap semantik mendalam; kode dengan sintaks berbeda bisa memiliki semantik yang sama (false negative), dan kode dengan sintaks serupa bisa memiliki semantik yang berbeda (false positive).<br>• **Masalah 2 (Integrasi Fitur):** Metode multimodal yang ada (seperti MMAN) tidak menggabungkan fitur token, AST, dan CFG secara efektif, memberikan peningkatan kinerja yang tidak signifikan.<br>• **Solusi:** Mengusulkan **DEGRAPHCS** (*Deep Graph for Code Search*), yang mengubah kode sumber menjadi **Variable-based Flow Graph (VFG)** berdasarkan **LLVM IR** (Intermediate Representation). VFG secara intrinsik mengintegrasikan **token, *data flow*, dan *control flow*** pada granularitas variabel (lebih halus dari granularitas *statement*) untuk representasi semantik yang lebih akurat.\\<br>**Contoh Penerapan:**<br>• Diuji pada dataset besar yang dikumpulkan dari GitHub, terdiri dari **41.152 cuplikan kode C**.<br>• Model ini diuji terhadap *state-of-the-art* (SOTA) menggunakan evaluasi otomatis (SuccessRate@k, MRR) dan evaluasi manual oleh 5 programmer berpengalaman.\\<br>**Metodologi:**<br>• **Representasi Kode:** Kode diubah menjadi **LLVM IR**, dan dari situ **VFG** dibangun. Node VFG adalah token (variabel, *opcode*, label), dan *edge* merepresentasikan *data dependencies* (solid blue) dan *control dependencies* (dotted red).<br>• **Optimasi Graf:** Mekanisme **Graph Optimization (GO)** dirancang untuk menghilangkan node redundan (e.g., *opcode* trivial, register sementara, instruksi terkait alamat/konversi) dan mengompresi blok kontrol, mengurangi node VFG hingga **51.88%** tanpa mengubah semantik.<br>• **Model Embedding:** VFG di-*embed* menggunakan **Improved Gated Graph Neural Network (GGNN)** dengan mekanisme *Attention* untuk menghasilkan vektor kode $h_{vfg}$.<br>• **Representasi Kueri:** Deskripsi komentar di-*embed* menggunakan **LSTM** dengan mekanisme *Attention*.<br>• **Pelatihan:** Model dilatih untuk meminimalkan *ranking loss function* (Triplet Loss) untuk mencocokkan vektor kode ($c$) dan deskripsi positif ($d^{+}$).\\<br>**Temuan Kunci:**<br>1. **Kinerja SOTA:** DEGRAPHCS mengungguli semua *baseline* (DeepCS, UNIF, MMAN, Self-Attention) dengan peningkatan MRR **19.14%** dan SuccessRate@1 **26.43%** dibandingkan SOTA sebelumnya (MMAN).<br>2. **VFG Superior:** VFG menunjukkan performa jauh lebih baik daripada representasi lain yang diuji (AST, FA-AST, CFG, IVFG), membuktikan VFG dapat menangkap semantik kode lebih akurat.<br>3. **Integrasi Graf vs. Multimodal:** Mengintegrasikan fitur (token, *data flow*, *control flow*) ke dalam **satu graf** (DEGRAPHCS) jauh lebih efektif daripada menggabungkannya melalui lapisan *attention* multimodal terpisah (MMAN-VFG).<br>4. **Efek Optimasi Graf:** Mekanisme GO meningkatkan MRR sebesar **13.77%** dan SuccessRate@1 sebesar **18.92%** sambil mengurangi waktu pelatihan hingga setengahnya.<br>5. **Evaluasi Manusia:** Dalam studi pengguna kualitatif, DEGRAPHCS mencapai SuccessRate@10 rata-rata **0.65** (vs. MMAN 0.57), menunjukkan nilai praktis yang lebih tinggi.<br><br>**Kontribusi Utama:**<br>• Mengusulkan DEGRAPHCS, metode representasi kode semantik novel yang mengintegrasikan token, *data flow*, dan *control flow* ke dalam *variable-based flow graph* (VFG) berbasis LLVM IR.<br>• Merancang mekanisme optimasi graf (GO) untuk menyederhanakan representasi graf dan meningkatkan efisiensi.<br>• Menunjukkan keunggulan model berbasis IR/graf dalam menangkap semantik mendalam dibandingkan representasi sintaksis tradisional.\\<br>**Dampak:**<br>• Memberikan arah baru untuk *code search* dengan memprioritaskan representasi semantik mendalam melalui IR daripada fitur sintaksis, menghasilkan presisi yang jauh lebih tinggi. Konsep VFG dapat diperluas ke masalah *software engineering* lain seperti *code translation* atau *API recommendation*. |

## 1. Pendahuluan & Masalah

*Code search* telah menjadi fokus utama karena meningkatnya repositori kode sumber terbuka (seperti GitHub). Tujuannya adalah mengambil fragmen kode yang relevan dengan kueri bahasa alami (NL) pengembang. Meskipun pendekatan berbasis *deep learning* yang ada (misalnya, DeepCS, MMAN) menawarkan solusi *end-to-end*, akurasi pencarian masih rendah, terutama pada repositori berskala besar.

Keterbatasan utama metode yang ada terletak pada dua aspek:

1.  **Representasi Kode yang Tidak Tepat:** Representasi yang didasarkan pada fitur token atau struktural (seperti AST atau *statement-based Control-Flow Graphs*) sulit mengungkapkan semantik kode secara mendalam. Kesenjangan antara sintaks dan semantik menyebabkan *false negative* (kode semantik sama, sintaks berbeda) dan *false positive* (kode semantik berbeda, sintaks serupa).
2.  **Integrasi Fitur yang Tidak Efektif:** Metode multimodal yang ada seringkali menggabungkan berbagai fitur yang diekstrak dari kode sumber secara tidak efektif. Peningkatan kinerja dari integrasi ini seringkali tidak signifikan, namun kompleksitas model meningkat (misalnya, MMAN hanya mengalami peningkatan kecil dalam MRR).

::: tip Solusi yang Diusulkan
Kami mengusulkan **DEGRAPHCS** (*Deep Graph for Code Search*), sebuah model novel yang mentransfer kode sumber menjadi **Variable-based Flow Graph (VFG)** menggunakan **LLVM IR** (Intermediate Representation) sebagai perantara. VFG merepresentasikan semantik pada granularitas variabel dengan mengintegrasikan token, *data flow*, dan *control flow*. Selain itu, dirancang mekanisme **Graph Optimization** dan digunakan **Attentional Gated Graph Neural Network** (GGNN) untuk pemodelan graf yang efisien dan akurat.
:::

## 2. Metodologi

DEGRAPHCS adalah kerangka kerja yang terdiri dari tiga bagian: persiapan data *offline*, inferensi *online*, dan arsitektur jaringan saraf yang berfokus pada representasi kode berbasis graf.

### A. Representasi Kode Neural

Kode sumber mula-mula diubah menjadi **LLVM IR**. Kemudian, **Variable-based Flow Graph (VFG)** dibangun pada granularitas variabel.

1.  **VFG Construction:** Node VFG merepresentasikan token dari instruksi LLVM IR (variabel, *opcode*, atau *label identifier*). *Edge* dibagi menjadi dua jenis:
    *   ***Data Dependency*** (Garis biru solid): Dibangun dari instruksi terkait komputasi (misalnya, "add", "sub") dan instruksi operasi alamat (misalnya, "load", "store").
    *   ***Control Dependency*** (Garis merah putus-putus): Dibangun dari instruksi lompatan bersyarat (misalnya, "br") dan operasi alamat.
2.  **VFG Optimization:** Mekanisme **Graph Optimization (GO)** diterapkan untuk mengurangi kebisingan dan meningkatkan efisiensi pelatihan. Proses ini meliputi: (1) Mengganti nomor variabel IR dengan nama variabel yang sesuai; (2) Menghapus *opcode* trivial (misalnya, terkait memori, konversi, *exception*); (3) Menghapus register sementara tanpa variabel yang sesuai; dan (4) Mengompresi *control flow graph* dengan menggabungkan blok yang terisolasi (satu *successor* dan satu *predecessor*).

### B. Graph2Vec (GGNN dengan Attention)

Untuk mendapatkan representasi vektor kode, VFG diolah menggunakan **Improved Gated Graph Neural Network (GGNN)** dengan mekanisme *Attention*.

1.  **GGNN:** Memperbarui *embedding* node ($h_{\upsilon_{i}}^{t}$) melalui mekanisme *message passing* dan fungsi pembaruan **GRU** (Gated Recurrent Unit), dengan mempertimbangkan jenis *edge* yang berbeda ($W_{l_{(\upsilon_{i},\upsilon_{j})}}$).
2.  **Attention Mechanism:** Menetapkan bobot $\alpha_{i}$ untuk setiap node $v_{i}$ berdasarkan kontribusi node terhadap semantik kode keseluruhan, memungkinkan model untuk fokus pada node yang paling penting. Representasi graf akhir ($h_{vfg}$) diperoleh dengan menjumlahkan *embedding* node yang dibobot:
    $$h_{vfg}=\sum_{v_{i}\in V}(\alpha_{i}h_{v_{i}})$$

### C. Representasi Deskripsi Komentar dan Pelatihan Model

1.  **Representasi Komen:** Komentar (kueri NL) diproses menggunakan **LSTM** dengan mekanisme *Attention* untuk menangkap relevansi butiran halus antara *hidden state* dan representasi akhir $E_{|d|}^{des}$.
2.  **Pelatihan Model:** Model dilatih dengan meminimalkan fungsi kerugian *ranking* (Triplet Loss) untuk membuat representasi kode ($c$) lebih mirip dengan deskripsi yang benar ($d^{+}$) daripada deskripsi yang salah ($d^{-}$):
    $$L(\theta)=\sum_{c\in C}\sum_{d^{+},d^{-}\in D}max(0,\beta-cos(c,d^{+})+cos(c,d^{-}))$$
    Di mana $\beta$ adalah *margin* konstan.

## 3. Detail Pengujian

### Dataset
*   **Pengumpulan Data:** 41.152 pasangan kode-komentar C dari GitHub (proyek *high-star*). Kode harus dapat dikompilasi menjadi LLVM IR.
*   **Pembagian:** $39.152$ untuk pelatihan, $2.000$ untuk pengujian.
*   **Skenario Uji Otomatis:** Setiap kueri dicocokkan dengan satu kandidat yang benar dan $1.999$ *distractor* (kode lain dari set tes).

### Baseline
Model dibandingkan dengan SOTA *deep code search*:
*   **DeepCS** (LSTM)
*   **CARLCS-CNN** (Co-Attention)
*   **Self-Attention** (BERT-based encoder)
*   **UNIF** (Bag-of-Words based)
*   **MMAN** (Multimodal Attention Network: Token+AST+S-CFG)

### Metrik Evaluasi
1.  **SuccessRate@k** ($k \in \{1, 5, 10\}$): Persentase kueri yang memiliki setidaknya satu *snippet* yang benar di antara $k$ hasil teratas.
    $$\text{SuccessRate}@k=(\frac{1}{|Q|}\sum_{q=1}^{Q}\delta(Rank_{q}\le k))$$
2.  **Mean Reciprocal Rank (MRR):** Rata-rata invers peringkat tertinggi dari *snippet* yang benar.
    $$MRR=\frac{1}{|Q|}\sum_{q=1}^{Q}\frac{1}{Rank_{q}}$$

## 4. Hasil Eksperimen

### RQ1: Perbandingan dengan Baseline

| Method | R@1 | R@5 | R@10 | MRR |
| :--- | :--- | :--- | :--- | :--- |
| DeepCS | 0.2350 | 0.4185 | 0.5045 | 0.3268 |
| MMAN | 0.3405 | 0.5325 | 0.6130 | 0.4342 |
| Self-Attention | 0.3965 | 0.5910 | 0.6570 | 0.4905 |
| **deGraphCS** | **0.4305** | **0.6175** | **0.6810** | **0.5173** |

*   **Peningkatan Kinerja:** DEGRAPHCS mengungguli semua *baseline* SOTA di semua metrik. Peningkatan MRR terhadap MMAN adalah $\frac{0.5173 - 0.4342}{0.4342} \approx \mathbf{19.14\%}$, dan R@1 meningkat dari $0.3405$ menjadi $0.4305$ (peningkatan $\approx 26.43\%$).
*   **Integrasi dengan Pre-trained Models:** Menambahkan urutan graf semantik IR ke model *pre-trained* modern (CodeBERT, CodeT5) selama *fine-tuning* memberikan peningkatan kinerja tambahan, mendukung klaim bahwa representasi graf semantik ini berharga.

### RQ2: Efek Representasi VFG

Model kode search yang sama (Improved GGNN) digunakan untuk menguji representasi kode yang berbeda.

| Method | R@1 | MRR | Improvement MRR vs. IVFG |
| :--- | :--- | :--- | :--- |
| CFG | 0.0945 | 0.1455 | - |
| AST | 0.2435 | 0.3276 | - |
| IVFG | 0.3025 | 0.3984 | - |
| **deGraphCS (VFG)** | **0.4305** | **0.5173** | $\mathbf{29.84\%}$ |

*   **VFG Terbaik:** VFG secara signifikan lebih baik daripada representasi lain. Peningkatan MRR terhadap IVFG (representasi IR berbasis instruksi) adalah $\approx \mathbf{29.84\%}$, menunjukkan keunggulan granularitas berbasis variabel dalam menangkap semantik.

### RQ3: Efek Integrasi Graf vs. Multimodal Attention

Perbandingan dilakukan antara DEGRAPHCS (integrasi semua fitur ke dalam satu graf) dan MMAN(Token+V-DFG+V-CFG) (menggabungkan fitur yang sama melalui *multi-modal attention*).

| Method | R@1 | MRR | MRR Decrease vs. deGraphCS |
| :--- | :--- | :--- | :--- |
| MMAN(Token+V-DFG+V-CFG) | 0.3380 | 0.4277 | $\mathbf{8.96\%}$ |
| **deGraphCS** | **0.4305** | **0.5173** | - |

*   **Integrasi Graf Unggul:** Mengintegrasikan token, *data flow*, dan *control flow* ke dalam satu representasi graf (DEGRAPHCS) adalah pilihan yang lebih baik daripada menggabungkannya secara kasar menggunakan lapisan *attention* multimodal yang terpisah.

### RQ5: Efek Komponen Graph Optimization (GO)

| Method | R@1 | MRR | MRR Improvement vs. deGraphCS-noGO |
| :--- | :--- | :--- | :--- |
| deGraphCS-noGO | 0.3620 | 0.4547 | - |
| **deGraphCS** | **0.4305** | **0.5173** | $\mathbf{13.77\%}$ |

*   **GO Efektif:** Mekanisme *Graph Optimization* (GO) meningkatkan MRR sebesar $\approx \mathbf{13.77\%}$ dan R@1 sebesar $\approx \mathbf{18.92\%}$. Analisis statistik menunjukkan GO mengurangi jumlah node total sebesar $\mathbf{51.88\%}$, yang juga mengurangi waktu pelatihan hampir setengahnya.

### RQ7: Evaluasi Manusia

Lima peserta berpengalaman menilai hasil yang dikembalikan oleh model (SuccessRate@10|MRR).

| Method | Avg. SuccessRate@10 | Avg. MRR |
| :--- | :--- | :--- |
| DeepCS | 0.42 | 0.26 |
| UNIF | 0.54 | 0.34 |
| MMAN | 0.57 | 0.40 |
| **DEGRAPHCS** | **0.65** | **0.48** |

*   **Nilai Praktis Tinggi:** DEGRAPHCS mengungguli *baseline* dalam skenario simulasi pencarian kode di dunia nyata, dengan rata-rata SuccessRate@10 $\mathbf{0.65}$ (peningkatan $\mathbf{14\%}$ dari MMAN).

## 5. Kesimpulan

DEGRAPHCS berhasil mengusulkan representasi kode semantik baru yang mengubah kode sumber menjadi *Variable-based Flow Graph* (VFG) melalui LLVM IR, mengintegrasikan token, *data flow*, dan *control flow* pada granularitas variabel. Ditambah dengan mekanisme *Graph Optimization* yang efektif dan GGNN dengan *attention*, DEGRAPHCS mencapai kinerja *code search* SOTA. Hasil pengujian otomatis dan manual mengkonfirmasi bahwa VFG lebih akurat dalam merepresentasikan semantik kode dibandingkan pendekatan struktural tradisional.

::: info Dampak Praktis
DEGRAPHCS menawarkan model yang tangguh dan akurat untuk *code search* dengan mengatasi keterbatasan representasi sintaksis. Pendekatan berbasis IR ini, dengan fokus pada *data flow* dan *control flow* pada level variabel, dapat dengan mudah ditransfer ke bahasa pemrograman lain (seperti Java atau Python) dengan menyesuaikan IR perantara, membuka arah baru untuk representasi kode semantik dalam berbagai masalah *software engineering*.
:::