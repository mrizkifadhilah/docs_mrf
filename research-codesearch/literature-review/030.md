---
title: Review Paper - I2R Pembelajaran Representasi Intra dan Inter-Modal untuk Pencarian Kode
description: Rangkuman paper tentang kerangka kerja I2R untuk optimasi konsistensi intra-modal dan penyelarasan inter-modal dalam pencarian kode (Intelligent Data Analysis, 2024).
head:
  - - meta
    - name: keywords
      content: Code search, semantic alignment, semantic representations, contrastive learning, R-drop, KL-divergence, pre-trained models, UniXcoder
---

# 030 - I²R: Intra and inter-modal representation learning for code search
Tautan (DOI) [10.3233/IDA-230082](https://doi.org/10.3233/IDA-230082)

**Penulis:** **Xu Zhang** ¹, **Yanzheng Xiang** ¹, **Zejie Liu** ¹, **Xiaoyu Hu** ¹, **Deyu Zhou** ¹*

**Afiliasi:**
* ¹ School of Computer Science and Engineering, Key Laboratory of Computer Network and Information Integration, Ministry of Education, Southeast University, Nanjing, Jiangsu, China

**Kronologi:** Tersedia Online: 2024 (Tanggal Rincian Tidak Disediakan)

<a href="https://www.scimagojr.com/journalsearch.php?q=11300153726&tip=sid" target="_blank"><img src="https://www.scimagojr.com/journal_img.php?id=11300153726" alt="SCImago Journal & Country Rank" /></a>

| Resume Eksekutif |
| :--- |
| **Publikasi:**<br>• **Jurnal:** Intelligent Data Analysis 28 (2024), 807–823<br>• **Topik:** Meningkatkan kinerja *code search* berbasis model *pre-trained* dengan mengatasi kurangnya penyelarasan implisit antar-modalitas (kueri dan kode) dan kurangnya kendala konsistensi dalam representasi intra-modal.<br><br>**Masalah & Solusi:**<br>• **Masalah 1 (Inter-Modal):** Model *pre-trained* yang ada, meskipun kuat, mengandalkan kesamaan kosinus langsung antara representasi kueri (bahasa alami) dan kode (bahasa pemrograman). Penyelarasan implisit representasi antar-modalitas ini tidak sepenuhnya dieksplorasi selama *fine-tuning*, menyebabkan bias dan hasil pencarian yang tidak optimal.<br>• **Masalah 2 (Intra-Modal):** Tidak adanya kendala konsistensi representasi untuk modalitas tunggal (intra-modal), yang dapat menghambat kinerja model, karena representasi tingkat tinggi yang konsisten (seperti yang ditunjukkan oleh R-drop) penting untuk peningkatan kinerja.<br>• **Solusi:** Mengusulkan kerangka kerja **I²R** (*Intra and Inter-modal Representation learning*). **Contrastive Learning** diperkenalkan untuk mengoptimalkan penyelarasan **inter-modal** (kueri-kode) secara eksplisit. Teknik **R-drop** (diterapkan melalui **KL-divergence**) digunakan untuk memastikan **konsistensi** representasi **intra-modal** (kueri-kueri dan kode-kode).<br><br>**Contoh Penerapan:**<br>• Diterapkan dan dievaluasi menggunakan model *pre-trained* **UniXcoder** sebagai arsitektur dasar. Diuji pada **tujuh dataset** (*benchmark*) utama, termasuk enam bahasa dari **CSN** (*CodeSearchNet*) dan **AdvTest**.<br><br>**Metodologi:**<br>• **Arsitektur Dasar:** Menggunakan model *siamese network* dengan **UniXcoder** sebagai *encoder* untuk menghasilkan representasi semantik kueri dan kode.<br>• **Pembelajaran Inter-Modal:** Menggunakan fungsi *contrastive loss* (InfoNCE Loss) untuk menyelaraskan representasi antar-modalitas secara bidireksional ($loss_{CLQ}$ dan $loss_{CLC}$), menarik sampel positif (pasangan kueri-kode yang benar) lebih dekat dan mendorong sampel negatif (kueri dengan kode lain dalam *batch*) menjauh.<br>• **Pembelajaran Intra-Modal:** Menggunakan teknik **R-drop** dengan menghitung **KL-divergence** antara dua submodel yang memiliki *dropout* berbeda (misalnya $0.1$ dan $0.4$). Tujuannya adalah memastikan konsistensi distribusi representasi antara dua submodel untuk modalitas yang sama (kueri-kueri dan kode-kode).<br>• **Fungsi Tujuan Akhir:** Meminimalkan gabungan *loss* inter-modal dan intra-modal: $L = loss_{CL} + loss_{KL}$.<br><br>**Temuan Kunci:**<br>1. **Kinerja SOTA:** I²R mengungguli *state-of-the-art* (UniXcoder) sebesar $\mathbf{0.8\%}$ pada tugas CSN secara keseluruhan, dan $\mathbf{2.5\%}$ pada dataset AdvTest.<br>2. **Kontribusi Modul:** Baik optimasi intra-modal (IntraR) maupun inter-modal (InterR) memberikan peningkatan kinerja yang signifikan dibandingkan *baseline* UniXcoder, dan kombinasi keduanya memberikan efek superimposisi terbaik ($\text{MRR}=75.2\%$).<br>3. **Generalisasi:** I²R berhasil diterapkan pada **GraphCodeBERT**, meningkatkan kinerjanya secara signifikan sehingga menjadi sebanding atau bahkan lebih baik dari *baseline* UniXcoder, menunjukkan **portabilitas** kerangka kerja.<br>4. **Visualisasi:** Visualisasi $3\text{D}$ menunjukkan bahwa I²R menghasilkan distribusi representasi pasangan kueri-kode yang **lebih terpusat** dan klaster yang lebih rapat, memvalidasi keberhasilan penyelarasan antar-modal dan konsistensi intra-modal.<br><br>**Kontribusi Utama:**<br>• Kerangka kerja $\mathbf{I^2R}$ novel yang secara bersamaan mengoptimalkan representasi intra-modal (melalui R-drop/KL-divergence) dan inter-modal (melalui *contrastive learning*).<br>• Bukti eksperimental ekstensif yang menunjukkan keefektifan I²R melampaui metode SOTA pada tujuh *benchmark* dataset.<br><br>**Dampak:**<br>• Meningkatkan **keandalan** dan **akurasi** mesin pencari kode yang didukung oleh model *pre-trained*, yang krusial untuk otomatisasi dan efisiensi dalam pengembangan perangkat lunak modern. |

## 1. Pendahuluan & Masalah

Pencarian kode (*Code search*) adalah proses menemukan dan menggunakan kembali *snippet* kode yang ada dalam repositori besar berdasarkan kebutuhan spesifik yang diungkapkan dalam bentuk kueri bahasa alami. Kemampuan pencarian kode otomatis yang presisi berpotensi meningkatkan efisiensi pengembangan perangkat lunak secara signifikan.

Penelitian terkini menunjukkan keberhasilan penggunaan teknik *deep learning* dan, yang lebih baru, model *pre-trained* seperti UniXcoder dan CodeBERT, untuk merepresentasikan kueri dan kode. Namun, masalah mendasar tetap ada: kode (bahasa pemrograman) dan kueri (bahasa alami) adalah dua modalitas yang berbeda, dan **penyelarasan representasi** di antara keduanya sangat penting.

Model *pre-trained* yang ada cenderung hanya mengandalkan representasi yang telah dipelajari dari data berskala besar dan menghitung kesamaan kosinus langsung. Hal ini mengabaikan dua aspek kritis selama fase *fine-tuning*:

1.  **Representasi Inter-Modal:** Penyelarasan implisit antara representasi kueri dan kode tidak sepenuhnya dieksplorasi. Hal ini dapat menyebabkan bias, di mana kode untuk kueri yang serupa (negatif) bisa memiliki kesamaan tinggi dengan kueri target (positif).
2.  **Representasi Intra-Modal:** Tidak adanya kendala konsistensi representasi dalam modalitas tunggal (misalnya, dua representasi yang dihasilkan dari kueri yang sama melalui *dropout* yang berbeda), yang telah terbukti penting untuk stabilitas dan kinerja model.

::: tip Solusi yang Diusulkan
Diusulkan model *code search* baru, **I²R** (*Intra and Inter-modal Representation learning*), yang secara simultan mengoptimalkan pembelajaran representasi **intra-modal** dan **inter-modal**. Penyelarasan inter-modal dicapai melalui **Contrastive Learning** (InfoNCE Loss) untuk membedakan pasangan kueri-kode positif dari negatif. Konsistensi intra-modal dijamin dengan teknik **R-drop** (menggunakan **KL-divergence**) untuk menyelaraskan representasi yang dihasilkan dari dua submodel *dropout* yang berbeda.
:::

## 2. Metodologi

I²R mengadopsi arsitektur *siamese network* di mana kode dan kueri disandikan secara terpisah oleh model *pre-trained* **UniXcoder**.

### A. UniXcoder-based Semantic Representation Model

UniXcoder dipilih sebagai model dasar karena kemampuannya dalam memahami kode struktural melalui informasi AST (Abstract Syntax Tree) dan kesesuaiannya dengan berbagai modalitas *encoder-decoder*. Representasi kueri $P^{w}(query_{i})$ dan kode $P^{w}(code_{i})$ diperoleh melalui *encoder* UniXcoder. Dalam I²R, dua pasang UniXcoder (dengan parameter bersama) digunakan untuk menghasilkan dua representasi untuk setiap modalitas, masing-masing dengan *dropout rate* yang berbeda.

### B. Inter-Modal Representation Learning

Tujuan utama adalah mencapai penyelarasan yang lebih baik antara kueri dan kode. *Contrastive learning* digunakan untuk menarik representasi pasangan positif ($query_i, code_i$) mendekat dan mendorong pasangan negatif menjauh. Pasangan negatif adalah semua kode lain dalam *batch* yang sama ($code_j, j \neq i$).

Loss untuk penyelarasan kueri-terpandu ($loss_{CLQ}$) dan kode-terpandu ($loss_{CLC}$) didefinisikan sebagai *InfoNCE Loss*:

$$ \text{loss}_{CLQ}=-\sum_{i=0}^{b-1}\log\frac{e^{\cos(P_{1}^{w_{1}}(query_{i}),P_{3}^{w_{3}}(code_{i}))/\tau}}{\sum_{j=0}^{b-1}e^{\cos(P_{1}^{w_{1}}(query_{i}),P_{3}^{w_{3}}(code_{j}))/\tau}} $$

$$ \text{loss}_{CLC}=-\sum_{i=0}^{b-1}\log\frac{e^{\cos(P_{3}^{w_{3}}(code_{i}),P_{1}^{w_{1}}(query_{i}))/\tau}}{\sum_{j=0}^{b-1}e^{\cos(P_{3}^{w_{3}}(code_{i}),P_{1}^{w_{1}}(query_{j}))/\tau}} $$

Total *loss* inter-modal adalah jumlah dari kedua arah: $loss_{CL1}=loss_{CLQ}+loss_{CLC}$. Karena adanya dua submodel (*dropout* yang berbeda), total *loss* inter-modal menjadi $loss_{CL}=loss_{CL1}+loss_{CL2}$.

### C. Intra-Modal Representation Learning

Teknik **R-drop** diterapkan untuk memastikan **konsistensi** representasi tingkat tinggi dalam modalitas yang sama. R-drop bekerja dengan mewajibkan konsistensi antara dua submodel yang dihasilkan dari satu model induk dengan *dropout* yang berbeda.

Konsistensi intra-modal diukur menggunakan **KL-Divergence** antara distribusi representasi dua submodel untuk kueri ($P_{1}^{w_{1}}$ dan $P_{2}^{w_{2}}$) dan untuk kode ($P_{3}^{w_{3}}$ dan $P_{4}^{w_{4}}$).

Loss untuk kueri:
$$ \text{loss}_{KLQ}=\frac{1}{2}(\text{KL}(P_{1}^{w_{1}}(query_{i})||P_{2}^{w_{2}}(query_{i})) + \text{KL}(P_{2}^{w_{2}}(query_{i})||P_{1}^{w_{1}}(query_{i}))) $$

Loss untuk kode:
$$ \text{loss}_{KLC}=\frac{1}{2}(\text{KL}(P_{3}^{w_{3}}(code_{i})||P_{4}^{w_{4}}(code_{i})) + \text{KL}(P_{4}^{w_{4}}(code_{i})||P_{3}^{w_{3}}(code_{i}))) $$

Total *loss* intra-modal: $loss_{KL}=loss_{KLQ}+loss_{KLC}$.

### D. Optimisation Objectives

Tujuan pelatihan akhir adalah meminimalkan total *loss*, yang merupakan gabungan dari *loss* inter-modal dan intra-modal:

$$ L=loss_{CL}+loss_{KL} $$

## 3. Detail Pengujian

### Dataset
*   **CSN (CodeSearchNet):** Enam bahasa: Ruby, Javascript, Go, Python, Java, PHP.
*   **AdvTest:** Dataset Python yang dinormalisasi untuk menguji pemahaman dan kemampuan generalisasi model.

### Baseline
Model SOTA yang dibandingkan: **ROBERTa, CodeBERT, GraphCodeBERT, SYNCOBERT, PLBART, CodeT5-base, dan UniXcoder**.

### Metrik Evaluasi
**Mean Reciprocal Rank (MRR)** digunakan untuk menilai kinerja sistem pencarian kode.

$$ \text{MRR}=\frac{1}{Q}\sum_{i=1}^{|Q|}\frac{1}{\text{rank}_{i}} $$

Di mana $Q$ adalah jumlah kueri, dan $rank_i$ adalah posisi peringkat dari item *ground-truth* pertama untuk kueri ke-$i$.

## 4. Hasil Eksperimen

### RQ1. Efektivitas I²R dalam Code Search

I²R mencapai kinerja terbaik secara keseluruhan dibandingkan semua metode *baseline*.

| Model | AdvTest | Ruby | Javascript | Go | Python | Java | PHP | Overall |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| UniXcoder | 41.3 | 74.0 | 68.4 | 91.5 | 72.0 | 72.6 | 67.6 | 74.4 |
| **I²R** | **43.8** | **75.5** | **69.2** | **91.6** | **73.1** | **73.4** | **68.2** | **75.2** |

*   I²R melampaui SOTA (UniXcoder) sebesar $\mathbf{0.8\%}$ pada tugas CSN keseluruhan.
*   Peningkatan paling signifikan terlihat pada dataset AdvTest, sebesar $\mathbf{2.5\%}$ di atas UniXcoder.

### RQ2. Pengaruh Representasi Intra- dan Inter-Modal

| Model | AdvTest | Ruby | Javascript | Go | Python | Java | PHP | Overall |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| UniXcoder | 41.3 | 74.0 | 68.4 | 91.5 | 72.0 | 72.6 | 67.6 | 74.4 |
| IntraR ($loss_{KL}$ saja) | 18.3 | 75.5 | 69.0 | 91.3 | 73.0 | 72.9 | 68.3 | 75.0 |
| InterR ($loss_{CL}$ saja) | 43.3 | 75.3 | 68.8 | 91.3 | 72.9 | 72.9 | 67.9 | 74.9 |
| **I²R** ($loss_{CL} + loss_{KL}$) | **43.8** | **75.5** | **69.2** | **91.6** | **73.1** | **73.4** | **68.2** | **75.2** |

*   **IntraR** (konsistensi intra-modal) secara signifikan meningkatkan MRR keseluruhan sebesar $0.6\%$ ($74.4\%$ menjadi $75.0\%$), menunjukkan pentingnya konsistensi representasi.
*   **InterR** (penyelarasan inter-modal) juga memberikan peningkatan yang kuat, sebesar $0.5\%$ ($74.4\%$ menjadi $74.9\%$).
*   Kombinasi **I²R** menghasilkan efek superimposisi, menunjukkan bahwa kedua jenis optimasi (intra dan inter-modal) saling melengkapi.

### RQ3. Kinerja pada Model Pre-Trained Lain (GraphCodeBERT)

| Model | AdvTest | Ruby | Javascript | Go | Python | Java | PHP | Overall |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| GraphCodeBERT | 35.2 | 70.3 | 64.4 | 89.7 | 69.2 | 69.1 | 64.9 | 71.3 |
| I²R-GraphCodeBERT | 41.6 | 72.9 | 67.0 | 90.6 | 72.0 | 72.1 | 67.1 | 73.6 |
| UniXcoder | 41.3 | 74.0 | 68.4 | 91.5 | 72.0 | 72.6 | 67.6 | 74.4 |

*   Penerapan I²R pada GraphCodeBERT (yang memiliki performa lebih lemah dari UniXcoder) menghasilkan peningkatan luar biasa (MRR keseluruhan $\mathbf{71.3\%}$ menjadi $\mathbf{73.6\%}$).
*   **Portabilitas Model:** Kinerja I²R-GraphCodeBERT (73.6%) menjadi sebanding atau bahkan lebih baik dari *baseline* UniXcoder (74.4% tanpa I²R) pada tugas *code search*, membuktikan bahwa I²R secara efektif mengatasi kelemahan penyelarasan GraphCodeBERT.

### RQ4. Dampak Perbedaan Dropout Rate

*   I²R menggunakan dua *dropout rate* yang berbeda, dengan satu dipertahankan pada *default* UniXcoder dan yang lain bervariasi.
*   Kinerja terbaik I²R dicapai ketika perbedaan *dropout rate* berada dalam kisaran yang wajar (misalnya **$0.3$ atau $0.4$** untuk submodel kedua).
*   *Dropout rate* $\mathbf{0.4}$ dipilih sebagai kompromi yang optimal, menghasilkan kinerja keseluruhan $75.1\%$ (MRR).

## 5. Kesimpulan

Penelitian ini memperkenalkan **I²R**, kerangka kerja novel yang mengoptimalkan pembelajaran representasi untuk *code search* dengan secara eksplisit mengatasi masalah **konsistensi representasi intra-modal** (menggunakan R-drop dan KL-divergence) dan **penyelarasan representasi inter-modal** (menggunakan *contrastive learning*). Dengan mengintegrasikan kedua modul ini, I²R berhasil meningkatkan kinerja *code search* secara substansial, mencapai *state-of-the-art* baru pada dataset *benchmark* utama. Visualisasi menunjukkan bahwa I²R menghasilkan representasi pasangan kueri-kode yang jauh lebih terpusat dan konsisten.

::: info Dampak Praktis
I²R menawarkan jalur yang jelas untuk meningkatkan model *code search* berbasis *pre-trained* yang sudah ada (seperti UniXcoder dan GraphCodeBERT). Metode ini menyediakan mekanisme **regularisasi dan penyelarasan** yang kuat, menghasilkan representasi yang lebih stabil dan deskriptif, yang pada akhirnya meningkatkan **akurasi** dan **keandalan** alat *code search* dalam lingkungan pengembangan perangkat lunak praktis.
:::