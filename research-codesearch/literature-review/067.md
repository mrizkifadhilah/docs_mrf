---
title: Review Paper - Pencarian Kode Multibahasa Berbasis Knowledge Distillation
description: Rangkuman paper tentang MPLCS, model pencarian kode multibahasa menggunakan Knowledge Distillation (Algorithms, 2022).
head:
  - - meta
    - name: keywords
      content: multilingual code search, knowledge distillation, MPLCS, joint vector representation, self-attention, MRR
---

# 067 - Knowledge Distillation-Based Multilingual Code Retrieval
Tautan (DOI) [https://doi.org/10.3390/a15010025]

**Penulis:** **Wen Li** $^{1}$, **Junfei Xu** $^{1}$, **Qi Chen** $^{1*}$

**Afiliasi:**
* $^{1}$ College of Computer Science and Technology, Zhejiang University, Hangzhou 310013, China

**Kronologi:** Received: 30 November 2021 • Accepted: 12 January 2022 • Published: 17 January 2022

<a href="https://www.scimagojr.com/journalsearch.php?q=21100199795&tip=sid" target="_blank"><img src="https://www.scimagojr.com/journal_img.php?id=21100199795" alt="SCImago Journal & Country Rank" /></a>

| Resume Eksekutif |
| :--- |
| **Publikasi:**<br>• **Jurnal:** Algorithms 15, 1 (2022)<br>• **Topik:** Mengatasi masalah pencarian kode (*code retrieval*) dalam skenario multibahasa (*multilingual*) dengan mengurangi kebutuhan korpus besar untuk setiap bahasa.<br><br>**Masalah & Solusi:**<br>• **Masalah:** Pendekatan *code retrieval* yang ada (berbasis IR atau DL) berfokus pada pemetaan satu bahasa alami ke satu bahasa pemrograman (monolingual). Pendekatan multibahasa secara konvensional memerlukan pelatihan model terpisah (atau korpus besar) untuk setiap bahasa, yang tidak efisien dan sulit diterapkan pada bahasa dengan korpus kecil.<br>• **Solusi:** Mengusulkan **MPLCS** (*Multi-Programming Language Code Search*). MPLCS menggunakan teknik **Knowledge Distillation (KD)** untuk memadukan pengetahuan dari enam *Monolingual Teacher Models* (TM) yang sudah terlatih menjadi satu *Student Model* (SM). Model tunggal ini mendukung pencarian kode multibahasa dengan persyaratan korpus yang rendah.<br><br>**Contoh Penerapan:**<br>• MPLCS dilatih dan diuji pada enam bahasa (Java, Python, PHP, Go, JavaScript, Ruby) dari *dataset* publik (berasal dari GitHub).<br>• MPLCS berhasil meningkatkan MRR untuk Ruby (bahasa dengan korpus terkecil) sebesar $\mathbf{20-25\%}$ dibandingkan model monolingualnya sendiri, dan secara konsisten mengungguli model *fusion* langsung (ALL) pada hampir semua bahasa.<br><br>**Metodologi:**<br>• **Arsitektur Dasar (TM & SM):** Menggunakan model *joint embedding* (Self-Attention sebagai *encoder*) untuk memetakan kode dan deskripsi NL ke ruang vektor bersama. *Encoder* mencakup lapisan *embedding*, *fully connected layer*, dan *Attention Vector* untuk agregasi token berbobot.<br>• **Pelatihan TM:** Dilatih secara independen untuk setiap bahasa menggunakan fungsi *Contrastive Loss* (*Rank Loss*): $$Loss_{teacher}=\max(0,1-\cos(c,d^{+})+\cos(c,d^{-}))$$.<br>• **Knowledge Distillation (KD):** SM dilatih untuk meniru perilaku TM. Fungsi *loss* SM menggabungkan tiga komponen:<br>1. *Loss self-training* SM (pada *triplet* $\langle C, D^{+}, D^{-} \rangle$).<br>2. *Loss* KD antara vektor kode SM dan vektor deskripsi TM.<br>3. *Loss* KD antara vektor kode TM dan vektor deskripsi SM.<br>• **Loss Fusion:** $$Loss_{student-ALL}=(1-\lambda)Loss_{student-self}+\lambda Loss_{KD}$$. Parameter $\lambda$ mengontrol kontribusi pengetahuan TM.<br><br>**Temuan Kunci:**<br>1. **Peningkatan Korpus Kecil:** MPLCS secara signifikan meningkatkan MRR pada Ruby (bahasa dengan korpus terkecil) sebesar $20-25\%$, menunjukkan kemampuannya untuk mempelajari komunalitas dari bahasa lain.<br>2. **Performa Lintas Bahasa:** MPLCS secara konsisten mengungguli model monolingual ketika model monolingual tersebut melakukan *retrieval* pada bahasa lain, serta mengungguli model *fusion* langsung (ALL).<br>3. **Model Kompak:** Model MPLCS tunggal memiliki parameter lebih sedikit dibandingkan enam model monolingual yang terpisah.<br><br>**Kontribusi Utama:**<br>• Mengusulkan **MPLCS**, model *deep code search* pertama yang mengatasi tugas *multi-programming language search* secara efisien menggunakan teknik **Knowledge Distillation**.<br>• Membuktikan bahwa KD dapat mentransfer pengetahuan lintas bahasa pemrograman untuk meningkatkan akurasi bahasa dengan data pelatihan terbatas.<br><br>**Dampak:**<br>• MPLCS memungkinkan pengembang untuk mencari kode secara efisien di berbagai bahasa menggunakan satu model terpadu, mengurangi biaya komputasi (ukuran model lebih kecil) dan memberikan dukungan yang lebih baik untuk bahasa dengan ekosistem data yang lebih kecil. |

## 1. Pendahuluan & Masalah

Pencarian kode semantik adalah tugas krusial dalam rekayasa perangkat lunak, yang bertujuan menjembatani kesenjangan antara bahasa alami (NL) yang digunakan dalam kueri dan bahasa pemrograman (PL) yang spesifik sintaksis dan logika.

Pendekatan *code retrieval* yang ada, baik berbasis *Information Retrieval* (IR) maupun *Deep Learning* (DL), sebagian besar berfokus pada pemetaan dari satu NL ke **satu PL** (monolingual). Namun, survei menunjukkan bahwa proyek perangkat lunak modern sering menggunakan banyak bahasa pemrograman (*multilanguage software development*, MLSD).

Pendekatan monolingual menjadi tidak praktis dan tidak efisien dalam skenario multibahasa:
1.  **Inefisiensi Model:** Memerlukan pelatihan dan pengelolaan model terpisah untuk setiap PL.
2.  **Kebutuhan Korpus Tinggi:** Memerlukan korpus pelatihan yang besar untuk setiap PL, yang merupakan tantangan besar untuk bahasa-bahasa dengan basis data kode yang relatif kecil.

::: tip Solusi yang Diusulkan
Untuk mengatasi masalah ini, diusulkan **MPLCS** (*Multi-Programming Language Code Search*), model *Student* tunggal yang dilatih menggunakan teknik **Knowledge Distillation (KD)**. Model ini menyerap pengetahuan dari enam *Monolingual Teacher Models* (TM) yang sudah ada, memungkinkan pencarian kode multibahasa yang efisien, mengurangi persyaratan korpus, dan meningkatkan akurasi untuk bahasa dengan data pelatihan yang kecil.
:::

## 2. Metodologi

MPLCS menggunakan pendekatan **Joint Vector Representation** untuk memetakan deskripsi NL dan kode PL yang heterogen ke ruang vektor berdimensi tinggi yang sama. KD adalah inti dari fusi multibahasa model.

### A. Arsitektur Model Guru (Teacher Model)

Model *Teacher* (TM) dilatih secara independen untuk setiap bahasa. Arsitektur yang digunakan adalah **Self-Attention** (dipilih karena performa terbaik di antara 1dCNN dan NBOW):
1.  **Embedding:** Kode dan deskripsi diubah menjadi urutan token, yang kemudian di-*embed* (misalnya $c_{i}=embedding(ctoken_{i})$).
2.  **Fully Connected Layer:** *Embedding* token melewati lapisan terhubung penuh dan fungsi aktivasi tangen hiperbolik ($\tanh$): $\tilde{c}_{i}=tanh(W_{c}\cdot c_{i})$.
3.  **Agregasi Bobot (*Attention*):** Vektor perhatian ($\mathbf{a}$) yang diinisialisasi secara acak digunakan untuk menghitung bobot setiap token, $\alpha_{i}$, yang dinormalisasi menggunakan fungsi *softmax*:
    $$\alpha_{i}=\frac{\exp(\tilde{c}_{i}^{\top}\cdot \mathbf{a})}{\sum_{j=1}^{n}\exp(\tilde{c}_{j}^{\top}\cdot \mathbf{a})}$$
4.  **Vektor Akhir:** Vektor kode akhir ($\mathbf{v}_{c}$) adalah penjumlahan berbobot dari vektor token:
    $$\mathbf{v}_{c} = \sum_{i=1}^{n}\alpha_{i}\tilde{c}_{i}$$

### B. Pelatihan Model Guru (*Teacher Model Learning*)

Setiap TM dilatih untuk meminimalkan *Rank Loss* (atau *Contrastive Loss*) pada *triplet* $\langle C, D^{+}, D^{-} \rangle$ (kode, deskripsi positif, deskripsi negatif). Tujuan fungsi *loss* adalah memastikan kesamaan kosinus ($\cos$) antara pasangan positif lebih tinggi daripada pasangan negatif:
$$Loss_{teacher}=\max(0,1-\cos(c,d^{+})+\cos(c,d^{-}))$$

### C. Pelatihan Model Siswa (*Student Model*) melalui Knowledge Distillation (KD)

Model *Student* (MPLCS) memiliki struktur yang sama dengan TM. Selama pelatihan, *loss* SM mencakup dua komponen utama:

1.  **Self-Loss ($Loss_{student-self}$):** *Loss* konvensional SM pada *triplet* datanya sendiri (seperti pelatihan TM).
2.  **Knowledge Distillation Loss ($Loss_{KD}$):** *Loss* yang mengukur kesamaan antara vektor *embedding* SM dan TM. $Loss_{KD}$ terdiri dari dua bagian: (i) Kesamaan antara vektor kode SM dan vektor deskripsi TM, dan (ii) Kesamaan antara vektor kode TM dan vektor deskripsi SM.

$$Loss_{KD} = \sum_{\text{teacher}} \left[ \max(0,1-\cos(c_{\text{student}}, d_{\text{teacher}}^{+})+\cos(c_{\text{student}}, d_{\text{teacher}}^{-})) + \max(0,1-\cos(c_{\text{teacher}}, d_{\text{student}}^{+})+\cos(c_{\text{teacher}}, d_{\text{student}}^{-})) \right]$$

*Loss* total SM adalah kombinasi berbobot:
$$Loss_{student-ALL}=(1-\lambda)Loss_{student-self}+\lambda Loss_{KD}$$

Parameter $\lambda$ (diuji dari $0.0$ hingga $1.0$) mengontrol seberapa besar pengaruh pengetahuan *Teacher* terhadap pelatihan *Student*. Algoritma ini juga menyertakan logika *Early Stop* dan mekanisme untuk mematikan kontribusi KD ($f^{l}=False$) jika SM sudah berkinerja lebih baik daripada TM untuk bahasa tertentu.

## 3. Detail Pengujian

### Data Preparation
*   Digunakan *dataset* publik (berasal dari GitHub) dengan total $\mathbf{2.326.976}$ fungsi/anotasi dari enam bahasa: **Java, Go, PHP, Python, JavaScript, dan Ruby**. Ruby memiliki korpus terkecil ($\mathbf{57.393}$ fungsi).
*   *Tokenization* menggunakan **Byte-Pair-Encoding (BPE)** untuk mengatasi masalah *Out-Of-Vocabulary* (OOV). Ukuran kamus kode dan kueri diatur ke $\mathbf{30.000}$.
*   Panjang kode diatur $\mathbf{200}$ dan deskripsi $\mathbf{30}$.

### Metrik Evaluasi
1.  **Mean Reciprocal Rank (MRR):** Rata-rata invers dari peringkat pertama hasil yang benar.
    $$MRR=\frac{1}{|Q|}\sum_{q=1}^{|Q|}\frac{1}{rank_{q}}$$
2.  **Success Rate@k ($R@k$):** Persentase kueri yang memiliki hasil benar di peringkat $k$ teratas.

## 4. Hasil Eksperimen

Pengujian dilakukan dengan empat kombinasi *encoder* (Self-Attention/Self-Attention, CNN/CNN, NBOW/NBOW, Self-Attention/NBOW) pada enam bahasa. Hasil di bawah menggunakan *encoder* Self-Attention/Self-Attention.

### A. Perbandingan MRR (Self-Attention/Self-Attention)

| Model | Go | Java | Javascript | Php | Python | Ruby |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| TM Monolingual (pada dirinya) | **0.7756** | **0.6632** | 0.5304 | **0.6424** | **0.7613** | 0.4773 |
| ALL (Fusion Langsung) | 0.7356 | 0.6350 | 0.5240 | 0.6191 | 0.7177 | 0.5717 |
| **MPLCS (Student)** | 0.7472 | 0.6404 | **0.5492** | 0.6079 | 0.7289 | **0.5977** |

**Analisis Kinerja:**
*   **Peningkatan MPLCS:** MPLCS secara konsisten mengungguli model *fusion* langsung (ALL) pada semua bahasa.
*   **Komunitas Bahasa Kecil:** Untuk **Ruby** (korpus terkecil), MPLCS mencapai MRR **0.5977**, yang merupakan peningkatan signifikan sebesar $\mathbf{25.2\%}$ dibandingkan TM Ruby aslinya ($\mathbf{0.4773}$). Ini membuktikan kemampuan MPLCS untuk mempelajari komunalitas dari bahasa lain.
*   **Pengunggulan TM:** MPLCS bahkan mengungguli TM pada bahasa aslinya (JavaScript dan Ruby), menunjukkan bahwa KD membantu dalam *generalization* pada korpus yang lebih kecil.
*   **Trade-off Akurasi:** Meskipun MPLCS tidak mengungguli TM pada bahasa dengan korpus besar (Go, Java, Python) pada tugas monolingual mereka sendiri, MPLCS unggul dalam akurasi saat TM monolingual diuji pada **bahasa lain** (misalnya, TM Go yang diuji pada Java menghasilkan MRR $\mathbf{0.5400}$, sedangkan MPLCS menghasilkan $\mathbf{0.6404}$ pada Java).

### B. Eksplorasi Parameter $\lambda$ (Kontribusi Teacher)

| Lambda ($\lambda$) | Go | Java | Javascript | Php | Python | Ruby | Avg |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| 0.0 (Self-Training Murni) | 0.7304 | 0.6365 | 0.5520 | 0.6041 | 0.7208 | 0.5925 | 0.6535 |
| **0.8 (Optimal)** | 0.7400 | 0.6473 | 0.5621 | 0.6191 | 0.7391 | 0.6059 | **0.6670** |
| 1.0 (KD Murni) | 0.7453 | 0.6456 | 0.5561 | 0.6155 | 0.7356 | 0.6042 | 0.6651 |

*   Peningkatan nilai $\lambda$ dari $0.0$ hingga $0.8$ secara konsisten meningkatkan MRR rata-rata, membuktikan bahwa **semakin banyak panduan dari Model *Teacher* yang dimasukkan, semakin banyak pengetahuan yang dipelajari Model *Student***. Nilai optimal MRR rata-rata dicapai pada $\lambda=\mathbf{0.8}$.

## 5. Kesimpulan

Paper ini berhasil memperkenalkan ide baru untuk *semantic code retrieval*: **Pencarian Kode Multi-Bahasa**. Dengan menerapkan teknik **Knowledge Distillation (KD)**, model **MPLCS** berhasil memadukan enam model monolingual menjadi satu model tunggal yang efisien dan unggul dalam mendukung pencarian multibahasa, terutama pada bahasa dengan data pelatihan yang terbatas (meningkatkan MRR Ruby hingga 25%). Keberhasilan ini menunjukkan bahwa KD adalah metode yang efektif untuk mentransfer pengetahuan lintas bahasa pemrograman dan mengungkapkan komunalitas di antara mereka.

::: info Dampak Praktis
MPLCS menawarkan solusi yang ringkas (ukuran model lebih kecil) dan kuat untuk kebutuhan pengembangan perangkat lunak multibahasa. Model ini dapat segera diterapkan oleh pengembang yang bekerja dengan *codebase* multi-PL, menghilangkan kebutuhan akan model *code search* terpisah untuk setiap bahasa dan meningkatkan akurasi pada bahasa yang kurang banyak didokumentasikan.
:::