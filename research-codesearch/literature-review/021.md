---
title: Review Paper - Pencarian Kode Neural yang Ditingkatkan Semantik Mendalam
description: Rangkuman paper tentang Algoritma SENCS untuk Peningkatan Akurasi Pencarian Kode (Electronics, 2024).
head:
  - - meta
    - name: keywords
      content: code search, graph serialization, semantic enhance, attention mechanism, SENCS, PDG
---

# 021 - Deep Semantics-Enhanced Neural Code Search
Tautan (DOI) [10.3390/electronics13234704](https://doi.org/10.3390/electronics13234704)

**Penulis:** **Ying Yin** ¹, **Longfei Ma** ¹, **Yuqi Gong** ¹, **Yucen Shi** ¹, **Fazal Wahab** ¹, dan **Yuhai Zhao** ¹*

**Afiliasi:**
* ¹ School of Computer Science and Engineering, Northeastern University, Shenyang 110819, China

**Kronologi:** Received: 18 October 2024 • Revised: 22 November 2024 • Accepted: 24 November 2024 • Available Online: 28 November 2024

<a href="https://www.scimagojr.com/journalsearch.php?q=21100829272&tip=sid" target="_blank"><img src="https://www.scimagojr.com/journal_img.php?id=21100829272" alt="SCImago Journal & Country Rank" /></a>

| Resume Eksekutif |
| :--- |
| **Publikasi:**<br>• **Jurnal:** Electronics 13 (2024) 4704<br>• **Topik:** Peningkatan akurasi *neural code search* dengan menangkap fitur struktural dan semantik yang lebih dalam melalui serialisasi *graph* dan mekanisme *attention* dua tahap.<br><br>**Masalah & Solusi:**<br>• **Masalah:** Algoritma *code search* yang ada gagal menangkap fitur semantik dan struktural yang lebih dalam dari kode dan struktur *code graph*-nya (misalnya, PDG), seringkali kehilangan informasi penting dan mengakibatkan akurasi yang lebih rendah.<br>• **Solusi:** Mengusulkan **SENCS** (*Deep Semantics-Enhanced Neural Code Search*), sebuah algoritma yang menggunakan **Serialisasi *Graph*** unik dari *Program Dependency Graph* (PDG) dan **Mekanisme *Attention* Dua Tahap** untuk memperkaya vektor kode dengan fitur struktural mendalam dan fitur semantik yang ditingkatkan.<br><br>**Contoh Penerapan:**<br>• **Peningkatan Efisiensi Pengembangan:** Digunakan dalam sistem rekomendasi kode atau tugas penyelesaian kode untuk menyediakan *snippet* kode yang lebih akurat dan relevan berdasarkan kueri bahasa alami, meningkatkan produktivitas pengembang.<br><br>**Metodologi:**<br>• **Fitur Kode:** Mengekstrak *Method Name* (Semantic), *Character Tag Sequences* (Semantic), dan *Program Dependency Graph Serialization* (Structural).<br>• **Serialisasi PDG:** Mengubah PDG menjadi urutan enkoding unik menggunakan traversal *depth-first* dengan prioritas tepi tertentu (termasuk *control-dependent* dan *data-dependent*), yang diproses oleh Bi-LSTM untuk fitur struktural.<br>• **Mekanisme *Attention* Dua Tahap:** (1) **Tahap 1:** *Multi-head Attention* diterapkan pada vektor fitur individual (Method Name, Character Tag, PDG Sequence, Deskripsi) untuk *Feature Enhancement*. (2) **Tahap 2:** *Multi-head Attention* diterapkan pada vektor-vektor yang ditingkatkan ini selama fase **Multi-Feature Vector Fusion** untuk mempelajari bobot fitur yang berbeda, menghasilkan *Code Vector* akhir yang diperkaya.<br><br>**Temuan Kunci:**<br>1. **Akurasi Superior:** SENCS meningkatkan metrik akurasi pencarian kode rata-rata sebesar $\mathbf{8.30\%}$ (MRR) dan $\mathbf{17.85\%}$ (DCG), serta $\mathbf{14.86\%}$ dalam SR@1, dibandingkan *baseline* terbaik (GSMM) pada dataset CodeSearchNet dan JavaNet.<br>2. **Kontribusi Mekanisme *Attention***: Mekanisme *Attention* dua tahap meningkatkan akurasi: Tahap 1 (Peningkatan Fitur) meningkatkan MRR sebesar $3.00\%$-$2.31\%$. Tahap 2 (Fusion) meningkatkan MRR sebesar $6.65\%$-$3.85\%$. Kombinasi keduanya (SENCS lengkap) memberikan peningkatan MRR terbesar ($9.44\%$-$5.70\%$).<br>3. **Signifikansi Fitur:** Penghapusan fitur **PDG Sequence** menyebabkan penurunan kinerja yang paling signifikan, menggarisbawahi pentingnya penangkapan fitur struktural mendalam.<br><br>**Kontribusi Utama:**<br>• Mengusulkan **SENCS**, algoritma *neural code search* yang ditingkatkan semantik mendalam, yang didasarkan pada serialisasi PDG dan mekanisme *attention* dua tahap.<br>• Merancang metode Serialisasi *Graph* unik untuk PDG yang mempertahankan informasi struktural yang kaya.<br>• Mendemonstrasikan bahwa mekanisme *attention* dua tahap secara efektif meningkatkan fitur dan menyelaraskan vektor kode/kueri di ruang vektor.<br><br>**Dampak:**<br>• Menyediakan metode yang lebih **akurat** dan **andal** untuk *code search*, memastikan *snippet* kode yang dikembalikan tidak hanya relevan secara tekstual tetapi juga fungsional dan struktural yang sesuai, mempercepat proses pengembangan perangkat lunak skala besar. |

## 1. Pendahuluan & Masalah

*Code search* adalah tugas penting dalam rekayasa perangkat lunak, yang bertujuan untuk mengambil *snippet* kode yang secara semantik mirip dengan kueri bahasa alami dari *codebase* yang luas. Meskipun kemunculan *deep learning* (DL) telah secara signifikan meningkatkan kinerja *code search* tradisional yang berbasis teks dan IR, sebagian besar algoritma DL saat ini, yang sering mengandalkan *Graph Neural Network* (GNN) untuk fitur struktural, masih berjuang untuk menangkap **fitur semantik dan struktural yang lebih dalam** di dalam kode dan struktur *code graph*-nya.

Keterbatasan ini menyebabkan **akurasi pencarian yang lebih rendah** dan hilangnya informasi struktural yang kompleks, seperti detail *control flow* dan *data flow* yang penting. Contoh model seperti DeepCS, MMAN, dan GSMM fokus pada fitur multimodal tetapi sering mengabaikan kedalaman struktural dan mekanisme pembobotan fitur yang canggih.

::: tip Solusi yang Diusulkan
Paper ini mengusulkan **SENCS** (*Deep Semantics-Enhanced Neural Code Search*), sebuah algoritma *code search* neural yang menggabungkan **Serialisasi *Program Dependency Graph* (PDG)** yang unik dengan **Mekanisme *Attention* Dua Tahap** untuk memastikan fitur struktural yang mendalam dan fitur semantik yang ditingkatkan sepenuhnya tertanam dalam representasi vektor kode, sehingga meningkatkan akurasi secara signifikan.
:::

## 2. Metodologi

Kerangka kerja **SENCS** terdiri dari empat tahap utama: generasi vektor fitur semantik, generasi vektor fitur struktural, fusi vektor *multi-feature*, dan pemetaan ruang vektor. Model ini dirancang untuk memetakan vektor kode dan vektor deskripsi bahasa alami ke ruang vektor yang sama untuk perhitungan kesamaan kosinus.

### A. Generasi Vektor Fitur Semantik dan Struktural (Tahap 1 *Attention*)

Model ini mempertimbangkan tiga jenis informasi fitur utama:

1.  **Fitur Nama Metode (Semantic):** Urutan nama metode diproses menggunakan model **Bi-LSTM** untuk menangkap informasi temporal dan kontekstual.
    $$ \vec{h_{t}} = \vec{LSTM}_{m}(h_{t-1}, methodname_{t}) $$
    Max-pooling kemudian diterapkan untuk menghasilkan vektor fitur ($\mathbf{methnaVec}$).
2.  **Fitur *Character Token Sequence* (Semantic):** Mengingat urutan *token* yang panjang, model **Multi-Layer Perceptron (MLP)** digunakan untuk menangkap hubungan non-linear dan fitur lokal, diikuti oleh max-pooling untuk menghasilkan ($\mathbf{tokVec}$).
3.  **Fitur Serialisasi *Graph* PDG (Structural):**
    *   **Serialisasi PDG:** Untuk mengatasi masalah kehilangan struktur akibat urutan traversal *graph* yang tidak unik, SENCS menggunakan metode **traversal *depth-first* yang dispesifikasi** (memprioritaskan tepi *control-dependent* dan *data-dependent*) untuk mengubah PDG menjadi urutan serial yang unik.
    *   **Pembelajaran Fitur Struktural:** Urutan serial ini diproses oleh model **Bi-LSTM** untuk menangkap fitur struktural dan semantik yang mendalam, menghasilkan vektor fitur *graph* ($\mathbf{pdgseqVec}$).

### B. Peningkatan Fitur Semantik dan Struktural (*Feature Enhancement*)

Untuk memperkaya $\mathbf{methnaVec}$, $\mathbf{tokVec}$, $\mathbf{pdgseqVec}$, dan $\mathbf{descVec}$ (vektor deskripsi NL), mekanisme **Multi-Head Attention** diterapkan (Tahap 1 *Attention*).
$$ Head^{i}=softmax(\frac{Q^{i}(K^{i})^{T}}{\sqrt{d_{K}}})V^{i} $$
Vektor output dari semua *head* digabungkan dan ditransformasikan linier untuk menghasilkan vektor yang ditingkatkan seperti $\mathbf{Vector}_{methna}$, $\mathbf{Vector}_{tok}$, $\mathbf{Vector}_{PDGseq}$, dan $\mathbf{Vector}_{desc}$.

### C. Fusi Vektor *Multi-Feature* (Tahap 2 *Attention*)

Setelah peningkatan, ketiga vektor fitur kode yang diperkaya ($\mathbf{Vector}_{methna}$, $\mathbf{Vector}_{tok}$, $\mathbf{Vector}_{PDGseq}$) digabungkan. **Mekanisme *Multi-Head Attention*** kedua diterapkan (Tahap 2 *Attention*) untuk mempelajari bobot informasi di antara ketiga vektor fitur yang berbeda ini, sehingga menghasilkan **Vektor Kode** akhir yang komprehensif.

### D. Pemetaan Ruang Vektor dan Fungsi *Loss*

Vektor Kode dan $\mathbf{Vector}_{desc}$ dipetakan ke ruang vektor berdimensi tinggi yang sama. Kesamaan diukur menggunakan kesamaan kosinus:
$$ \cos (\mathbf{codevector}, \mathbf{descvector}) = \frac{\mathbf{codevector} \cdot \mathbf{descvector}}{\Vert \mathbf{codevector} \Vert \Vert \mathbf{descvector} \Vert} $$
Model dilatih menggunakan *triplet margin ranking loss* untuk memaksimalkan kesamaan antara kode ($\mathbf{c}$) dan deskripsi positif ($\mathbf{d}^{+}$) sambil meminimalkan kesamaan dengan deskripsi negatif ($\mathbf{d}^{-}$):
$$ \mathcal{L}(\theta) = \sum_{\langle\mathbf{c},\mathbf{d}^{+},\mathbf{d}^{-}\rangle \in \mathbf{DATASET}_{P}} \max(0,\in - \cos(\mathbf{c},\mathbf{d}^{+}) + \cos(\mathbf{c},\mathbf{d}^{-})) $$

## 3. Detail Pengujian

### Dataset
*   **CodeSearchNet:** 307,651 (Train), 9709 (Valid), 18,597 (Test).
*   **JavaNet:** 438,111 (Train), 48,678 (Valid), 500,000 (Test).

### Baseline Models
*   **DeepCS:** Model berbasis DL multimodal awal.
*   **MMAN:** Model *multi-modal attention network* yang juga menggunakan PDG (diadaptasi untuk Java).
*   **GSMM:** Model berbasis serialisasi *graph* yang merupakan *baseline* terkuat.

### Metrik Evaluasi
Tiga metrik akurasi digunakan:
1.  **SuccessRate@K (SR@K):** Probabilitas hasil benar pertama berada di *Top K*.
    $$ \mathbf{SR@K} = \frac{1}{|Q_{num}|}\sum_{query}^{Q_{num}}F(Frank_{q}\le K) $$
    Di mana $F(\cdot)$ adalah fungsi diskriminan. Digunakan $\mathbf{SR@1}$, $\mathbf{SR@5}$, $\mathbf{SR@10}$.
2.  **Mean Reciprocal Rank (MRR):** Rata-rata invers peringkat hasil benar pertama.
    $$ \mathbf{MRR} = \frac{1}{Q_{num}}\sum_{query}^{Q_{num}}\frac{1}{Frank_{q}} $$
3.  **Discounted Cumulative Gain (DCG):** Mengukur kualitas daftar peringkat, memberikan bobot lebih pada hasil yang relevan di peringkat yang lebih tinggi.
    $$ \mathbf{DCG} = \sum_{i}^{k}\frac{rel_{i}}{log_{2}(i+1)} $$
    Di mana $rel_{i}$ adalah skor korelasi untuk hasil ke-$i$.

## 4. Hasil Eksperimen

### Perbandingan Akurasi (SENCS vs. Baselines)

| Model | MRR | SR@1 | SR@5 | SR@10 | DCG |
| :--- | :--- | :--- | :--- | :--- | :--- |
| **CodeSearchNet Dataset** | | | | | |
| DeepCS | 0.354 | 0.274 | 0.461 | 0.535 | 0.397 |
| GSMM (Best Baseline) | 0.475 | 0.342 | 0.587 | 0.623 | 0.503 |
| **SENCS** | **0.510** | **0.393** | **0.646** | **0.736** | **0.559** |
| **JavaNet Dataset** | | | | | |
| DeepCS | 0.415 | 0.339 | 0.498 | 0.623 | 0.474 |
| GSMM (Best Baseline) | 0.628 | 0.506 | 0.803 | 0.862 | 0.689 |
| **SENCS** | **0.686** | **0.581** | **0.814** | **0.877** | **0.729** |

**Analisis Hasil:**
*   **Akurasi Superior:** SENCS melampaui semua *baseline* pada kedua dataset. Dibandingkan dengan *baseline* terbaik (GSMM), SENCS menunjukkan peningkatan rata-rata $\mathbf{9.70\%}$ dalam metrik akurasi, dan peningkatan $\mathbf{14.86\%}$ dalam SR@1 (jumlah hasil paling relevan di peringkat teratas).
*   **Keunggulan Serialisasi PDG dan *Attention*:** Kinerja yang unggul ini diatribusikan pada kemampuan SENCS untuk menangkap fitur struktural yang kaya melalui serialisasi PDG yang unik dan mekanisme *attention* dua tahap yang secara efektif membobot dan menggabungkan fitur.

### Analisis Mekanisme *Attention* Dua Tahap

| Model | MRR | SR@1 | DCG |
| :--- | :--- | :--- | :--- |
| **SENCS (Lengkap)** | **0.510** | **0.393** | **0.559** |
| $SENCS_{-w.att}$ (Tanpa *Attention*) | 0.466 | 0.348 | 0.511 |
| $SENCS_{-w.att1}$ (Tanpa Peningkatan Fitur) | 0.480 | 0.362 | 0.537 |
| $SENCS_{-w.att2}$ (Tanpa Fusion) | 0.497 | 0.378 | 0.544 |

**Analisis:**
*   Kombinasi kedua tahap *attention* (SENCS lengkap) memberikan peningkatan kinerja terbaik (MRR meningkat $\mathbf{9.44\%}$ dibandingkan model tanpa *attention*).
*   **Tahap Fusion (*att2*)** menyumbang peningkatan akurasi yang lebih besar (MRR $0.497$) daripada **Tahap Peningkatan Fitur (*att1*)** (MRR $0.480$). Ini menunjukkan bahwa mekanisme *attention* sangat penting dalam fase fusi untuk mempelajari bobot di antara vektor fitur kode yang berbeda.

### Analisis Ablasi Fitur

Penghapusan fitur **PDG Sequence** ($\mathbf{SENCS-w.graseq}$) menghasilkan penurunan kinerja yang paling signifikan, terutama dalam SR@1, dibandingkan dengan penghapusan fitur Method Name ($\mathbf{SENCS-w.mname}$). Ini menegaskan bahwa **fitur struktural PDG** yang ditangkap melalui serialisasi adalah **komponen paling krusial** dalam model SENCS untuk mencapai akurasi tinggi.

## 5. Kesimpulan

Paper ini memperkenalkan **SENCS**, sebuah algoritma *neural code search* canggih yang mengatasi keterbatasan model yang ada dalam menangkap fitur semantik dan struktural yang mendalam. Dengan memanfaatkan **Serialisasi PDG** dan **Mekanisme *Attention* Dua Tahap**, SENCS berhasil menghasilkan vektor kode yang diperkaya, secara signifikan meningkatkan akurasi *code search*. Eksperimen ekstensif pada dua dataset besar menunjukkan bahwa SENCS mencapai efek pencarian terbaik dibandingkan dengan model *state-of-the-art*, membuktikan efektivitas pendekatan yang diusulkan.

::: info Dampak Praktis
Signifikansi praktis SENCS terletak pada kemampuannya untuk secara substansial **meningkatkan efisiensi dan efektivitas** *code search* bagi pengembang perangkat lunak. Dengan menyediakan hasil yang **lebih akurat** dan **relevan secara kontekstual** (mempertimbangkan logika dan struktur kode, bukan hanya teks), SENCS mengurangi waktu yang dibutuhkan pengembang untuk menemukan *snippet* kode yang sesuai, sehingga mempercepat proses pengembangan, terutama dalam proyek perangkat lunak skala besar.
:::