---
title: Review Paper - CORES Summarisasi Representasi Kode untuk Pencarian Kode
description: Rangkuman paper tentang kerangka CORES untuk ringkasan representasi multi-view dalam pencarian kode (IEEE Transactions on Consumer Electronics, 2024).
head:
  - - meta
    - name: keywords
      content: Code search, code summarization, multi-view representation, attention mechanism, GNN, BERT
---

# 041 - CORES: COde REpresentation Summarization for Code Search
Tautan (DOI) [10.1109/TCE.2024.3444458]

**Penulis:** **Jiali Zeng** ᵃ, **Bo Liu** ᵃ*, **Xiangzheng Liu** ᵃ, **Lu Tang** ᵃ, **Zhiqiang Zhu** ᵃ, **Shengping Yao** ᵇ

**Afiliasi:**
* ᵃ School of Computer Science and Engineering, Hunan University of Science and Technology, Xiangtan 411201, China
* ᵇ School of Computer Science and Technology, Wuhan University of Technology, Wuhan 430070, China

**Kronologi:** Received: 29 April 2024 • Revised: 8 July 2024 • Accepted: 29 July 2024 • Available Online: 16 August 2024

<a href="https://www.scimagojr.com/journalsearch.php?q=26048&tip=sid" target="_blank"><img src="https://www.scimagojr.com/journal_img.php?id=26048" alt="SCImago Journal & Country Rank" /></a>

| Resume Eksekutif |
| :--- |
| **Publikasi:**<br>• **Jurnal:** IEEE Transactions on Consumer Electronics, Vol. 70, No. 4, pp. 638-649 (Nov 2024)<br>• **Topik:** Peningkatan efektivitas *code search* dengan mengatasi masalah **redudansi** dan **kehilangan informasi penting** dalam representasi multi-view kode (Token, AST, CFG).<br><br>**Masalah & Solusi:**<br>• **Masalah 1 (Kualitas Representasi):** Model *code search* multi-view (*multi-modal*) yang ada seringkali menghasilkan representasi yang **redundant** (duplikasi fitur) dan kehilangan fokus pada elemen kode yang paling relevan secara semantik, yang menyebabkan akurasi yang suboptimal.<br>• **Solusi:** Mengusulkan kerangka **CORES** (*COde REpresentation Summarization*) yang beroperasi dalam dua tahap: (1) **Penghapusan Redundansi:** Menggunakan *Attention Fusion Module* untuk menyatukan representasi multi-view menjadi vektor tunggal yang ringkas (representasi multi-view lokal). (2) **Penyorotan Fitur:** Menggunakan *Cross-Modal Attention* untuk menyoroti bagian kueri dan kode yang paling relevan secara timbal balik, menghasilkan *Global Context Vector* yang komprehensif.<br><br>**Contoh Penerapan:**<br>• Dievaluasi pada *dataset* **CodeSearchNet** (6 bahasa) dan **CosBench** (kueri StackOverflow). CORES digunakan untuk *retrieval* kode sumber yang cocok dengan kueri bahasa alami (NL) dari *codebase* besar.<br><br>**Metodologi:**<br>• **Arsitektur:** Menggunakan model *dual encoder* dengan **BERT** sebagai *encoder* kueri, dan **GNN, Bi-LSTM** sebagai *encoder* multi-view kode (Token, AST, CFG).<br>• **Code Representation Summarization (CRS):** Terdiri dari dua lapisan: **(1) Lokal (Intra-view Fusion):** Menggunakan *Multi-Head Attention* dan *Weighted Fusion* untuk menggabungkan Token, AST, dan CFG menjadi satu representasi kode ringkas. **(2) Global (Cross-Modal Attention):** Menggunakan *Attention* untuk membandingkan kode ringkas dengan kueri, mendapatkan bobot kontekstual timbal balik.<br>• **Loss Function:** Menggunakan **Triplet Loss** dengan *hard negative sampling* untuk memaksimalkan margin antara pasangan positif dan negatif.<br>$$\mathcal{L}=\sum_{i}\max(0, \alpha - \cos(\mathbf{q},\mathbf{c}^{+}) + \cos(\mathbf{q},\mathbf{c}^{-}))$$<br><br>**Temuan Kunci:**<br>1. **Kinerja SOTA:** CORES mencapai kinerja MRR tertinggi secara keseluruhan (AVG MRR **0.781** pada CSN), mengungguli *baseline* SOTA seperti GraphCodeBERT, UniXcoder, dan CodeT5. Kinerja di CosBench juga melampaui SOTA.<br>2. **Peran Summarization:** Kontribusi terbesar berasal dari modul **CRS (Summarisasi Representasi Kode)** yang meningkatkan MRR sebesar **2.3%** dibandingkan tanpa CRS, membuktikan bahwa penghapusan redudansi pada representasi multi-view sangat penting.<br>3. **Fusion Optimal:** Strategi *Weighted Fusion* yang digunakan di lapisan CRS menghasilkan MRR tertinggi, menunjukkan bahwa tidak semua modalitas (Token, AST, CFG) harus diperlakukan sama.<br><br>**Kontribusi Utama:**<br>• Mengusulkan kerangka **CORES** novel untuk *code search* yang secara eksplisit melakukan ringkasan representasi multi-view kode.<br>• Merancang arsitektur ringkasan kode dua tahap (Lokal dan Global) untuk menghilangkan redudansi dan menyoroti fitur semantik yang paling relevan.<br>• Melakukan eksperimen ekstensif untuk membuktikan efektivitas *Representation Summarization* di berbagai *dataset* dan bahasa.<br><br>**Dampak:**<br>• CORES menyediakan metodologi yang lebih efektif untuk *code search* dengan mengatasi masalah redudansi bawaan dalam fitur multi-modal, yang meningkatkan akurasi dan keandalan pengambilan kode sumber dalam sistem *software engineering*. |

## 1. Pendahuluan & Masalah

Pencarian kode (*Code search*) adalah tugas mendasar dalam rekayasa perangkat lunak, yang bertujuan untuk mengambil *snippet* kode yang relevan dari *codebase* besar berdasarkan kueri bahasa alami. Model *deep learning* saat ini berupaya menjembatani kesenjangan semantik dengan menggunakan representasi **multi-view** (multimodal), menggabungkan fitur dari Token, *Abstract Syntax Tree* (AST), dan *Control Flow Graph* (CFG).

Namun, pendekatan *multi-view* yang ada menghadapi dua tantangan kritis:

1.  **Redundansi Representasi:** Fitur-fitur dari modalitas yang berbeda (misalnya, Token dan AST) seringkali memiliki informasi yang **tumpang tindih** atau **redundant**. Penggabungan fitur-fitur ini secara sederhana (misalnya, *concatenation*) dapat menghasilkan vektor dimensi tinggi yang memiliki *noise* dan kurang diskriminatif.
2.  **Kehilangan Fokus Semantik:** Model gagal menyoroti bagian kode yang paling penting dan secara kontekstual relevan dengan kueri, yang diperlukan untuk pemadanan semantik yang akurat.

Kombinasi dari redudansi dan kurangnya fokus ini mengakibatkan representasi kode yang berkualitas rendah dan akurasi *code search* yang suboptimal.

::: tip Solusi yang Diusulkan
Paper ini mengusulkan **CORES** (*COde REpresentation Summarization*), sebuah kerangka kerja yang memperkenalkan **ringkasan representasi kode** (*code representation summarization*) dua tahap. CORES bertujuan untuk mengatasi redudansi representasi multi-view dan meningkatkan akurasi *code search* dengan memaksimalkan informasi yang relevan dan minimal.
:::

## 2. Metodologi

CORES menggunakan arsitektur *dual encoder* dengan **BERT** sebagai *encoder* kueri dan gabungan **GNN/Bi-LSTM** sebagai *encoder* multi-view kode. Inti dari CORES adalah **Modul Ringkasan Representasi Kode (CRS)**.

### A. Code Representation Summarization (CRS)

CRS beroperasi dalam dua lapisan *attention* untuk menghasilkan vektor ringkas:

1.  **Ringkasan Lokal (Intra-view Fusion):**
    * Tiga modalitas fitur kode (Token, AST, CFG) di-*embed* menggunakan *encoders* spesifik (Bi-LSTM untuk Token, GNN untuk AST/CFG).
    * Vektor dari ketiga *encoder* ini digabungkan melalui *Multi-Head Attention* dan **Weighted Fusion** (*weighted fusion*). *Weighted Fusion* dipilih karena terbukti lebih baik daripada *concatenation* sederhana dalam menghasilkan satu representasi ringkas kode yang bebas dari redudansi internal.
    * Hasilnya adalah representasi *ringkas lokal* kode ($\mathbf{V}_{code}^L$).

2.  **Ringkasan Global (Cross-Modal Attention):**
    * Tahap ini berfokus pada hubungan timbal balik antara kueri dan kode.
    * Mekanisme **Cross-Modal Attention** diterapkan untuk membandingkan vektor kueri ($\mathbf{V}_{query}$) dengan representasi ringkas kode lokal ($\mathbf{V}_{code}^L$).
    * Bobot perhatian yang dihasilkan digunakan untuk mendapatkan **vektor konteks global** ($\mathbf{V}_{code}^G$) yang menyoroti bagian kode yang paling relevan dengan niat kueri.
    * Representasi kode akhir adalah fusi dari vektor lokal dan global: $\mathbf{V}_{code} = \mathbf{V}_{code}^L \oplus \mathbf{V}_{code}^G$.

### B. Similarity Learning dan Loss Function

Model dilatih menggunakan **Triplet Loss** dengan *hard negative sampling*.
$$\mathcal{L}=\sum_{i}\max(0, \alpha - \cos(\mathbf{q}_i,\mathbf{c}_{i}^{+}) + \cos(\mathbf{q}_i,\mathbf{c}_{i}^{-}))$$
Tujuan pelatihan adalah memaksimalkan jarak antara *embedding* kueri ($\mathbf{q}$) dan *snippet* kode positif ($\mathbf{c}^{+}$) dibandingkan dengan *snippet* kode negatif ($\mathbf{c}^{-}$) dengan margin ($\alpha$). Sampel negatif keras digunakan untuk mempercepat pembelajaran diskriminatif model.

## 3. Detail Pengujian

### Dataset
* **CodeSearchNet (CSN):** Dataset utama yang mencakup enam bahasa (Python, Java, JavaScript, PHP, Ruby, Go).
* **CosBench:** Dataset tambahan yang berfokus pada kueri StackOverflow, digunakan untuk memvalidasi kinerja pengambilan kode di skenario dunia nyata.

### Baseline
Model dibandingkan dengan *baseline* SOTA yang kuat: **GraphCodeBERT, UniXcoder, CodeT5, MoCoCS, dan CoCoSoDa** (model *multi-modal* terbaik).

### Metrik Evaluasi
Metrik utama yang digunakan adalah **Mean Reciprocal Rank (MRR)**.
$$MRR=\frac{1}{|Q|}\sum_{j=1}^{|Q|}\frac{1}{Rank_{j}}$$
Di mana $Q$ adalah jumlah kueri, dan $Rank_j$ adalah posisi peringkat *snippet* kode yang benar pertama.

## 4. Hasil Eksperimen

### RQ1: Kinerja SOTA (MRR Rata-rata)

| Model | MRR Rata-rata (CSN) | MRR Rata-rata (CosBench) |
| :--- | :--- | :--- |
| UniXcoder | 0.765 | 0.865 |
| MoCoCS | 0.771 | 0.867 |
| **CORES** | **0.781** ($\mathbf{1.3\% \uparrow}$ vs MoCoCS) | **0.875** ($\mathbf{0.9\% \uparrow}$ vs MoCoCS) |

**Analisis:** CORES mencapai kinerja *state-of-the-art* baru, melampaui SOTA sebelumnya (MoCoCS dan UniXcoder) di kedua *dataset* besar. Keunggulan di CosBench menunjukkan bahwa mekanisme ringkasan sangat efektif untuk skenario kueri yang kompleks dan nyata.

### RQ2: Kontribusi Modul Ringkasan (Ablasi)

Studi ablasi mengonfirmasi peran penting Modul Ringkasan Representasi Kode (CRS).

* **CORES (Lengkap) MRR:** **0.781**
* **w/o CRS (Fusion Sederhana) MRR:** **0.758** ($\mathbf{\downarrow 2.3\%}$)

**Analisis:** Penghapusan Modul CRS menyebabkan penurunan MRR sebesar $\mathbf{2.3\%}$, membuktikan bahwa strategi **ringkasan dua tahap** (menghilangkan redudansi dan menyoroti fokus) adalah komponen paling penting untuk peningkatan kinerja.

### RQ3: Efek Strategi Fusion Lokal

Perbandingan dilakukan antara strategi *fusion* untuk menggabungkan Token, AST, dan CFG:

| Strategi Fusion Lokal | MRR Rata-rata |
| :--- | :--- |
| Concatenation (Sederhana) | 0.770 |
| *Attention-based Average* | 0.773 |
| **Weighted Fusion (CORES)** | **0.781** |

**Analisis:** Strategi **Weighted Fusion** (yang digunakan CORES) mengungguli *concatenation* dan *average pooling* sederhana, menunjukkan bahwa fitur multimodal tidak memiliki kontribusi yang sama dan perlu dibobotkan secara berbeda untuk mencapai representasi kode yang paling ringkas dan efektif.

## 5. Kesimpulan

CORES adalah kerangka kerja *code search* berbasis representasi yang inovatif, yang berhasil mengatasi tantangan **redudansi representasi** dan **kehilangan fokus semantik** yang melekat pada model *multi-view* sebelumnya. Dengan memperkenalkan ringkasan representasi kode dua tahap (Lokal dan Global *Cross-Modal*), CORES mencapai kinerja *state-of-the-art* pada dataset CodeSearchNet dan CosBench.

Keberhasilan CORES memvalidasi bahwa optimasi pembelajaran representasi dengan secara eksplisit menghasilkan vektor yang ringkas dan bebas redudansi adalah arah kunci untuk penelitian *code search* di masa depan.

::: info Dampak Praktis
CORES menyediakan metodologi yang sangat efektif untuk meningkatkan **akurasi** dan **keandalan** mesin pencari kode. Dengan memfokuskan model pada semantik yang paling relevan (penyorotan fitur), CORES dapat secara signifikan meningkatkan pengalaman pengembang dan mempercepat penggunaan kembali kode yang berkualitas dalam sistem rekayasa perangkat lunak skala besar.
:::