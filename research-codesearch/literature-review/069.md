---
title: Review Paper - Pengayaan Semantik Kueri untuk Pencarian Kode dengan Reinforcement Learning
description: Rangkuman paper tentang QueCos, model pencarian kode berbasis Reinforcement Learning untuk mengurangi semantic gap antara kueri dan deskripsi (Neural Networks, 2022).
head:
  - - meta
    - name: keywords
      content: Code search, Query semantics, Semantic enrichment, Reinforcement learning, QueCos
---

# 069 - Enriching query semantics for code search with reinforcement learning
Tautan (DOI) [https://doi.org/10.1016/j.neunet.2021.09.025]

**Penulis:** **Chaozheng Wang** $^{a,1}$, **Zhenhao Nong** $^{a,1*}$, **Cuiyun Gao** $^{a}$, **Zongjie Li** $^{a}$, **Jichuan Zeng** $^{b}$, **Zhenchang Xing** $^{c}$, **Yang Liu** $^{d}$

**Afiliasi:**
* $^{a}$ School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen, China
* $^{b}$ Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong, China
* $^{c}$ Research School of Computer Science, Australian National University, Australia
* $^{d}$ School of Computer Science and Engineering, Nanyang Technology University, Singapore

**Kronologi:** Received: 19 April 2021 • Revised: 25 Juli 2021 • Accepted: 27 September 2021 • Available Online: 11 Oktober 2021

<a href="https://www.scimagojr.com/journalsearch.php?q=24804&tip=sid" target="_blank"><img src="https://www.scimagojr.com/journal_img.php?id=24804" alt="SCImago Journal & Country Rank" /></a>

| Resume Eksekutif |
| :--- |
| **Publikasi:**<br>• **Jurnal:** Neural Networks 145 (2022)<br>• **Topik:** Meningkatkan akurasi pencarian kode (*code search*) dengan mengatasi **kesenjangan semantik (*semantic gap*)** antara kueri pengguna yang pendek dan deskripsi kode yang panjang.<br><br>**Masalah & Solusi:**<br>• **Masalah:** Model *code search* berbasis *Deep Learning* (DL) dilatih pada pasangan *Code-Description* (Kode-Deskripsi). Namun, kueri pengguna (NL) seringkali lebih pendek dan memiliki semantik yang berbeda dari deskripsi yang dibuat oleh pengembang. Model yang dilatih pada deskripsi cenderung berkinerja buruk pada kueri pengguna aktual (terjadi *semantic distance*).<br>• **Solusi:** Mengusulkan **QueCos** (*Query-enriched Code search model*). QueCos menggunakan **Reinforcement Learning (RL)** untuk melatih model *Sequence-to-Sequence* (*Query Semantic Enrichment*, QSE) agar menghasilkan **kueri yang diperkaya secara semantik** (*semantic enriched queries*). Performa pencarian kode secara eksplisit digunakan sebagai *reward* untuk memandu proses pengayaan semantik.<br><br>**Contoh Penerapan:**<br>• Diuji pada *dataset* CodeSearchNet (Java, Python) dan *dataset* baru yang dikumpulkan (berisi triples: **Code-Description-Query**).<br>• QueCos berbasis DeepCS mencapai peningkatan $\mathbf{28.4\%}$ pada R@1 dan $\mathbf{23.9\%}$ pada MRR (rata-rata Java), dan secara konsisten mengungguli empat model *baseline* DL (*DeepCS, UNIF, OCOR, CodeBERT*) ketika diuji pada kueri pengguna.<br><br>**Metodologi:**<br>• **Arsitektur:** Terdiri dari tiga komponen utama: (1) *Code Search* (CS) Model yang sudah terlatih, (2) *Query Semantic Enrichment* (QSE) Model berbasis bi-directional LSTM dengan mekanisme *Attention*, dan (3) *Hybrid Ranking*.<br>• **QSE Training via RL:** Proses QSE (generasi deskripsi) dimodelkan sebagai *Markov Decision Process* (MDP). Algoritma **A2C (Advantage Actor-Critic)** digunakan untuk mengoptimalkan *policy gradient*.<br>• **Fungsi Reward:** Kualitas kueri yang diperkaya dievaluasi berdasarkan dua faktor yang disatukan dalam *reward* $r$: $\mathbf{Rank}$ (menggunakan MRR CS Model sebagai pengukur) dan **BLEU** (*relevancy* terhadap deskripsi *ground truth*).<br>$$r(s_{t},d_{t})=\begin{cases}\alpha \times \text{Rank}(C,d_{1..t-1})+ (1-\alpha) \times \text{BLEU}(d_{1..t-1},d^{\text{gt}}) & \text{if } d_{t}=\langle \text{EOS} \rangle \\ 0 & \text{otherwise}\end{cases}$$<br>   - $\alpha=1.0$ digunakan, memprioritaskan performa *Rank* sebagai *reward* tunggal.<br>• **Hybrid Ranking (HR):** Skor pencarian akhir menggabungkan kesamaan kueri asli ($q$) dan kueri yang diperkaya ($q'$) dengan kode kandidat ($c$):<br>$$score(q,c)=\beta \times \text{sim}(q',c)+(1-\beta) \times \text{sim}(q,c)$$<br>   - $\beta=0.6$ digunakan untuk menyesuaikan bobot.<br><br>**Temuan Kunci:**<br>1. **Adanya Semantic Gap:** Studi awal membuktikan bahwa model yang dilatih pada Deskripsi berkinerja jauh lebih rendah pada Kueri pengguna (penurunan MRR rata-rata $18.7\%$ pada Java dan $37.4\%$ pada Python).<br>2. **Keunggulan QueCos:** QueCos secara signifikan mengungguli semua *baseline* pada kueri pengguna di kedua *dataset*, memvalidasi efektivitas pengayaan semantik yang dipandu RL.<br>3. **Pentingnya RL:** Model tanpa komponen RL (*QueCos w/o RL*) berkinerja buruk, membuktikan bahwa hanya menghasilkan deskripsi (tanpa umpan balik performa pencarian) dapat menimbulkan bias semantik.<br>4. **QE Kurang Efektif:** Pendekatan *Query Expansion* (QE) berbasis sinonim (WordNet) seringkali tidak memberikan manfaat, bahkan menurunkan performa *baseline*.<br><br>**Kontribusi Utama:**<br>• Menyajikan *dataset* pertama yang berisi tiga rangkap **Kode, Deskripsi, dan Kueri Pengguna** yang selaras.<br>• Mengusulkan **QueCos**, model yang memanfaatkan **Reinforcement Learning** untuk menghasilkan kueri yang diperkaya semantik, menggunakan performa pencarian kode sebagai *reward* untuk memitigasi *semantic gap*.<br><br>**Dampak:**<br>• Menyediakan metode yang lebih **akurat dan praktis** untuk *code search* karena model dilatih secara eksplisit untuk menjembatani kesenjangan antara kueri pengguna sehari-hari dan representasi kode, yang meningkatkan produktivitas pengembang. |

## 1. Pendahuluan & Masalah

Pencarian korpus kode yang besar adalah praktik umum bagi pengembang. Tantangan utama dalam *code search* adalah *knowledge gap* atau **kesenjangan semantik** antara kode sumber dan bahasa alami (kueri) yang digunakan pengembang.

Model *code search* berbasis *Deep Learning* (DL) terbaru (seperti DeepCS, OCOR) berfokus pada pembelajaran relasi pencocokan semantik antara kode dan **deskripsi kode** yang disediakan pengembang. Mereka berasumsi bahwa jarak semantik antara deskripsi dan kueri pengguna adalah minimal.

Namun, studi awal menunjukkan bahwa **kueri pengguna** (yang umumnya lebih pendek, misal $11.97$ token pada Python) berkinerja jauh lebih buruk pada model terlatih daripada deskripsi kode (misal $73.61$ token pada Python), menandakan adanya **kesenjangan semantik** yang signifikan. Kesenjangan ini membuat model *code search* DL yang ada tidak efektif untuk kueri pengguna di dunia nyata.

::: tip Solusi yang Diusulkan
Untuk memitigasi *semantic distance* ini, diusulkan **QueCos** (*Query-enriched Code search model*). QueCos menggunakan **Reinforcement Learning (RL)** untuk menghasilkan *semantic enriched queries* (kueri yang diperkaya secara semantik). Kunci inovasinya adalah menggunakan **performa pencarian kode** (diukur dengan MRR) secara eksplisit sebagai *reward* untuk memandu RL, memastikan kueri yang diperkaya menghasilkan hasil pencarian yang lebih akurat.
:::

## 2. Metodologi

Kerangka kerja QueCos terdiri dari tiga komponen yang bekerja secara berurutan: *Code Search* (CS), *Query Semantic Enrichment* (QSE), dan *Hybrid Ranking* (HR).

### A. Code Search (CS)

Komponen CS bertujuan mempelajari representasi vektor terpadu untuk potongan kode ($C$) dan deskripsi ($D$) dalam ruang semantik yang sama.
*   **Pelatihan:** Model CS dilatih untuk meminimalkan *ranking loss* pada triplet $\langle D, c^{+}, c^{-} \rangle$.
*   **Fungsi Loss:**
    $$\mathcal{L}(\theta)_{CS}=\sum_{\langle D,c^{+},c^{-}\rangle}\max(0,\epsilon-\text{sim}(d,c^{+})+\text{sim}(d,c^{-}))$$
    Di sini, $d, c^{+}, c^{-}$ adalah vektor terenkode dari deskripsi, kode positif, dan kode negatif. $\epsilon$ adalah margin konstan (ditetapkan $0.05$).

### B. Query Semantic Enrichment (QSE)

QSE adalah model *Sequence-to-Sequence* berbasis **bi-directional LSTM dengan mekanisme *Attention***. Tujuannya adalah menghasilkan teks yang diperkaya ($d$) dari kueri masukan ($q$), di mana teks yang dihasilkan ini bertindak sebagai kueri yang diperkaya semantik ($q'$).

#### Pelatihan QSE melalui Reinforcement Learning (RL)
Proses generasi deskripsi dimodelkan sebagai *Markov Decision Process* (MDP) dan dioptimalkan menggunakan algoritma **Advantage Actor-Critic (A2C)**.

1.  **Aksi:** Memilih kata berikutnya $d_{t}$ dari kosakata.
2.  **Reward:** *Reward* $r$ dirancang untuk menilai apakah kueri yang diperkaya dapat memberi peringkat pada kode yang relevan lebih tinggi, dan juga mengukur relevansi dengan deskripsi *ground truth* ($d^{\text{gt}}$) (meskipun ini tidak digunakan dalam konfigurasi akhir).
    $$r(s_{t},d_{t})=\begin{cases}\alpha \times \text{Rank}(C,d_{1..t-1})+ (1-\alpha) \times \text{BLEU}(d_{1..t-1},d^{\text{gt}}) & \text{if } d_{t}=\langle \text{EOS} \rangle \\ 0 & \text{otherwise}\end{cases}$$
    Nilai $\alpha=1.0$ (ditemukan optimal dalam eksperimen) digunakan, yang berarti *reward* **hanya** didasarkan pada performa *Rank* (menggunakan MRR) dari kode yang dicari, memfokuskan RL untuk menghasilkan kueri yang paling efektif untuk tugas pencarian.
3.  **Optimasi:** Fungsi *gradient* kebijakan dioptimalkan dengan A2C untuk menstabilkan pelatihan.

### C. Hybrid Ranking (HR)

Pada fase pengujian, komponen *Hybrid Ranking* menggabungkan skor kecocokan dari kueri asli ($q$) dan kueri yang diperkaya ($q'$) untuk memberikan hasil akhir:
$$score(q,c)=\beta \times \text{sim}(q',c)+(1-\beta) \times \text{sim}(q,c)$$
Parameter $\beta$ (ditetapkan $0.6$) mengontrol bobot relatif antara kueri yang diperkaya ($q'$) dan kueri asli ($q$).

## 3. Detail Pengujian

### Dataset
1.  **CodeSearchNet:** Digunakan untuk pelatihan awal model CS (pasangan Kode-Deskripsi).
2.  **Dataset Kumpulan Sendiri (Ours):** Dataset pertama yang berisi triplet **Code-Description-Query** yang selaras. Diperoleh dari SOTorrent (Stack Overflow - GitHub), dan dibagi menjadi 8:1:1 (Latih:Validasi:Uji).
    *   Java: 11.252 triplet. Rata-rata panjang kueri $12.91$ vs Deskripsi $55.75$.
    *   Python: 26.237 triplet. Rata-rata panjang kueri $11.97$ vs Deskripsi $73.61$.

### Metrik Evaluasi
1.  **R@k (Recall at k):** Menilai apakah jawaban yang benar berada di antara $k$ hasil teratas.
    $$R@k=\frac{1}{|Q|}\sum_{q=1}^{|Q|}\delta(\text{FRank}_{q}\le k)$$
2.  **MRR (Mean Reciprocal Rank):** Rata-rata kebalikan peringkat jawaban yang benar.
    $$MRR=\frac{1}{|Q|}\sum_{q=1}^{|Q|}\frac{1}{\text{FRank}_{q}}$$

### Baseline
*   **Model CS:** DeepCS, UNIF, OCOR, CodeBERT.
*   **Model *Query Expansion*:** QE (Menggunakan WordNet untuk memperluas kueri dengan sinonim).

## 4. Hasil Eksperimen

### A. Performa QueCos vs. Baseline (Tabel 3)

| Pendekatan | Java (R@1) | Java (MRR) | Python (R@1) | Python (MRR) |
| :--- | :--- | :--- | :--- | :--- |
| DeepCS | 0.180 | 0.278 | 0.100 | 0.161 |
| **QueCos (DeepCS-based)** | **0.282** | **0.394** | **0.149** | **0.224** |
| CodeBERT | 0.219 | 0.355 | 0.142 | 0.232 |
| **QueCos (CodeBERT-based)** | **0.255** | **0.420** | **0.174** | **0.295** |
| QE (DeepCS-based) | 0.169 | 0.266 | 0.105 | 0.167 |
| QueCos w/o RL (DeepCS) | 0.110 | 0.180 | 0.048 | 0.095 |
| QueCos w/o HR (DeepCS) | 0.192 | 0.294 | 0.117 | 0.182 |

**Analisis Kinerja Utama:**
*   **Keunggulan QueCos:** QueCos secara signifikan meningkatkan performa semua model *baseline* pada kueri pengguna. Rata-rata peningkatan R@1 adalah $28.4\%$ dan MRR $23.9\%$ pada *dataset* Java.
*   **Pentingnya RL:** *QueCos w/o RL* (tanpa *reward* performa pencarian) menunjukkan kinerja terburuk. Ini menegaskan bahwa hanya menghasilkan deskripsi (tanpa umpan balik RL) dapat menimbulkan **bias semantik** karena kueri yang pendek sulit untuk secara akurat menghasilkan deskripsi yang panjang. RL berfungsi sebagai mekanisme *quality assurance* berbasis performa.
*   **Efek QE:** Pendekatan *Query Expansion* (QE) tidak efektif; pada kasus DeepCS-based Java, QE bahkan sedikit menurunkan R@1 ($0.180 \rightarrow 0.169$).
*   **Kontribusi HR:** Komponen *Hybrid Ranking* (*QueCos w/o HR* vs. *QueCos*) menunjukkan bahwa mengintegrasikan kueri asli dengan kueri yang diperkaya lebih efektif daripada hanya menggunakan kueri yang diperkaya.

### B. Analisis Parameter $\alpha$ (Reward Tuning)

Eksperimen menunjukkan bahwa seiring dengan peningkatan $\alpha$, performa (R@1 dan MRR) QueCos meningkat. Hasil ini mengindikasikan bahwa mempertimbangkan kesamaan BLEU (relevansi dengan deskripsi *ground truth*) sebagai *reward* **tidak membantu** tugas pencarian kode. Oleh karena itu, *reward tuning parameter* $\alpha$ ditetapkan $\mathbf{1.0}$, yang berarti *reward* hanya berasal dari skor *Rank* (MRR).

## 5. Kesimpulan

Paper ini berhasil mengidentifikasi dan mengatasi **kesenjangan semantik** yang terabaikan antara kueri pengguna yang singkat dan deskripsi kode yang lebih panjang dalam tugas *code search*. Diusulkan **QueCos**, sebuah model berbasis **Reinforcement Learning** yang secara unik menggunakan performa *code search* (MRR) sebagai *reward* untuk menghasilkan kueri yang diperkaya secara semantik. QueCos secara signifikan mengungguli model *state-of-the-art* DL, membuktikan bahwa pengayaan semantik yang dipandu performa adalah kunci untuk *code search* yang efektif. Selain itu, paper ini merilis *dataset* pertama yang berisi tiga rangkap **Kode-Deskripsi-Kueri** yang selaras.

::: info Dampak Praktis
QueCos menyediakan metode yang **lebih akurat dan praktis** untuk *code search* di lingkungan pengembangan yang nyata. Dengan memitigasi kesenjangan antara kueri alami dan repositori kode, model ini secara langsung meningkatkan produktivitas pengembang. Pekerjaan di masa depan akan mencakup penggabungan *knowledge graph* eksternal untuk memperkaya semantik kueri lebih lanjut.
:::