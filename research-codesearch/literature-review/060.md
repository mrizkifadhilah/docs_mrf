---
title: Review Paper - Peningkatan Pencarian Kode Semantik dengan Deep Graph Matching
description: Rangkuman paper tentang SeCoDeGrMa untuk Semantic Code Retrieval (IEEE Access, 2023).
head:
  - - meta
    - name: keywords
      content: Codesearchnet, natural language, deep neural networks, semantic matching, GNN, source code reuse, recommendation system
---

# 060 - Enhancing Semantic Code Search With Deep Graph Matching
Tautan (DOI) [https://doi.org/10.1109/ACCESS.2023.3263878]

**Penulis:** **Nazia Bibi** $^{1}$, **Ayesha Maqbool** $^{1}$, **Tauseef Rana** $^{1}$, **Farkhanda Afzal** $^{2}$, **Ali Akgül** $^{3,4,5*}$, **Sayed M. Eldin** $^{6}$

**Afiliasi:**
* $^{1}$ Department of Computer Software Engineering, National University of Sciences and Technology (NUST), Islamabad 44000, Pakistan
* $^{2}$ Department of Humanities and Basic Sciences, National University of Sciences and Technology (NUST), Islamabad 44000, Pakistan
* $^{3}$ Department of Computer Science and Mathematics, Lebanese American University, Beirut 56100, Lebanon
* $^{4}$ Department of Mathematics, Art and Science Faculty, Siirt University, 56100 Siirt, Turkey
* $^{5}$ Department Mathematics, Mathematics Research Center, Near East University, 99138 Nicosia, Turkey
* $^{6}$ Center of Research, Faculty of Engineering, Future University in Egypt, New Cairo 11835, Egypt

**Kronologi:** Received: 13 March 2023 • Revised: 24 March 2023 • Accepted: 24 March 2023 • Available Online: 3 April 2023

<a href="https://www.scimagojr.com/journalsearch.php?q=21100374601&tip=sid" target="_blank"><img src="https://www.scimagojr.com/journal_img.php?id=21100374601" alt="SCImago Journal & Country Rank" /></a>

| Resume Eksekutif |
| :--- |
| **Publikasi:**<br>• **Jurnal:** IEEE Access, Vol. 11 (2023)<br>• **Topik:** Peningkatan pencarian kode semantik (*semantic code search*) dengan mengatasi kesenjangan semantik antara kueri bahasa alami dan kode sumber menggunakan *deep graph matching*.<br><br>**Masalah & Solusi:**<br>• **Masalah 1 (Kesenjangan Semantik):** Pencarian kode memerlukan penentuan kesamaan semantik antara kueri (NL) dan kode (PL). Pendekatan DL standar (misalnya, DeepCS) memperlakukan keduanya sebagai *embedding* independen dan gagal menangkap informasi struktural dan ketergantungan jarak jauh (*long-range dependencies*) dalam kode.<br>• **Masalah 2 (Kurangnya Granularitas):** Model *sequence encoding* (seperti RNN/LSTM) tidak mampu menangkap hubungan struktural yang kaya dan hubungan semantik halus (*fine-grained*) antara kueri dan kode.<br>• **Solusi:** Mengusulkan model **SeCoDeGrMa** (*Semantic Code Search using Deep Graph Matching*). Model ini merepresentasikan **kueri dan kode sebagai graf terstruktur** (berbasis AST/Parse Tree), menggunakan **RGCN (Relational Graph Convolutional Networks)** untuk *encoding* node, dan menerapkan **Semantic Matching Operations (SMO)** berbasis mekanisme *cross-attention* untuk mempelajari kesamaan semantik yang mendalam dan halus.<br><br>**Contoh Penerapan:**<br>• Dievaluasi pada dataset *benchmark* **CodeSearchNet** yang mencakup **enam bahasa pemrograman** (Ruby, JavaScript, Java, Go, PHP, Python).<br>• Kinerja model secara keseluruhan di atas **$85\%$ MRR** dan **$95\%$ SR@10** untuk semua bahasa, dengan akurasi model keseluruhan sekitar $97\%$.\\<br>**Metodologi:**<br>• **Pre-training:** Menggunakan model **T5** (*Text-to-Text Transfer Transformer*) untuk pra-pelatihan pada pasangan teks/kueri dan kode, memfasilitasi pembelajaran *text-to-text* yang terpadu.<br>• **Graph Generation:** Kueri (NL) diubah menjadi **Graf Teks** (berbasis *Parse Tree* / CFG) dan Kode Sumber menjadi **Graf Kode** (berbasis **AST**, dengan *edge* Child, NextToken, LastLexicalUse).<br>• **Graph Encoding (GE):** Menggunakan **RGCN** (jenis GNN) untuk mempelajari *node embeddings* ($X_{q}, X_{e}$) yang mempertahankan informasi struktural dan semantik.<br>• **Semantic Matching Operations (SMO):** Menggunakan mekanisme *cross-attention* (berbasis $cosine similarity$) untuk menghitung $\alpha_{i,j}$ dan *context representation* $\overline{e_{G}^{i}}$. Operasi perbandingan **Sub, Mul, dan SubMul** (konkatenasi Subtraction dan Multiplication) diterapkan untuk memodifikasi *node embeddings* dan menangkap hubungan semantik mendalam.\\• **Code Search & Ranking:** Node *embedding* yang diperbarui diagregasi menggunakan operasi **FCMax** (Fully Connected Max-pooling), dan kesamaan akhir $\text{sim}(q,e)$ dihitung menggunakan $cosine similarity$ antara vektor graf akhir $H_{q}$ dan $H_{e}$.<br><br>**Temuan Kunci:**<br>1. **Kinerja SOTA:** SeCoDeGrMa secara signifikan mengungguli semua *baseline* (termasuk DeepCS, UNIF, CAT) di semua metrik dan semua 6 bahasa CodeSearchNet. Di Python, mencapai MRR **$93.7\%$** dan SR@10 **$98.8\%$**.<br>2. **Pentingnya SMO:** Model dengan SMO (Sub, Mul, SubMul) memberikan hasil yang jauh lebih tinggi daripada model tanpa SMO (SeCoDeGrMa-No). **SubMul** (konkatenasi Sub dan Mul) berkinerja terbaik, membuktikan kombinasi ini menangkap fitur yang lebih banyak.<br>3. **RGCN Efektif:** Penggunaan RGCN (dibandingkan MPNN atau CGCN) menunjukkan kinerja yang konsisten dan sangat baik, memvalidasi pilihan *encoder* graf.<br>4. **AO Unggul:** Operasi agregasi berbasis *Fully Connected* (**FCMax** dan FCAvg) mengungguli operasi dasar (Average, Max). FCMax berkinerja terbaik karena kemampuannya membedakan kesamaan semantik secara efektif.<br><br>**Kontribusi Utama:**<br>• Model *code search* pertama yang menggabungkan model **T5** dengan *deep graph matching* untuk pencarian kode sumber semantik.<br>• Model *code search* neural pertama yang merepresentasikan **kueri NL dan kode sebagai graf terstruktur** untuk menangkap informasi struktural dan semantik secara bersamaan.<br>• Mengusulkan dan memvalidasi **SMO** (*Semantic Matching Operations*) untuk mempelajari hubungan semantik halus antara kueri dan kode.<br><br>**Dampak:**<br>• Memberikan solusi DL yang sangat efektif untuk mengatasi masalah *reusability* kode dengan menjembatani kesenjangan semantik, menghasilkan peningkatan produktivitas *developer* secara signifikan dan menyediakan hasil pencarian yang sangat akurat (Top-10 *ranked results*). |

## 1. Pendahuluan & Masalah

*Reusability* kode adalah aspek penting dalam pengembangan perangkat lunak modern. Namun, dengan koleksi kode sumber yang terus bertambah di repositori, menemukan cuplikan kode yang diperlukan oleh pengembang, yang seringkali dilakukan melalui kueri *Natural Language* (NL), menjadi tugas yang menantang. Tantangan utama terletak pada **kesenjangan semantik** antara bahasa pemrograman dan bahasa alami.

Pendekatan *Deep Learning* (DL) yang ada (seperti DeepCS, UNIF) umumnya mengkodekan kode sumber dan kueri sebagai *embedding* vektor independen, kemudian mengukur kesamaan vektor. Namun, strategi ini memiliki dua masalah signifikan:

1.  **Kurangnya Informasi Struktural:** Model *sequence encoding* tradisional (seperti RNN, LSTM) tidak dapat menangkap informasi struktural yang kaya (seperti berbagai fitur ketergantungan dan ketergantungan jarak jauh) yang terkandung dalam kode sumber.
2.  **Scarcity of Semantic Insight:** Kurangnya wawasan tentang hubungan semantik yang halus (*fine-granularity*) antara kueri teks dan cuplikan kode menghalangi model untuk menyelaraskan informasi yang tersebar secara efektif.

::: tip Solusi yang Diusulkan
Kami memperkenalkan **SeCoDeGrMa** (*Semantic Code Search using Deep Graph Matching*). Model ini mengatasi masalah ini dengan (1) merepresentasikan **kueri NL dan cuplikan kode dalam format graf terstruktur** (berbasis *Parse Tree* dan AST) untuk abstraksi dan detail struktural yang lebih besar; (2) menggunakan **GNN (Graph Neural Networks)**, khususnya **RGCN**, untuk pembelajaran *node embeddings*; dan (3) menerapkan mekanisme **Semantic Matching Operations (SMO)** berbasis *cross-attention* untuk mengeksploitasi hubungan semantik yang mendalam dan menghasilkan pencocokan kode yang paling akurat.
:::

## 2. Metodologi

SeCoDeGrMa adalah kerangka kerja *deep neural* yang bekerja dalam enam langkah utama, mulai dari pra-pemrosesan data hingga peringkat hasil.

### A. Pra-pelatihan dan Pra-pemrosesan Data

1.  **Pra-pemrosesan Data (Langkah 1):** Dataset dibersihkan (penghapusan *stop word*), *tokenization* dilakukan (dengan mengganti nilai sensitif dengan *mask*), dan data dibagi (80\% pelatihan, 20\% pengujian).
2.  **Model T5 (Langkah 2):** Model **T5** (*Text-to-Text Transfer Transformer*) digunakan untuk pra-pelatihan. Model ini mengubah semua tugas pengkodean berbasis teks menjadi format *text-to-text* dan menggunakan arsitektur *encoder-decoder* dalam tujuan terpadu, yang membantu dalam pembelajaran representasi yang kaya dan transfer *learning*.

### B. Graph Generation dan Encoding

1.  **Graph Generation (Langkah 3):** Kueri dan kode diubah menjadi graf karena format ini lebih efektif dalam merepresentasikan informasi struktural dan semantik yang kaya daripada urutan token sederhana.
    *   **Graf Kode ($G_{e}$):** Dibangun menggunakan **AST (Abstract Syntax Tree)**. Node mewakili token sintaks, dan *edge* menangkap hubungan sintaksis dan semantik (misalnya, *Child, NextToken*, dan *LastLexicalUse*).
    *   **Graf Kueri ($G_{q}$):** Dibangun menggunakan **Parse Tree** (berbasis *Context-Free Grammar* - CFG). Node mewakili simbol terminal (kata) dan non-terminal (frasa), dan *edge* merepresentasikan aturan tata bahasa. Urutan kata juga diintegrasikan melalui *edge* berantai (panah biru) untuk menangkap informasi kontekstual.
2.  **Graph Encoding (Langkah 4):** Modul ini menggunakan **RGCN (Relational Graph Convolutional Networks)** untuk mempelajari *node embeddings* ($X_{q}, X_{e}$) untuk $G_{q}$ dan $G_{e}$. RGCN sangat cocok karena graf yang dihasilkan berlabel dan diarahkan. Proses pembaruan *node embedding* ($Q_{i}^{(l+1)}$) pada lapisan $(l+1)^{th}$ dihitung sebagai:
    $$Q_{i}^{(l+1)}=ReLU(W_{G}^{(l)}+\sum_{r\in R_{q}}\sum_{j\in N_{i}^{r}}\frac{1}{|N_{i}^{r}|}W_{r}^{(l)}q_{j}^{(l)})$$
    Di mana $R_{q}$ adalah himpunan jenis interkoneksi (*edge types*).

### C. Semantic Matching dan Peringkat

1.  **Semantic Matching Process (SMO - Langkah 5):** Fungsi $f_{match}$ berbasis *cross-attention* ini bertujuan untuk memperkaya dan memodifikasi *node embeddings* dengan menangkap hubungan semantik mendalam antara $G_{q}$ dan $G_{e}$.
    *   **Attention:** Kesamaan *cross-attention* $\alpha_{i,j}$ dihitung menggunakan *cosine similarity* antara setiap *node embedding* kueri $q_{i}$ dan *node embedding* kode $e_{j}$:
        $$\alpha_{i,j} = \text{cosine}(q_{i}, e_{j}), \forall_{j=1,...,N}$$
    *   **Context Representation:** Representasi konteks ($\overline{e_{G}^{i}}$) dihitung sebagai rata-rata berbobot dari *node embedding* kode:
        $$\overline{e_{G}^{i}}=\frac{1}{N}\sum_{j}^{N}\alpha_{i,j}e_{j}$$
    *   **Comparison Operations (SMO):** Operasi perbandingan diterapkan untuk memodifikasi $q_{i}$ dan $\overline{e_{G}^{i}}$ menjadi vektor ($\hat{q_{i}}$):
        *   **Subtraction (Sub):** $\hat{q_{i}}=Sub(q_{i},\overline{e_{G}^{i}})=(q_{i}-\overline{e_{G}^{i}})\odot(\overline{e_{G}^{i}}-q_{i})$
        *   **Multiplication (Mul):** $\hat{q_{i}}=Mul(q_{i},\overline{e_{G}^{i}})=q_{i}\odot\overline{e_{G}^{i}}$
        *   **Subtraction-Multiplication (SubMul) - Final Model:** $\hat{q_{i}}=SubMul(q_{i},\overline{e_{G}^{i}})=\text{Concator} [Sub(q_{i},\overline{e_{G}^{i}}), Mul(q_{i},\overline{e_{G}^{i}})]$
2.  **Code Search (Langkah 6) & Ranking (Langkah 7):** Node *embedding* yang diperbarui diagregasi menjadi representasi vektor tingkat-graf ($H_{q}, H_{e}$) menggunakan operasi **FCMax** (*Fully Connected Max-pooling*), yang merupakan bentuk modifikasi dari *max-pooling* setelah transformasi lapisan *fully connected* (FC).
    $$H_{q} = \text{FCMax}(X_{q})$$
    $$H_{e} = \text{FCMax}(X_{e})$$
    Peringkat hasil dilakukan dengan mengukur skor kesamaan $\text{sim}(q,e)$ menggunakan *cosine similarity* antara $H_{q}$ dan $H_{e}$:
    $$\text{sim}(q,e)=sim(G_{q},G_{e})=\text{cosine}(H_{q},H_{e})$$

## 3. Detail Pengujian

### Dataset
*   **CodeSearchNet Benchmark:** Dataset yang berisi hampir satu juta fungsi dari enam bahasa pemrograman: **Ruby, JavaScript, Java, Go, PHP, dan Python.**
*   **Pembagian Data:** Mengikuti pembagian *benchmark* (misalnya, Python memiliki $250.820$ data pelatihan, $15.418$ pengujian).

### Baseline
Tujuh pendekatan *deep end-to-end code search* SOTA digunakan untuk perbandingan:
*   **Neural BoW**
*   **CNN**
*   **BIRNN**
*   **SelfAtnn**
*   **DeepCS**
*   **UNIF**
*   **CAT**

### Metrik Evaluasi
1.  **Mean Reciprocal Rank (MRR):**
    $$MRR=\frac{1}{|Q|}\sum_{i=1}^{|Q|}\frac{1}{Rank_{i}}$$
2.  **Recall@k (R@k, $k=1, 5, 10$):**
    $$R@K=\frac{1}{|Q|}\sum_{i=1}^{|Q|}\delta(Rank_{i}\le k)$$

## 4. Hasil Eksperimen

### A. Efektivitas Pendekatan yang Diusulkan (Perbandingan Baseline)

| Dataset | Model | MRR (%) | SR@1 (%) | SR@5 (%) | SR@10 (%) |
| :--- | :--- | :--- | :--- | :--- | :--- |
| Ruby | SeCoDeGrMa | **85.8** | **80.8** | **96.7** | 95.5 |
| JavaScript | SeCoDeGrMa | **86.9** | **81.7** | **96.5** | **96.7** |
| Java | SeCoDeGrMa | **88.5** | **82.9** | **97.1** | **97.8** |
| Go | SeCoDeGrMa | **90.3** | **84.6** | **96.6** | **96.9** |
| PHP | SeCoDeGrMa | **91.3** | **85.6** | **96.6** | **97.0** |
| Python | SeCoDeGrMa | **93.7** | **88.3** | **96.9** | **98.8** |

*   **Kinerja Superior:** SeCoDeGrMa secara signifikan mengungguli semua *baseline* pada semua metrik dan semua 6 bahasa. Secara keseluruhan, kinerja MRR model berada di atas $85\%$ dan SR@10 di atas $95\%$ untuk seluruh dataset.
*   **Daya Ungkit Semantik:** Kinerja terbaik pada Python (MRR $93.7\%$, SR@10 $98.8\%$) menunjukkan bahwa representasi graf dan SMO berhasil menangkap semantik mendalam, yang diperlukan untuk bahasa yang kaya ini.

### B. Studi Ablasi (Pengaruh Komponen)

1.  **Efek SMO (Semantic Matching Operations):**
    *   Model dengan SMO (Sub, Mul, SubMul) menghasilkan skor yang jauh lebih tinggi daripada **SeCoDeGrMa-NO** (tanpa SMO).
    *   Model **SeCoDeGrMa (SubMul)** secara konsisten mengungguli Sub dan Mul secara terpisah, memvalidasi bahwa konkatenasi kedua operasi perbandingan ini menangkap lebih banyak karakteristik semantik. (Misal, Python: MRR SubMul **93.1\%** vs Mul $86.2\%$ vs Sub $81.9\%$).
2.  **Efek Graph Encoding Models (GEMs):**
    *   Membandingkan RGCN dengan MPNN dan CGCN menunjukkan bahwa kinerja semua model sebanding, mengindikasikan desain model SeCoDeGrMa tangguh terhadap pilihan spesifik GNN di modul *encoding*. Namun, RGCN tetap menjadi pilihan yang efisien.
3.  **Efek Operasi Agregasi (AO):**
    *   Operasi agregasi berbasis *Fully Connected* (FCMax, FCAvg) berkinerja lebih baik daripada operasi dasar (Average, Max).
    *   **FCMax** secara konsisten mengungguli FCAvg, yang dihipotesiskan karena *max pooling* lebih baik dalam membandingkan dan menentukan perbedaan semantik antara graf teks dan kode.

## 5. Kesimpulan

SeCoDeGrMa adalah model *deep graph matching* yang sangat efektif untuk *semantic code retrieval*. Dengan merepresentasikan kueri NL dan kode sumber sebagai graf terstruktur (berbasis *Parse Tree* dan AST), mengkodekannya menggunakan RGCN, dan menerapkan mekanisme SMO SubMul berbasis *cross-attention*, model ini berhasil menjembatani kesenjangan semantik dan struktural yang diabaikan oleh model *sequence encoding* sebelumnya. Eksperimen pada CodeSearchNet membuktikan bahwa SeCoDeGrMa memberikan hasil SOTA yang unggul secara signifikan di semua metrik dan bahasa.

::: info Dampak Praktis
SeCoDeGrMa secara substansial meningkatkan kemungkinan pengembang menemukan cuplikan kode yang benar dalam daftar hasil peringkat teratas (SR@5 di atas $95\%$). Hal ini secara langsung berkontribusi pada peningkatan produktivitas perangkat lunak dan kualitas kode melalui *reusability* yang lebih efisien dan akurat.
:::

---

Apakah Anda ingin saya memberikan perincian lebih lanjut tentang bagaimana Graf Teks (Parse Tree) dan Graf Kode (AST) dikonstruksi dalam metodologi SeCoDeGrMa?