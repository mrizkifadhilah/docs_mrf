---
title: Review Paper - BASA Kombinasi BiLSTM dan Self-Attention untuk Code Search
description: Rangkuman paper tentang model BASA untuk meningkatkan akurasi code search melalui gabungan BiLSTM dan Self-Attention (Concurrency and Computation Practice and Experience, 2023).
head:
  - - meta
    - name: keywords
      content: Code search, deep learning, BiLSTM, self-attention, semantic matching, code recommendation
---

# 052 - Combining bidirectional long short-term memory and self-attention mechanism for code search
Tautan (DOI) [https://doi.org/10.1002/cpe.7634]

**Penulis:** **Kun Liu** $^{a,b}$, **Jianxun Liu** $^{a,b}$*, **Haize Hu** $^{a,b}$

**Afiliasi:**
* $^a$ Hunan University of Science and Technology, School of Computer Science and Engineering, Xiangtan, 411201, China
* $^b$ Hunan Provincial Key Lab for Services Computing and Novel Software Technology, Xiangtan, 411201, China

**Kronologi:** Received: 7 May 2022 • Revised: 13 December 2022 • Accepted: 22 December 2022 • Available Online: 3 February 2023

<a href="https://www.scimagojr.com/journalsearch.php?q=27871&tip=sid" target="_blank"><img src="https://www.scimagojr.com/journal_img.php?id=27871" alt="SCImago Journal & Country Rank" /></a>

| Resume Eksekutif |
| :--- |
| **Publikasi:**<br>• **Jurnal:** Concurrency and Computation: Practice and Experience 35, 13 (2023)<br>• **Topik:** Mengatasi masalah *semantic gap* dan *word ambiguity* dalam *code search* dengan mengusulkan model yang lebih andal berbasis jaringan *self-attention* dua tahap dan BiLSTM.<br><br>**Masalah & Solusi:**<br>• **Masalah 1 (Semantic Gap & Ambiguity):** Kueri bahasa alami (NL) yang singkat dan kode sumber (PL) memiliki kosakata dan konteks yang berbeda, menyebabkan **kesenjangan semantik** dan **ambiguitas kata** (*word ambiguity*).<br>• **Masalah 2 (Keterbatasan Model):** Model *code search* sebelumnya (misalnya, DeepCS) yang hanya menggunakan *long short-term memory* (LSTM) tidak dapat menangkap representasi semantik yang akurat karena kesulitan menangani dependensi kontekstual yang jauh (*long-range dependencies*).<br>• **Solusi:** Mengusulkan model **BASA** (*Bi-directional LSTM and Self-Attention*). BASA menggunakan **Bi-LSTM** untuk menangkap ketergantungan sekuensial dan kontekstual dua arah. Yang terpenting, BASA menggunakan **Mekanisme *Self-Attention* Dua Tahap** untuk memprioritaskan kata-kata kunci dan memadukan fitur yang relevan secara adaptif, sehingga menghasilkan representasi vektor yang lebih kaya semantik.<br><br>**Contoh Penerapan:**<br>• Model dilatih pada tugas *code retrieval* (Java) untuk membandingkan *cosine similarity* vektor kode dan kueri. BASA berhasil menjembatani kesenjangan semantik dan meningkatkan akurasi pemeringkatan secara signifikan.<br><br>**Metodologi:**<br>• **Ekstraksi Fitur:** Kode dibagi menjadi tiga *field* (Nama Metode, Urutan API, Token) dan kueri (Deskripsi).<br>• **Encoding Bi-LSTM:** Semua fitur (kode dan kueri) dienkode menggunakan **Bi-LSTM** untuk menangkap konteks sekuensial dua arah.\\($h_{i}^{L}=\vec{h}_{i}\oplus\overleftarrow{h}_{i}\\$).<br>• **Self-Attention Tahap 1 (Kontekstual):** Diterapkan pada output Bi-LSTM untuk menangkap bobot setiap kata dalam konteks kalimatnya sendiri, menghasilkan matriks fitur yang diperkuat secara kontekstual.\\($A_{i}=softmax(score(h_{i},h_{j}))\\$).<br>• **Self-Attention Tahap 2 (Final Fusion):** Matriks fitur dari keempat *field* kode digabungkan. Tahap 2 *Self-Attention* diterapkan untuk mempelajari bobot penekanan dan korelasi di antara fitur-fitur yang berbeda (Nama Metode, API, Token), menghasilkan vektor kode akhir yang komprehensif ($C_{vector}$).\\($C_{vector}=\text{Attention}(A_{feat\_list}, W\_score)\\$).<br>• **Optimasi:** Menggunakan **Triplet Loss** dengan *margin* untuk memaksimalkan jarak antara pasangan positif dan negatif dalam ruang vektor bersama.<br><br>**Temuan Kunci:**<br>1. **Kinerja SOTA:** BASA secara konsisten melampaui semua *baseline* SOTA, termasuk DeepCS, UNIF, dan CodeAttention, di semua metrik (MRR $\mathbf{+12.6\%}$ dibandingkan DeepCS) pada dataset Java CodeSearchNet dan Hu et al.'s.<br>2. **Kontribusi Atensi:** Penggunaan mekanisme *self-attention* dua tahap menyumbang peningkatan kinerja terbesar, membuktikan pentingnya pembobotan kata dan fusi fitur adaptif.<br>3. **Efektivitas Bi-LSTM:** Penggunaan Bi-LSTM lebih efektif daripada LSTM satu arah (*unidirectional*), menunjukkan peran vital pemahaman konteks dua arah.<br><br>**Kontribusi Utama:**<br>• Mengusulkan model **BASA** yang menggabungkan Bi-LSTM dan *Self-Attention* untuk meningkatkan akurasi *code search*.<br>• Merancang **Mekanisme *Self-Attention* Dua Tahap** untuk menangkap fitur yang lebih komprehensif dan efisien.<br>• Membuktikan bahwa pemaduan representasi *multi-field* kode dengan *attention* jauh lebih unggul daripada sekadar menggunakan representasi sekuensial. |

## 1. Pendahuluan & Masalah

Pencarian kode (*code search*) adalah tugas yang sangat penting dalam Rekayasa Perangkat Lunak, bertujuan untuk menemukan *snippet* kode yang relevan dengan kueri bahasa alami (NL) dari basis kode yang besar. Meskipun model *Deep Learning* (DL) telah menggantikan metode *Information Retrieval* (IR) tradisional, mereka masih menghadapi dua tantangan utama:

1.  **Kesenjangan Semantik dan Ambiguitas:** Terdapat perbedaan besar antara kosakata NL dan bahasa pemrograman (PL), menyebabkan **kesenjangan semantik** yang signifikan. Selain itu, kueri NL yang pendek seringkali memiliki **ambiguitas** kata, yang memicu masalah *word mismatch* dan menurunkan akurasi.
2.  **Keterbatasan Model Sekuensial:** Model DL awal, seperti DeepCS, yang menggunakan *Long Short-Term Memory* (LSTM), terbatas dalam menangkap dependensi kontekstual jangka panjang (*long-range dependencies*) dan tidak mampu secara akurat memprioritaskan kata-kata kunci dalam kueri.

::: tip Solusi yang Diusulkan
Untuk mengatasi tantangan ini, kami mengusulkan model **BASA** (*Bidirectional LSTM and Self-Attention*). BASA menggabungkan kekuatan **Bi-LSTM** (untuk pemahaman konteks sekuensial dua arah) dan **Mekanisme *Self-Attention* Dua Tahap** (untuk pembobotan kata-kata kunci dinamis dan fusi fitur adaptif) untuk menghasilkan representasi vektor yang kaya semantik dan andal.
:::

## 2. Metodologi

Arsitektur BASA adalah model *pseudo-siamese* yang memproses kode dan kueri secara paralel dan independen sebelum memadankan keduanya di ruang vektor bersama.

### A. Ekstraksi Fitur Multimodal
Kode sumber dipecah menjadi tiga *field* (fitur) utama:
1.  **Nama Metode (M):** Judul fungsi.
2.  **Urutan API (P):** Urutan pemanggilan API.
3.  **Token (T):** Token kode sumber lainnya.
Kueri adalah *Description* ($D$).

### B. Bi-LSTM Encoder
Semua fitur yang diekstraksi ($M, P, T, D$) di-*embed* dan diumpankan ke lapisan **Bi-LSTM**. Bi-LSTM memproses urutan dalam arah maju ($\vec{h}$) dan mundur ($\overleftarrow{h}$), menggabungkan kedua *hidden state* untuk menghasilkan representasi kontekstual dua arah ($h_{i}^{L}$), yang lebih unggul daripada LSTM satu arah.
$$h_{i}^{L}=\vec{h}_{i}\oplus\overleftarrow{h}_{i}$$

### C. Self-Attention Mechanism Tahap 1 (Kontekstual)
Tahap 1 *Self-Attention* diterapkan pada output Bi-LSTM dari setiap *field* (termasuk kueri) untuk menangkap bobot penekanan pada kata-kata penting dalam konteks kalimatnya sendiri.
$$A_{i}=softmax(score(h_{i},h_{j}))$$
Mekanisme ini menghasilkan matriks fitur yang diperkuat secara kontekstual untuk setiap *field*.

### D. Self-Attention Mechanism Tahap 2 (Fusion)
Tahap 2 *Self-Attention* berfungsi sebagai lapisan **fusi adaptif** untuk menggabungkan representasi dari berbagai *field* kode yang berbeda.
1.  **Input:** Matriks fitur dari tiga *field* kode ($M', P', T'$) yang dihasilkan dari Tahap 1.
2.  **Bobot Fusi Adaptif:** *Self-Attention* diterapkan pada ketiga matriks ini untuk menentukan bobot yang berbeda (*adaptive weight*) untuk setiap *field* kode sehubungan dengan konteks *code snippet* secara keseluruhan.
3.  **Output:** Vektor kode akhir ($C_{vector}$) diperoleh melalui penjumlahan berbobot. Vektor ini adalah representasi paling komprehensif, dan digunakan untuk menghitung skor kesamaan dengan vektor kueri.
$$C_{vector}=\text{Attention}(A_{feat\_list}, W\_score)$$

### E. Optimization (Triplet Loss)
Model dilatih menggunakan **Triplet Loss** berbasis *margin*:
$$\mathcal{L}(\theta)=\sum_{(C,D^{+},D^{-})\in G}\max(0, \epsilon - \cos(C, D^{+}) + \cos(C, D^{-}))$$
Di mana $\epsilon$ adalah *margin*, $D^{+}$ adalah deskripsi positif, dan $D^{-}$ adalah deskripsi negatif (diambil dari *minibatch*).

## 3. Detail Pengujian

### Dataset
BASA diuji pada dua *dataset* publik:
* **CodeSearchNet (CSN):** $\approx 4.5$ juta pasangan kode-kueri (digunakan untuk membandingkan kinerja antara Bi-LSTM, GNN, dan BASA).
* **Dataset Hu et al.'s:** $\approx 480$ ribu pasangan kode-kueri (digunakan untuk perbandingan dengan *baseline* DeepCS, UNIF).

### Baseline
Model dibandingkan dengan *baseline* utama:
* **DeepCS:** Baseline berbasis LSTM (*multi-field*).
* **UNIF:** Model berbasis *attention* yang lebih sederhana.
* **CodeAttention (Attention-based):** Model berbasis *attention* dan *co-attention*.

### Metrik Evaluasi
Tiga metrik utama digunakan untuk mengukur efektivitas *ranking*:
* **Recall@k ($R@k$):** Persentase keberhasilan (jawaban benar ada di Top-k).
* **Mean Reciprocal Rank (MRR):** Mengukur posisi urutan hasil yang benar.
* **Normalized Discounted Cumulative Gain (NDCG).**

## 4. Hasil Eksperimen

### RQ1. Perbandingan Kinerja Utama
BASA menunjukkan kinerja superior di semua metrik.

| Model | MRR (Hu et al.'s) | MRR (CSN) |
| :--- | :--- | :--- |
| DeepCS | 0.5891 | 0.6120 |
| UNIF | 0.6033 | 0.6310 |
| CodeAttention | 0.6409 | 0.6800 |
| **BASA** | **0.6631** | **0.7200** |

* **Peningkatan MRR:** BASA melampaui DeepCS sebesar $\mathbf{12.6\%}$ MRR (di *dataset* Hu et al.'s) dan $\mathbf{17.7\%}$ MRR (di CSN), menunjukkan kemampuan yang lebih baik dalam mengatasi ambiguitas dan dependensi.

### RQ2. Kontribusi Komponen BASA (Ablasi)

| Varian | MRR | Keterangan |
| :--- | :--- | :--- |
| **BASA** | **0.6631** | Lengkap (BiLSTM + SA Tahap 1 + SA Tahap 2) |
| w/o SA-Stage 2 | 0.6493 | Tanpa Fusi Fitur Adaptif (Tahap 2) |
| w/o SA-Stage 1 & 2 | 0.6111 | Hanya BiLSTM |
| w/o Bi-LSTM | 0.6358 | Hanya LSTM satu arah |

* **Kontribusi SA:** Penghapusan **Mekanisme *Self-Attention* (Tahap 1 & 2)** menyebabkan penurunan kinerja terbesar, membuktikan bahwa fitur kontekstual dan pembobotan dinamis yang dihasilkan oleh *Attention* sangat krusial.
* **Kontribusi Bi-LSTM:** Bi-LSTM lebih unggul dari LSTM satu arah (MRR $0.6111$ vs $0.6033$), memvalidasi pentingnya pemahaman konteks dua arah.
* **Fusi Adaptif:** Kontribusi *Self-Attention* Tahap 2 (Fusi Adaptif) sangat signifikan (peningkatan MRR $1.38\%$ dari *w/o SA-Stage 2*), menunjukkan bahwa bobot adaptif antar *field* kode menghasilkan representasi akhir yang paling informatif.

## 5. Kesimpulan

Model **BASA** berhasil meningkatkan akurasi *code search* secara signifikan dengan mengintegrasikan **Bi-LSTM** dan **Mekanisme *Self-Attention* Dua Tahap** yang inovatif. Bi-LSTM secara efektif menangkap konteks sekuensial, sementara *Self-Attention* dua tahap mengatasi ambiguitas kata dan menghasilkan representasi kode yang komprehensif melalui fusi fitur adaptif. Hasilnya, BASA mencapai kinerja *state-of-the-art* (MRR $\mathbf{0.7200}$ pada CSN) dibandingkan semua *baseline* SOTA.

::: info Dampak Praktis
BASA menyediakan metodologi yang **efektif dan praktis** untuk *code search* yang dapat diadopsi secara luas. Kemampuannya untuk menghasilkan representasi yang lebih akurat dengan memanfaatkan pembobotan kata dan fusi fitur adaptif akan secara langsung meningkatkan **produktivitas pengembang** dengan menyajikan *snippet* kode yang paling relevan di peringkat teratas.
:::