---
title: Review Paper - QobCS Model Pencarian Kode Berbasis Dua Tahap Atensi
description: Rangkuman paper tentang model dua tahap berbasis atensi berorientasi kueri (QobCS) untuk pencarian kode (The Journal of Systems and Software, 2024).
head:
  - - meta
    - name: keywords
      content: Code search, Attention mechanism, Query-oriented attention mechanism, Code structural feature, Semantic gap
---

# 031 - Query-oriented two-stage attention-based model for code search
Tautan (DOI) [10.1016/j.jss.2023.111948](https://doi.org/10.1016/j.jss.2023.111948)

**Penulis:** **Huanhuan Yang** $^{a,b}$, **Ling Xu** $^{a,b}$*, **Chao Liu** $^{a,b}$, **Luwen Huangfu** $^{c,d}$

**Afiliasi:**
* $^a$ Key Laboratory of Dependable Service Computing in Cyber Physical Society (Chongqing University), Ministry of Education, China
* $^b$ School of Big Data and Software Engineering, Chongqing University, Chongqing, China
* $^c$ Fowler College of Business, San Diego State University, CA, 92182, USA
* $^d$ Center for Human Dynamics in the Mobile Age, San Diego State University, CA, 92182, USA

**Kronologi:** Received: 7 July 2022 • Revised: 23 November 2023 • Accepted: 27 December 2023 • Available Online: 3 January 2024

<a href="https://www.scimagojr.com/journalsearch.php?q=19309&tip=sid" target="_blank"><img src="https://www.scimagojr.com/journal_img.php?id=19309" alt="SCImago Journal & Country Rank" /></a>

| Resume Eksekutif |
| :--- |
| **Publikasi:**<br>• **Jurnal:** The Journal of Systems and Software 210 (2024) 111948<br>• **Topik:** Meningkatkan efektivitas dan efisiensi *code search* dengan menjembatani kesenjangan semantik (*semantic gap*) antara kueri bahasa alami dan kode bahasa pemrograman, serta mengatasi kerugian informasi niat kueri pada model sebelumnya.<br><br>**Masalah & Solusi:**<br>• **Masalah 1 (Kesenjangan Semantik):** Model *Deep Learning* (DL) yang ada tidak secara mendalam menganalisis perbedaan dan korelasi semantik antara kueri dan kode (misalnya, bagaimana "word" dalam bahasa alami berkorelasi dengan "str" atau "sb" dalam kode).<br>• **Masalah 2 (Kehilangan Niat Kueri):** Mekanisme *co-attention* pada model sebelumnya (*CARLCS-CNN, TabCS*) cenderung menyaring informasi yang tampaknya tidak relevan antara kode dan kueri, padahal informasi ini mungkin merupakan **niat penting** dari kueri, yang berakibat fatal pada akurasi pencarian.<br>• **Masalah 3 (Efisiensi):** Model DL berbasis jaringan kompleks (*CNN, LSTM*) cenderung memperlambat tugas *code search*.<br>• **Solusi:** Mengusulkan **QobCS** (*Query-oriented two-stage attention-based model for Code Search*). Menggunakan dua tahap berbasis **Attention Mechanism** yang sederhana dan cepat. Tahap 2 secara khusus menggunakan **Query-Oriented Attention Mechanism** untuk: (1) menjaga representasi kueri tetap lengkap; dan (2) menghitung representasi kode berdasarkan korelasi dengan niat kueri yang sudah lengkap.<br><br>**Contoh Penerapan:**<br>• Diuji pada dua dataset Java berskala besar dari GitHub: dataset1 (485k pasangan kueri-kode) dan dataset2 (542k pasangan kueri-kode). Juga diuji pada 50 kueri nyata (*real queries*) dari Stack Overflow.<br><br>**Metodologi:**<br>• **Fitur Kode:** Mengekstrak empat fitur: tiga fitur tekstual (Nama Metode, Urutan API, *Tokens*) dan satu fitur struktural (Urutan AST).<br>• **Stage 1 (Deeper Semantics):** Menggunakan **Independent Attention Mechanism** pada setiap fitur (termasuk kueri) untuk menyaring kata-kata yang kurang bermakna (frekuensi tinggi) dan mempertahankan kata-kata yang penting, menghasilkan 5 matriks fitur.<br>• **Stage 2 (Query-Oriented Attention):** (1) Representasi kueri akhir ($g^Q$) dihitung melalui *Mean-Pooling* pada matriks kueri Tahap 1, dan **tidak pernah berubah** untuk kode kandidat yang berbeda. (2) Setiap matriks fitur kode Tahap 1 diumpankan ke **Query-Oriented Attention Mechanism** independen. Mekanisme ini menggabungkan representasi kueri dan kata kode, menghasilkan vektor penyatuan $u_{m_i} = \tanh(g^m + w_{m_i})$. (3) Vektor kode akhir ($g^C$) adalah fusi *Mean-Pooling* dari empat vektor representatif kode yang dihasilkan.<br>• **Optimasi:** Menggunakan *triplet loss* berbasis margin $\mathcal{L}(\theta)=\sum_{(Q,C^{+},C^{-})\in G}\max(0, \beta - \text{diff})$, di mana $\text{diff} = S(q, c^{+}) - S(q, c^{-})$.<br><br>**Temuan Kunci:**<br>1. **Efektivitas SOTA:** QobCS mencapai MRR $\mathbf{0.701}$ (dataset1) dan $\mathbf{0.595}$ (dataset2), mengungguli model DL sebelumnya (DeepCS, CARLCS-CNN, UNIF, TabCS) secara signifikan (misalnya, $\mathbf{109.25\%}$ di atas DeepCS pada dataset1).<br>2. **Keunggulan Query-Oriented:** Mekanisme *query-oriented attention* menunjukkan keunggulan yang jelas atas *co-attention* (QobCS mengalahkan QobCS-C sebesar $\mathbf{5.99\%}$ MRR pada dataset1), karena ia menjaga niat kueri tetap utuh dan independen dari kode kandidat.<br>3. **Efisiensi:** QobCS menunjukkan kinerja yang cepat dan efisien, karena arsitektur berbasis *attention* yang sederhana memiliki *trainable parameter* yang lebih sedikit dibandingkan model berbasis *CNN/LSTM* yang kompleks.<br>4. **Fitur Struktural:** Fitur struktural AST memberikan kontribusi yang efektif dan positif terhadap kinerja model.<br><br>**Kontribusi Utama:**<br>• Mengusulkan $\mathbf{QobCS}$, model pencarian kode dua tahap berbasis *attention* yang berorientasi kueri.<br>• Merancang **Query-Oriented Attention Mechanism** untuk menjembatani *semantic gap* sambil melestarikan niat kueri.<br>• Mendemonstrasikan kinerja yang **efektif dan efisien** dibandingkan model DL SOTA sebelumnya.<br><br>**Dampak:**<br>• Menyediakan model *code search* yang lebih akurat dan lebih cepat, memfasilitasi pengembang dalam menemukan dan menggunakan kembali kode yang ada. Model ini dapat dipindahtangankan ke model *pre-trained* (*CodeBERT*) dengan peningkatan kinerja lebih lanjut. |

## 1. Pendahuluan & Masalah

Penggunaan kembali kode (*code reusing*) dari komunitas *open-source* sangat penting untuk meningkatkan efisiensi pengembangan perangkat lunak. Model *Deep Learning* (DL) telah diterapkan untuk tugas pencarian kode (*code search*) dengan menghitung representasi semantik yang lebih dalam untuk kueri (bahasa alami) dan kode kandidat (bahasa pemrograman), lalu memeringkatnya berdasarkan kesamaan.

Namun, penelitian sebelumnya gagal mengatasi dua tantangan utama:

1.  **Kesenjangan Semantik (*Semantic Gap*):** Model DL belum secara memadai menganalisis korelasi mendalam antara kueri dan kode. Misalnya, bagaimana kata "word" dalam kueri harus dikaitkan dengan singkatan kode seperti "str" atau "sb".
2.  **Kerugian Niat Kueri:** Meskipun mekanisme *co-attention* (*CARLCS-CNN, TabCS*) digunakan untuk membangun korelasi, mereka cenderung menyaring informasi yang tampaknya tidak relevan. Informasi yang disaring ini seringkali merupakan **niat penting** dari kueri, dan kehilangannya dapat merusak akurasi pencarian kode yang sangat bergantung pada semantik kueri.

Selain itu, sebagian besar model DL sebelumnya menggunakan jaringan yang kompleks (*CNN, LSTM*), yang memperlambat tugas *code search*.

::: tip Solusi yang Diusulkan
Diusulkan **QobCS** (*Query-oriented two-stage attention-based model for Code Search*), model *code search* berbasis dua tahap atensi yang sederhana dan cepat. Tahap pertama mendapatkan representasi semantik yang dalam. Tahap kedua menerapkan **Query-Oriented Attention Mechanism** yang revolusioner. Mekanisme ini menghitung representasi kode berdasarkan (1) semantik kueri yang **lengkap** (niat kueri), dan (2) korelasi spesifik antara kueri dan kode.
:::

## 2. Metodologi

QobCS menggunakan kerangka kerja dua tahap berbasis mekanisme atensi sederhana untuk menjembatani kesenjangan semantik dan mempertahankan niat kueri.

### A. Stage 1: Deeper Semantics Representation

Tahap 1 bertujuan untuk mengekstrak semantik independen dari kode dan kueri, menyaring kata-kata yang tidak penting (frekuensi tinggi) melalui **Independent Attention Mechanism**.

1.  **Fitur Kode:** Digunakan empat fitur:
    *   Tiga fitur tekstual: *Method Name*, *API Sequence*, dan *Tokens*.
    *   Satu fitur struktural: **AST Sequence** (dari *Abstract Syntax Tree*).
2.  **Embedding & Attention:** Setiap fitur dipetakan ke matriks *embedding* (misalnya $F \in \mathbb{R}^{o \times K}$) dan diumpankan ke mekanisme atensi independen. Atensi ($a$) menghitung bobot ($\alpha$) untuk setiap kata/simpul, memastikan kata-kata yang sering digunakan (misalnya "is", "if", "return") mendapat bobot rendah.
    $$ \alpha_{m_i}=\frac{\exp(a_m\cdot m_i^T)}{\sum_{i=1}^{n}\exp(a_m\cdot m_i^T)} $$
3.  **Output:** Tahap 1 menghasilkan lima matriks fitur (M, P, T, AST, dan Q) yang telah difilter dari *noise*.

### B. Stage 2: Query-Oriented Correlation Learning

Tahap 2 membangun korelasi semantik dan menggunakan niat kueri untuk menghitung representasi kode yang lebih baik.

1.  **Query Representation ($g^Q$):** Representasi kueri akhir $g^Q \in \mathbb{R}^{k}$ diperoleh langsung dengan *Mean-Pooling* pada matriks fitur kueri Q dari Tahap 1. Kunci pentingnya: **$g^Q$ tidak pernah berubah** untuk kode kandidat yang berbeda.
    $$ g^Q=\text{MeanPooling}(Q_1,...,Q_q) $$
2.  **Query-Oriented Attention Mechanism (QOM):** Empat mekanisme QOM independen diterapkan pada empat matriks fitur kode. QOM bekerja dengan langkah-langkah berikut (contoh untuk *Method Name*, M):
    *   *Transformasi & Penyatuan:* Representasi kueri $g^Q$ dan kata kode $M_i$ ditransformasi oleh lapisan *full collection* ($G_m, W_m$) dan disatukan (menggunakan tanh) menjadi vektor union $u_{m_i}$.
        $$ u_{m_i}=\tanh(g^m+w_{m_i}) $$
    *   *Penghitungan Bobot Korelasi:* Vektor $u_{m_i}$ diubah menjadi bobot korelasi $\gamma_{m_i}$ menggunakan vektor atensi $a_m$. Bobot ini mewakili seberapa besar kata kode tersebut relevan dengan niat kueri.
        $$ \gamma_{m_i}=\frac{\exp(a_m\cdot u_{m_i}^T)}{\sum_{i=1}^{n}\exp(a_m\cdot u_{m_i}^T)} $$
    *   *Representasi Akhir Kode Fitur:* Vektor representatif akhir metode nama ($m$) adalah penjumlahan berbobot kata kode dengan bobot korelasi $\gamma_m$. $m = M\gamma_m$.
3.  **Feature Fusion:** Empat vektor representatif kode fitur ($m, p, t, ast$) digabungkan menjadi matriks $C \in \mathbb{R}^{k \times 4}$, dan *Mean-Pooling* diterapkan untuk mendapatkan vektor representatif kode akhir $g^C$.

### C. Model Optimization

Model dilatih menggunakan **triplet loss** berbasis margin:
$$ \mathcal{L}(\theta) = \sum_{(Q,C^{+},C^{-})\in G}\max(0, \beta - (S(q, c^{+}) - S(q, c^{-})) ) $$
Di mana $S(g^C, g^Q)$ adalah *cosine similarity* (Eq. 34), dan $\beta$ adalah parameter *margin* yang mengatur seberapa besar skor kode positif harus lebih tinggi dari kode negatif.

## 3. Detail Pengujian

### Dataset
*   **Dataset1 (Hu et al. 2019):** 485k pasangan kueri-kode Java (475k train, 10k test).
*   **Dataset2 (Husain et al. 2019):** 542k pasangan kueri-kode Java (532k train, 10k test).
*   **Real Queries:** 50 kueri nyata dari Stack Overflow untuk validasi kinerja di dunia nyata.

### Baseline
*   **DeepCS:** Model berbasis LSTM independen.
*   **CARLCS-CNN (CARLCS):** Model berbasis CNN + *co-attention*.
*   **UNIF:** Model berbasis *attention* yang lebih sederhana dan independen.
*   **TabCS:** Studi pendahuluan penulis berbasis *attention* + *co-attention*.

### Metrik Evaluasi
Tiga metrik digunakan, dihitung setelah 10 kali pengujian dengan pembagian data *train/test* yang berbeda:
*   **SuccessRate@k ($R@k$):** Persentase kueri di mana kode *ground-truth* ada dalam $k$ hasil teratas.
*   **Mean Reciprocal Rank (MRR):** Rata-rata kebalikan peringkat kode *ground-truth* pertama (untuk $k=10$).
    $$ \text{MRR} = |Q|^{-1}\sum_{i=1}^{|Q|}\sigma_{k}(\text{Rank}_{Q_i}) $$
    Di mana $\sigma_k(\text{Rank}_{Q_i})$ adalah $1/\text{Rank}_{Q_i}$ jika $\text{Rank}_{Q_i} \le k$, dan $0$ sebaliknya.
*   **Normalized Discounted Cumulative Gain (NDCG):** Mengukur korelasi antara hasil peringkat dan kueri, di mana hasil dengan korelasi tinggi di peringkat atas lebih dihargai.

## 4. Hasil Eksperimen

### RQ1. Perbandingan Kinerja (MRR)
QobCS secara signifikan mengungguli semua *baseline* di kedua dataset.

| Model | MRR (Dataset1) | MRR (Dataset2) |
| :--- | :--- | :--- |
| DeepCS | 0.335 | 0.307 |
| CARLCS | 0.485 | 0.409 |
| UNIF | 0.516 | 0.419 |
| TabCS | 0.562 | 0.513 |
| **QobCS** | **0.701** ($\uparrow 24.73\%$ vs TabCS) | **0.595** ($\uparrow 15.98\%$ vs TabCS) |

### RQ2. Kontribusi Query-Oriented Attention
Model varian ($Q$ untuk *Query-Oriented*, $C$ untuk *Co-Attention*) diuji untuk membandingkan mekanisme *attention*.

| Model | MRR (Dataset1) | Keterangan |
| :--- | :--- | :--- |
| DeepCS | 0.335 | Baseline |
| DeepCS-Q | 0.390 | DeepCS + QOM ($\uparrow 16.42\%$) |
| TabCS-Q | 0.607 | TabCS + QOM ($\uparrow 8.01\%$ vs TabCS) |
| QobCS-C | 0.659 | QobCS + Co-Attention ($\downarrow 5.99\%$ vs QobCS) |
| **QobCS** | **0.701** | **QobCS dengan QOM** |

*   Penerapan **QOM** pada *baseline* (DeepCS-Q, UNIF-Q, TabCS-Q) secara konsisten **meningkatkan** kinerja, memvalidasi efektivitas QOM dalam membangun korelasi.
*   QobCS (dengan QOM) secara signifikan **mengungguli** QobCS-C (dengan *co-attention*), membuktikan bahwa QOM lebih unggul karena ia melestarikan **niat kueri yang lengkap** dan independen dari kode kandidat, mencegah distorsi atau pemfokusan satu sisi yang dialami *co-attention*.

### RQ3. Kontribusi Fitur Kode
Hasil menunjukkan kontribusi setiap fitur, dengan **AST** ($A$) memberikan peningkatan positif, dan **Method Name** ($M$) sebagai fitur terpenting.

*   Menghapus AST (QobCS $(M+P+T)$) hanya mengurangi MRR sebesar $1.59\%$ (Dataset1), menjadikannya fitur yang paling kecil kontribusinya, tetapi tetap efektif.
*   Menghapus Method Name (QobCS $(P+T+A)$) menghasilkan penurunan MRR terbesar ($\downarrow 8.68\%$) pada Dataset1.
*   Menerapkan keempat fitur (M+P+T+A) pada *baseline* (CARLCS/UNIF) juga meningkatkan kinerja mereka, membuktikan efektivitas fitur struktural AST.

### RQ4. Efisiensi
Model berbasis mekanisme *attention* (UNIF, TabCS, QobCS) secara signifikan lebih cepat dalam pelatihan dan pengujian daripada model berbasis jaringan kompleks (DeepCS, CARLCS). QobCS memiliki sedikit kerugian efisiensi dibandingkan UNIF/TabCS, tetapi peningkatan efektivitasnya dianggap sebanding dengan kerugian waktu.

*   Waktu Pelatihan QobCS (300 *epoch*): $6.3 \text{ jam}$ (dataset1).
*   Waktu Pengujian QobCS: $0.6 \text{ s/kueri}$ (dataset1).

### RQ5. Dampak Margin ($\beta$)
Nilai *margin* ($\beta$) dalam *triplet loss* sangat memengaruhi kinerja model.

*   Kinerja optimal untuk QobCS pada Dataset1 tercapai pada $\beta = \mathbf{0.4}$.
*   Kinerja optimal untuk QobCS pada Dataset2 tercapai pada $\beta = \mathbf{0.25}$.
*   Hasil menunjukkan bahwa menetapkan nilai $\beta$ berdasarkan kinerja MRR validasi (*ValidMRR*) adalah strategi yang andal.

## 5. Kesimpulan

Paper ini memperkenalkan QobCS, model *code search* berbasis dua tahap atensi berorientasi kueri. QobCS secara efektif mengatasi *semantic gap* dan masalah hilangnya niat kueri yang dialami model sebelumnya. Kinerja yang unggul (MRR $\mathbf{0.701}$ pada dataset1) dan efisiensi yang baik menegaskan QobCS sebagai peningkatan signifikan dibandingkan model DL *code search* sebelumnya. Keberhasilan ini didorong oleh kooperasi dua tahap *attention* dan kemampuan **Query-Oriented Attention Mechanism** untuk mempertahankan semantik kueri yang utuh.

::: info Dampak Praktis
QobCS menyediakan solusi *code search* yang lebih akurat dan cepat, meningkatkan produktivitas pengembang. Selain itu, kerangka kerja QobCS terbukti kompatibel dan dapat memindahkan manfaat kinerjanya ke model *pre-trained* seperti **CodeBERT** (QobCS-cb melampaui CodeBERT $\mathbf{1.31\%}$ MRR), menjadikannya metode yang berharga untuk tugas *fine-tuning* pada model representasi kode yang canggih.
:::