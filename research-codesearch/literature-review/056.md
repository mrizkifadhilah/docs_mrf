---
title: Review Paper - Efektivitas Transfer Learning untuk Pencarian Kode
description: Rangkuman paper tentang efektivitas Transfer Learning dan arsitektur Transformer (BERT) untuk Code Search (IEEE TRANSACTIONS ON SOFTWARE ENGINEERING, 2023).
head:
  - - meta
    - name: keywords
      content: Code search, transfer learning, BERT, Transformer, multimodal embeddings, StackOverflow, deep learning
---

# 056 - On the Effectiveness of Transfer Learning for Code Search

Tautan (DOI) [https://doi.org/10.1109/TSE.2022.3192755](https://doi.org/10.1109/TSE.2022.3192755)

**Penulis:** **Pasquale Salza** ᵃ*, **Christoph Schwizer** ᵃ, **Jian Gu** ᵃ, and **Harald C. Gall** ᵃ, Member, IEEE

**Afiliasi:**
* ᵃ University of Zurich, Zurich, Switzerland

**Kronologi:** Received: 9 August 2021 • Revised: 9 June 2022 • Accepted: 1 July 2022 • Available Online: 21 July 2022

<a href="https://www.scimagojr.com/journalsearch.php?q=18711&tip=iss" target="_blank"><img src="https://www.scimagojr.com/journal_img.php?id=18711" alt="SCImago Journal & Country Rank" /></a>


| Resume Eksekutif |
| :--- |
| **Publikasi:** IEEE TRANSACTIONS ON SOFTWARE ENGINEERING, Vol. 49, No. 4, April 2023.<br><br>**Masalah & Solusi:**<br>• **Masalah:** *Code search* tradisional menghadapi **"Lexical Gap"** (kesenjangan leksikal) antara kueri bahasa alami dan token kode, yang mengurangi akurasi pencarian.<br>• **Solusi:** Menggunakan **Transfer Learning** (Pre-training + Fine-tuning) pada model berbasis **Transformer (BERT)** untuk membuat **Multimodal Embedding Model (MEM)**. MEM memproyeksikan kueri NL dan kode sumber ke dalam ruang semantik bersama untuk pencarian berbasis kemiripan jarak.<br><br>**Contoh Penerapan:**<br>• Model diuji pada pasangan Judul Pertanyaan StackOverflow (sebagai kueri NL) dan *Code Snippet* Jawaban yang Diterima (sebagai dokumen yang dicari).<br>• Digunakan dataset GITHUB (CodeSearchNet) untuk *pre-training* dan StackOverflow Q&A untuk *fine-tuning* pada bahasa JAVASCRIPT, JAVA, dan PYTHON.<br><br>**Metodologi:**<br>• **Pre-training:** Dua *encoder* BERT terpisah di-*pre-train* pada data NL (BERT base) dan data kode (MCM dan NLPred) dari GITHUB.<br>• **Fine-tuning:** Kedua *encoder* dirakit menjadi MEM dan di-*fine-tune* pada data StackOverflow Q&A untuk meminimalkan jarak kosinus.<br>• **Kombinasi:** Diusulkan pendekatan **LUMEM** (*pipeline* Lucene $\rightarrow$ MEM) untuk pencarian pada korpus besar (Strategi Full).<br><br>**Temuan Kunci:**<br>1. Model *pre-trained* secara konsisten mengungguli model non-*pre-trained* dan *baseline* IR (Lucene) pada ruang pencarian kecil (Strategi 1K).<br>2. Kombinasi **LUMEM** menghasilkan kinerja terbaik secara keseluruhan pada ruang pencarian besar (Strategi Full) dengan peningkatan signifikan pada MRR dan skor Aroma.<br>3. *Pre-training* pada *Code Encoder* memberikan dampak yang lebih besar daripada *Query Encoder*, yang disebabkan oleh perbedaan panjang urutan masukan (kode $\approx 225$ token; kueri $\approx 9$ token).<br><br>**Kontribusi Utama:**<br>• Menerapkan dan memvalidasi Transfer Learning berbasis BERT untuk *code search* dengan arsitektur MEM dua *encoder*.<br>• Mendesain dan menguji tugas *pre-training* spesifik kode (MCM dan NLPred).<br>• Membuktikan efektivitas model *pipeline* (LUMEM) yang mengintegrasikan IR tradisional dan *deep learning* untuk skenario skala besar.<br><br>**Dampak:**<br>• Mendorong adopsi model Transformer dari NLP untuk berbagai tugas analisis kode sumber, membuka arah penelitian baru dalam pemodelan kode yang kaya konteks. |

## 1. Pendahuluan & Masalah

*Code search* adalah alat penting bagi pengembang perangkat lunak, namun sistem *retrieval* tradisional (berbasis *token matching* seperti $tf$-$idf$) seringkali gagal karena adanya **"Lexical Gap"** atau **"Heterogeneity Gap"**. Kesenjangan ini terjadi ketika token kueri bahasa alami (NL) tidak cocok dengan token kode sumber meskipun semantiknya relevan.

Pendekatan *deep learning*, khususnya arsitektur **Transformer** (seperti BERT), telah menunjukkan potensi besar dalam memodelkan ketergantungan antar token, terutama dalam urutan yang panjang. Kekuatan utama model ini dalam NLP adalah **Transfer Learning**—melatih model pada korpus besar (*pre-training*) dan menyempurnakannya pada tugas spesifik (*fine-tuning*).

::: tip Solusi yang Diusulkan
Paper ini mengusulkan untuk menerapkan Transfer Learning pada *code search* menggunakan **Multimodal Embedding Model (MEM)** berbasis **BERT**. MEM menggunakan dua *encoder* BERT terpisah ($E_q$ untuk kueri NL dan $E_c$ untuk kode sumber) yang dilatih untuk memproyeksikan kedua modalitas ke dalam ruang vektor bersama (*semantic space*). Hal ini memungkinkan pencarian yang efisien menggunakan metrik kesamaan berbasis jarak (misalnya, *cosine similarity*).
:::

## 2. Metodologi

Pendekatan ini mengikuti alur kerja yang melibatkan *pre-training* dua *encoder* independen, penambangan data *fine-tuning*, dan perakitan model akhir.

### A. Pre-Training Encoder Kueri ($E_q$)

*Encoder* kueri $E_q$ diinisialisasi menggunakan bobot dari model **BERT base** (uncased) yang telah dilatih pada korpus Bahasa Inggris yang besar (BOOKCORPUS dan WIKIPEDIA).

### B. Pre-Training Encoder Kode ($E_c$)

*Encoder* kode $E_c$ di-*pre-train* menggunakan dataset **CODESEARCHNET** (berasal dari GITHUB) yang berisi definisi fungsi.

1.  **Tugas Pre-training Kode:**
    *   **Masked Source Code Modeling (MCM):** Memprediksi token kode yang ditutup (*masked*) dalam urutan input.
    *   **Next Line Prediction (NLPred):** Tugas klasifikasi biner untuk memprediksi apakah baris kode $B$ langsung mengikuti baris kode $A$. Tugas ini dirancang untuk menggantikan *Next Sentence Prediction* dalam konteks kode.
2.  **Konfigurasi Data:** Data kode dipertahankan *case-sensitively* (peka huruf besar/kecil) karena informasi kasus memiliki sinyal berharga (misalnya, membedakan variabel dan konstanta). Digunakan panjang urutan maksimum $256$ token dan tokenisasi **WORDPIECE**.

### C. Penambangan Pasangan Kueri dan Kode (Fine-Tuning Data)

Dataset *fine-tuning* dikumpulkan dari pertanyaan dan jawaban yang **diterima** dari **STACKOVERFLOW** untuk bahasa JAVASCRIPT, JAVA, dan PYTHON. Judul pertanyaan digunakan sebagai kueri NL, dan *snippet* kode dari jawaban yang diterima digunakan sebagai dokumen kode yang relevan. *Filtering* ketat diterapkan (minimal 3 baris kode, minimal 3 *upvotes* untuk pertanyaan dan jawaban) untuk meningkatkan kualitas data.

### D. Fine-Tuning Multimodal Embedding Model (MEM)

Kedua *encoder* (yang sekarang memiliki bobot *pre-trained*) dirakit menjadi arsitektur MEM. Model dilatih dengan tujuan untuk mengurangi jarak, yaitu *cosine distance*, antara vektor kueri ($E_q$) dan vektor kode ($E_c$) di ruang vektor. Untuk pelatihan digunakan *optimizer* LAMB.

## 3. Detail Pengujian

Dilakukan *10-fold cross-validation* pada dataset.

### A. Strategi Evaluasi

1.  **1K:** Mencari jawaban yang benar di antara $1.000$ *snippet* kode (1 benar dan 999 *distractor*).
2.  **Full:** Menggunakan seluruh *test set* sebagai korpus pencarian.

### B. Metrik Evaluasi

1.  **Mean Reciprocal Rank (MRR):**
    $$
    MRR=\frac{1}{|Q|}\sum_{q=1}^{Q}\frac{1}{Rank_{q}}
    $$
    Di mana $|Q|$ adalah jumlah kueri dan $Rank_{q}$ adalah peringkat tertinggi dari dokumen yang benar.
2.  **Top-k Accuracy:** Persentase kueri yang memiliki dokumen yang benar dalam $k$ hasil teratas.
3.  **Aroma-Based Similarity Score (Aroma):** Metrik kemiripan kode-ke-kode struktural yang digunakan untuk menilai seberapa baik model mengidentifikasi solusi alternatif yang secara semantik dekat dengan jawaban yang benar.

## 4. Hasil Eksperimen

Tabel 7 (dokumen sumber) merangkum hasil utama:

### A. Kinerja Model Transfer Learning Murni (RQ1-RQ3)

*   Pada **Strategi 1K**, model **MEM** yang di-*pre-train* pada kedua modalitas (MEM-{EN+LANG}) mengungguli *baseline* **LUCENE** dan **DEEPCS** di semua bahasa. Peningkatan ini paling signifikan.
    *   Contoh: MEM-{EN+JS}-[JS]-(JS) mencapai MRR $\mathbf{0.3105}$ (vs. Lucene $0.2374$).
*   **Dampak Modalitas:** *Pre-training* pada *Code Encoder* (MEM-{NO+LANG}) memberikan peningkatan kinerja yang lebih besar daripada *Pre-training* hanya pada *Query Encoder* (MEM-{EN+NO}). Hal ini disebabkan kueri NL cenderung sangat pendek ($\approx 9$ token), membatasi manfaat *contextual embedding* BERT.

### B. Model Kombinasi (RQ5)

*   Pada **Strategi Full** (ruang pencarian besar), MEM murni gagal mengungguli *baseline* IR **LUCENE**.
*   Model *pipeline* **LUMEM** (LUCENE $\rightarrow$ MEM) mengatasi masalah ini. **LUMEM-{EN+TP}-[TP]-(TP)** (menggunakan *pre-training* multi-bahasa TOP) mencapai MRR median $\mathbf{0.2050}$ (Top Test Set, Strategi Full), secara signifikan melampaui LUCENE murni ($\mathbf{0.1124}$).
*   **Aroma Score** LUMEM juga tertinggi ($\mathbf{0.4478}$), memvalidasi kemampuan model *hybrid* untuk menemukan solusi yang sangat relevan secara semantik dalam korpus besar.

## 5. Kesimpulan

Paper ini berhasil mendemonstrasikan bahwa **Transfer Learning** adalah metode yang sangat efektif untuk meningkatkan kinerja *code search* berbasis jaringan saraf, terutama dengan menggunakan arsitektur Transformer (BERT). Model MEM yang di-*pre-train* mengungguli semua *baseline* pada korpus kecil.

Temuan paling penting adalah bahwa untuk skenario *code search* skala besar, pendekatan gabungan (**LUMEM**) yang menggunakan metode IR tradisional (**LUCENE**) untuk memfilter kandidat diikuti dengan penyaringan semantik oleh Transformer yang di-*pre-train* (**MEM**) memberikan kinerja terbaik secara keseluruhan.

::: info Dampak Praktis
Model berbasis Transformer yang dirancang untuk NLP dapat diterapkan secara langsung pada tugas analisis kode sumber. Rekomendasi praktisnya adalah mengadopsi model *pipeline* (LUCENE $\rightarrow$ MEM) untuk mesin pencari kode yang dioptimalkan, di mana LUCENE mengurangi ruang pencarian, dan MEM memberikan pemeringkatan semantik yang akurat. Hal ini membuka jalan untuk meningkatkan tugas rekayasa perangkat lunak lain seperti *code summarization* dan *code generation*.
:::

---
Apakah ada paper lain yang ingin Anda rangkum menggunakan format yang sama?