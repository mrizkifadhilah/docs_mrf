---
title: Review Paper - HMOE for Unified Code Search
description: Rangkuman paper tentang model Hierarchical Mixture-of-Experts (HMOE) untuk pencarian kode terpadu yang efisien dan akurat (Information Fusion, 2026).
head:
  - - meta
    - name: keywords
      content: Code Search, Mixture-of-Experts, Hierarchical Model, Unified Model, Deep Learning, Prefix knowledge
---

# 001 - Hierarchical Mixture-of-Experts model for unified code search

**Penulis:** **Zexu Lin** ᵃ, **Xu Zhang** ᵃ, **Xiaoyu Hu** ᵃ, **Pengfei Li** ᵃ, **Deyu Zhou** ᵃ, **Darong Lai** ᵃ,* (Corresponding Author)

**Afiliasi:**
* ᵃ School of Computer Science and Engineering, Key Laboratory of New Generation Artificial Intelligence Technology and Its Interdisciplinary Applications, Ministry of Education, Southeast University, China

**Kronologi:** Received: 30 January 2025 • Revised: 29 June 2025 • Accepted: 2 July 2025 • Available Online: 15 July 2025

<a href="https://www.scimagojr.com/journalsearch.php?q=26099&tip=sid&clean=0" target="_blank"><img src="https://www.scimagojr.com/journal_img.php?id=26099" alt="SCImago Journal Rank" /></a>

| Resume Eksekutif |
| :--- |
| **Publikasi:**<br>• **Jurnal:** Information Fusion, Vol 125 (2026)<br>• **Penerbit:** Elsevier (Q1, High Impact)<br>• **Topik:** Unified Code Search, Mixture-of-Experts (MoE)<br><br>**Masalah & Solusi:**<br>• **Masalah:** Model pencarian kode yang ada monolitik, sulit menangkap hubungan semantik yang rumit, dan tidak efisien/skalabel karena membutuhkan model terpisah untuk setiap bahasa pemrograman.<br>• **Solusi:** Mengusulkan model **Hierarchical Mixture-of-Experts (HMOE)** yang menggabungkan *Macro-MoE* (untuk pola global/bahasa) dan *Micro-MoE* (untuk detail halus/lokal) dalam satu arsitektur terpadu (*unified*).<br><br>**Contoh Penerapan (Input $\to$ Output):**<br>• *Input Kueri:* "how to read a text file line by line in java"<br>• *Proses:* Macro-MoE mengaktifkan pakar "Java" $\to$ Micro-MoE menangkap detail "read file" & "line by line".<br>• *Output Kode:* `BufferedReader reader = new BufferedReader(new FileReader("file.txt"));` ...<br><br>**Metodologi:**<br>• **Dataset:** CodeSearchNet (6 Bahasa: Python, Java, JavaScript, PHP, Ruby, Go).<br>• **Arsitektur:** Menggunakan CoCoSoDa sebagai *backbone*, ditambah modul Macro-MoE (memanfaatkan *prefix knowledge* dari klasifikasi bahasa) dan Micro-MoE (menggunakan *pyramid dilated convolution*).<br><br>**Temuan Kunci:**<br>1. **Akurasi Tinggi:** HMOE mencapai skor MRR keseluruhan **79.18**, mengungguli *state-of-the-art* seperti CoCoSoDa (73.80).<br>2. **Efisiensi Terpadu:** Satu model *unified* mampu menangani 6 bahasa sekaligus, sangat mengurangi biaya *deployment* dan memori.<br>3. **Sinergi Pengetahuan:** Model menunjukkan *transfer learning* positif antar bahasa (pola bahasa sumber daya tinggi membantu bahasa sumber daya rendah).<br><br>**Kontribusi Utama:**<br>• Mengusulkan arsitektur hierarkis baru (HMOE) yang menyeimbangkan kompleksitas komputasi dengan akurasi.<br>• Memperkenalkan mekanisme *Macro-MoE* untuk menangkap pola semantik global (bahasa spesifik).<br>• Membuktikan bahwa model terpadu dapat mengungguli model bahasa tunggal melalui eksperimen ekstensif.<br><br>**Dampak:**<br>• **Efisiensi Industri:** Memungkinkan perusahaan teknologi menggunakan satu model tunggal untuk melayani pencarian berbagai bahasa.<br>• **Akselerasi Pengembangan:** Meningkatkan produktivitas *software engineer* dengan hasil pencarian yang lebih presisi.<br><br>**Tautan:** [10.1016/j.inffus.2025.103489](https://doi.org/10.1016/j.inffus.2025.103489) |

## 1. Pendahuluan & Masalah

*Unified code search* (pencarian kode terpadu) adalah pilar pengembangan perangkat lunak modern, bertujuan mengambil potongan kode yang relevan dari repositori besar menggunakan kueri bahasa alami. Tantangan utama dalam bidang ini adalah **skalabilitas** dan **efisiensi**, karena metode *Deep Learning* yang ada sering kali monolitik dan membutuhkan model terpisah untuk setiap bahasa, serta kesulitan menjembatani *semantic gap* yang rumit.

::: tip Solusi yang Diusulkan
Paper ini memperkenalkan **Hierarchical Mixture-of-Experts (HMOE)**. Ini adalah kerangka kerja berjenjang yang dirancang untuk menyeimbangkan kompleksitas komputasi dengan akurasi pengambilan, memungkinkan satu model tunggal menangani berbagai bahasa pemrograman secara efektif.
:::

## 2. Metodologi: Arsitektur HMOE

Model ini dibangun di atas *backbone* model CoCoSoDa dan terdiri dari dua komponen sinergis utama:

### A. Macro Mixture-of-Experts (Macro-MoE)
Bagian ini berfokus pada pola semantik global dan domain spesifik (spesifik bahasa pemrograman).
* **Fungsi:** Memilih ahli tingkat tinggi (*high-level experts*) untuk menangkap pola bahasa tertentu.
* **Mekanisme:** Menggunakan *code classification* sebagai tugas awal (*source task*) untuk menghasilkan **prefix knowledge** yang disuntikkan ke dalam model pencarian. Prefix ini membawa informasi spesifik bahasa (misal: sintaks Java vs Python).

### B. Micro Mixture-of-Experts (Micro-MoE)
Bagian ini berfokus pada interaksi lokal yang halus (*fine-grained*) antara kueri dan kode.
* **Fungsi:** Memodelkan interaksi mendetail antara kueri dan kode.
* **Mekanisme:** Menggunakan **pyramid dilated convolutions** dengan berbagai ukuran kernel dan *dilation rates*. Ini memungkinkan model melihat kode dari berbagai perspektif (*multi-view*) dan skala untuk menangkap detail kontekstual yang kaya.

### C. Unified Learning (Pembelajaran Terpadu)
Data dari berbagai bahasa diproses menggunakan teknik *Independent and Identically Distributed (IID) sampler* dan *Federated Averaging* (FedAvg) untuk menyeimbangkan distribusi data dan melatih model global yang kuat.

## 3. Detail Pengujian (*Experimental Setup*)

### Dataset
* **Sumber:** CodeSearchNet (CSN).
* **Bahasa:** 6 Bahasa (Ruby, JavaScript, Go, Python, Java, PHP).
* **Volume:** Ratusan ribu pasang kode-kueri untuk pelatihan dan pengujian.

### Baseline
Dibandingkan dengan berbagai model *state-of-the-art*, termasuk **CoCoSoDa** (baseline utama), UniXcoder, CodeBERT, dan lainnya.

### Metrik
Evaluasi menggunakan **Mean Reciprocal Rank (MRR)**, yang mengukur seberapa tinggi peringkat kode yang relevan muncul dalam hasil pencarian.

$$MRR = \frac{1}{|Q|} \sum_{i=1}^{|Q|} \frac{1}{rank_i}$$

## 4. Hasil Eksperimen

### A. Perbandingan Kinerja (Unified Setting)
HMOE secara konsisten mengungguli model lain dalam pengaturan *unified* (satu model untuk semua bahasa).

| Model | MRR Overall | Peningkatan (vs CoCoSoDa) |
| :--- | :--- | :--- |
| UniXcoder | 49.2 | - |
| CoCoSoDa | 73.80 | - |
| **HMOE** | **79.18** | **+5.38%** |

* Pada bahasa **Go**, skor mencapai **91.12**.
* Pada **Ruby** dan **Java**, peningkatan kinerja sangat signifikan berkat *prefix knowledge* yang disediakan oleh Macro-MoE.

### B. Studi Ablasi
Pengujian menunjukkan bahwa penghapusan komponen Macro-MoE atau Micro-MoE secara individual menyebabkan penurunan kinerja, membuktikan bahwa kedua komponen tersebut bekerja secara sinergis dan penting untuk akurasi optimal.

### C. Keunggulan Model Terpadu (Unified vs Single)
HMOE menunjukkan bahwa model *Unified* dapat mengungguli model *Single-Language* dalam hal efisiensi *deployment* (hanya perlu *deploy* 1 model) dan kinerja, karena bahasa dengan sumber daya rendah mendapat manfaat dari *transfer knowledge* yang dipelajari dari bahasa dengan sumber daya tinggi.

## 5. Kesimpulan

Paper ini menyimpulkan bahwa **HMOE** berhasil mengatasi keterbatasan skalabilitas dalam pencarian kode. Dengan menerapkan arsitektur hierarkis, **Macro-MoE** efektif menangani perbedaan distribusi antar bahasa, sementara **Micro-MoE** menangkap detail semantik kode yang halus.

::: info Dampak Praktis
HMOE memungkinkan pengembang membangun mesin pencari kode yang **skalabel** (satu model untuk semua bahasa) tanpa mengorbankan akurasi. Sebaliknya, model ini justru meningkatkan akurasi melalui sinergi pengetahuan lintas bahasa, mempercepat produktivitas *software engineer*.
:::