---
title: Review Paper - GraphSearchNet untuk Code Search Semantik
description: Rangkuman paper tentang GraphSearchNet, kerangka kerja berbasis GNN yang ditingkatkan dengan multi-head attention untuk Semantic Code Search (IEEE Transactions on Software Engineering, 2023).
head:
  - - meta
    - name: keywords
      content: Code search, graph neural networks, multi-head attention, BiGGNN, semantic mapping
---

# 054 - GraphSearchNet: Enhancing GNNs via Capturing Global Dependencies for Semantic Code Search
Tautan (DOI) [https://doi.org/10.1109/TSE.2022.3233901]

**Penulis:** **Shangqing Liu** $^{1}$, **Xiaofei Xie** $^{2*}$, **Jingkai Siow** $^{1}$, **Lei Ma** $^{3}$, **Guozhu Meng** $^{4}$, **Yang Liu** $^{1}$

**Afiliasi:**
* $^{1}$ Nanyang Technological University, Singapore 639798, Singapore
* $^{2}$ Singapore Management University, Singapore 188065, Singapore (Corresponding Author)
* $^{3}$ University of Alberta, Edmonton, AB T6G 2R3, Canada
* $^{4}$ SKLOIS, Institute of Information Engineering, Chinese Academy of Sciences, Beijing 100045, China

**Kronologi:** Received: 5 February 2022 • Revised: 12 November 2022 • Accepted: 28 December 2022 • Available Online: 4 January 2023

<a href="https://www.scimagojr.com/journalsearch.php?q=18711&tip=sid" target="_blank"><img src="https://www.scimagojr.com/journal_img.php?id=18711" alt="SCImago Journal & Country Rank" /></a>

| Resume Eksekutif |
| :--- |
| **Publikasi:**<br>• **Jurnal:** IEEE Transactions on Software Engineering 49, 4 (April 2023)<br>• **Topik:** Peningkatan akurasi *code search* semantik dengan mengatasi kelemahan Jaringan Saraf Graf (*GNNs*) dalam menangkap ketergantungan global (*global dependencies*).<br><br>**Masalah & Solusi:**<br>• **Masalah 1 (Kesenjangan Semantik & Heterogenitas):** Pemetaan semantik tingkat tinggi antara kode sumber dan kueri bahasa alami sulit dilakukan karena sifat kedua bahasa yang heterogen.<br>• **Masalah 2 (Abaikan Struktur):** Model sekuensial (*LSTM, Self-Attention*) gagal memanfaatkan informasi struktural kaya yang tersembunyi dalam kode (*AST, Data-flow*) dan kueri (*Dependency Parsing*).<br>• **Masalah 3 (Keterbatasan GNN):** GNN konvensional (termasuk GGNN) efektif dalam menangkap ketergantungan lokal (*local dependencies*) tetapi gagal menangkap ketergantungan global (*global dependencies*) dari node yang berjauhan dalam graf, membatasi kapasitas pembelajaran model.<br>• **Solusi:** Mengusulkan **GraphSearchNet**, kerangka kerja berbasis graf yang menggunakan **BiGGNN** untuk menangkap informasi struktural lokal, dan **Mekanisme *Multi-Head Attention*** untuk melengkapi ketergantungan global yang terlewatkan oleh BiGGNN, sehingga meningkatkan kapasitas pembelajaran model secara signifikan.<br><br>**Contoh Penerapan:**<br>• Model dilatih pada dataset publik **CodeSearchNet** (Java dan Python) dan diuji pada $\mathbf{99}$ kueri bahasa alami nyata. GraphSearchNet mengembalikan program yang lebih relevan dan akurat (misalnya, untuk kueri "get executable path") dibandingkan dengan *baseline* SOTA berbasis sekuensial (*SelfAtt Encoder*).<br><br>**Metodologi:**<br>• **Konstruksi Graf:** Program diubah menjadi graf multiedge berarah menggunakan *syntactic edges* (AST, NextToken, SubToken) dan *data-flow edges* (ComputedFrom, LastUse, LastWrite). Kueri (*Summary*) diubah menjadi graf berdasarkan *dependency parsing* dan *NextToken/SubToken*.<br>• **BiGGNN (Bidirectional GGNN):** Digunakan untuk kedua *encoder* (Program $f_{c}$ dan Summary $f_{s}$) untuk menangkap informasi struktural **lokal** melalui penyebaran pesan dua arah (incoming dan outgoing).\\($h_{v}^{k}=\text{GRU}(h_{v}^{k-1},h_{\mathcal{N}_{(v)}}^{k})$\\).<br>• **Multi-Head Attention:** Diterapkan pada urutan terminal nodes (tokens) untuk melengkapi ketergantungan **global** yang hilang dari BiGGNN. Vektor hasil *Multi-Head Attention* ($h^{c}$) digabungkan (*concatenate*) dengan vektor BiGGNN ($h^{g}$) untuk representasi akhir $r=[h^{g};h^{c}]$.<br>• **Training:** Menggunakan fungsi *Cross-Entropy Loss* untuk meminimalkan produk dalam (*inner product*) antara kueri dan program yang benar sambil memaksimalkan produk dalam dengan program distractor.\\($-\frac{1}{n}\sum_{i}^{n}log(\frac{r_{i}^{T}r_{i}^{\prime}}{\sum_{j}r_{j}^{T}r_{i}^{\prime}})\\$).<br><br>**Temuan Kunci:**<br>1. **Kinerja SOTA:** GraphSearchNet secara signifikan mengungguli 13 *baseline* SOTA (termasuk NBOW, biRNN, SelfAtt, DeepCS, dan GNN murni) di semua metrik ($R@k$, MRR, NDCG) pada dataset Java dan Python.<br>2. **Eksplorasi Struktur Kueri:** Membangun graf untuk **kueri** (melalui *dependency parsing*) terbukti bermanfaat dan penting, yang sering diabaikan oleh model sebelumnya.<br>3. **BiGGNN vs Multi-Head:** BiGGNN menyumbang peningkatan kinerja yang lebih besar (informasi struktural lokal lebih krusial) daripada Multi-Head Attention, tetapi penggabungan keduanya menghasilkan kinerja terbaik.<br><br>**Kontribusi Utama:**<br>• Mengusulkan kerangka kerja berbasis graf baru untuk menangkap informasi struktural dan semantik secara komprehensif untuk *code search*.<br>• Merancang **GraphSearchNet** untuk meningkatkan kapasitas pembelajaran GNN dengan BiGGNN dan *Multi-Head Attention*.<br>• Melakukan evaluasi ekstensif yang menunjukkan keunggulan signifikan GraphSearchNet atas *baseline* SOTA. |

## 1. Pendahuluan & Masalah

*Code search* otomatis adalah masalah krusial dalam era *Big Code*, bertujuan untuk menemukan *snippet* kode yang akurat berdasarkan kueri bahasa alami (NL), yang secara langsung meningkatkan produktivitas dan kualitas perangkat lunak. Meskipun metode berbasis *Deep Learning* (DL) telah menggantikan metode *Information Retrieval* (IR) tradisional, pemetaan semantik tingkat tinggi antara kode (PL) dan kueri (NL) masih menjadi tantangan besar.

Tiga tantangan utama yang diatasi oleh penelitian ini adalah:
1.  **Heterogenitas Semantik:** Aturan tata bahasa dan struktur antara PL dan NL sangat berbeda, membuat pemetaan semantik menjadi sulit.
2.  **Abaikan Informasi Struktural:** Kebanyakan model DL (sekuensial seperti LSTM atau *Self-Attention*) memperlakukan kode dan kueri sebagai urutan datar (*flat sequence*) token, mengabaikan informasi struktural yang kaya (misalnya, *Abstract Syntax Tree*, *data-flow*) di dalamnya.
3.  **Keterbatasan GNNs dalam Ketergantungan Global:** Meskipun *Graph Neural Networks* (GNNs) telah terbukti efektif dalam memodelkan semantik program, mereka menderita masalah dalam menangkap **ketergantungan global** (*global dependencies*), yaitu hubungan antara node yang berjauhan dalam graf. Hal ini disebabkan oleh jumlah *hops* yang terbatas ($K$) untuk menghindari masalah *over-smoothing* dan *over-squashing* pada GNN.

::: tip Solusi yang Diusulkan
Kami merancang **GraphSearchNet**, kerangka kerja jaringan saraf yang secara kolektif memanfaatkan informasi struktural baik dari kode maupun kueri. GraphSearchNet mengkombinasikan **Bidirectional Gated Graph Neural Network (BiGGNN)** untuk menangkap informasi struktural **lokal** dengan **Mekanisme *Multi-Head Attention*** untuk melengkapi ketergantungan **global** yang hilang, sehingga menghasilkan representasi vektor yang jauh lebih akurat.
:::

## 2. Metodologi

GraphSearchNet beroperasi dalam dua fase: Fase Pelatihan dan Fase Kueri.

### A. Konstruksi Graf
Untuk memaksimalkan pemanfaatan struktur, baik program maupun kueri diubah menjadi graf berarah:

1.  **Graf Program:** Dibangun menggunakan node dari *Abstract Syntax Tree* (AST) dan node *SubToken*. Edge dikategorikan menjadi:
    *   *Syntactic Edges*: AST Edge, NextToken, SubToken (memecah identifier seperti *camelCase* atau *pascal_case*).
    *   *Data-flow Edges*: ComputedFrom, LastUse, LastWrite.
2.  **Graf Summary (Kueri):** Dibangun berdasarkan *dependency parsing* dan diperkuat dengan edge NextToken dan SubToken. *Dependency parsing* dipilih karena secara efektif menangkap hubungan gramatikal antar token dalam kueri.

### B. BiGGNN (Bidirectional Gated Graph Neural Network)
Graf yang telah dikonstruksi diumpankan ke lapisan BiGGNN. BiGGNN dirancang untuk meningkatkan pembelajaran graf konvensional dengan menyertakan penyebaran pesan dua arah (masuk dan keluar) pada setiap node.

Setiap *hop* BiGGNN memperbarui representasi node $h_{v}$ menggunakan unit rekuren GRU:
$$h_{\mathcal{N}_{(v)}}^{k}=\text{Fuse}(h_{\mathcal{N}_{+}(v)}^{k},h_{\mathcal{N}_{\vdash(v)}}^{k})$$
$$h_{v}^{k}=\text{GRU}(h_{v}^{k-1},h_{\mathcal{N}_{(v)}}^{k})$$
Setelah $K$ *hops*, representasi graf lokal $h^{g}$ diperoleh melalui *max-pooling* dari semua representasi node final.

### C. Multi-Head Attention Enhancement
Untuk mengatasi keterbatasan GNN dalam menangkap ketergantungan global (hubungan jarak jauh), modul *Multi-Head Attention* diterapkan pada **urutan terminal node** (tokens/subtokens) program/kueri.
$$\text{Attention}(Q,K,V)=\text{Softmax}(\frac{QK^{T}}{\sqrt{d_{k}}})V$$
Vektor hasil *Multi-Head Attention* ($h^{c}$) diperoleh melalui operasi *mean-pooling* (dipilih karena memberikan hasil yang lebih tinggi).
Representasi vektor akhir $r$ untuk program/kueri adalah hasil penggabungan (*concatenation*) dari output BiGGNN (fitur lokal) dan *Multi-Head Attention* (fitur global):
$$r=[h^{g};h^{c}]$$

## 3. Detail Pengujian

### Dataset
*   **Dataset:** Java dan Python dari **CodeSearchNet** (lebih dari 2 juta fungsi).
*   **Kueri Nyata:** $\mathbf{99}$ kueri bahasa alami nyata digunakan untuk analisis kuantitatif.

### Baseline
Total 13 *baseline* dibandingkan, termasuk:
*   *Sequential Models*: NBOW, biRNN, SelfAtt Encoder, 1D-CNN, UNIF, DeepCS.
*   *GNN Variants*: GGNN, GCN (diimplementasikan dari awal untuk perbandingan).
*   *Retrieval-based*: CodeMatcher.
*   *Pre-trained Models* (dibahas): CodeBERT, GraphCodeBERT.

### Metrik Evaluasi
Metrik otomatis yang digunakan adalah:
*   **Success Rate@k** (R@k, dengan $k=\{1, 5, 10\}$):
$$\text{Success Rate}@k = \frac{1}{|Q|}\sum_{q=1}^{|Q|} \delta(\text{FRank}_{q} \le k)$$
*   **Mean Reciprocal Rank (MRR):**
$$MRR=\frac{1}{|Q|}\sum_{q=1}^{|Q|}\frac{1}{\text{FRank}_{q}}$$
*   **Normalized Discounted Cumulative Gain (NDCG):**
$$\text{NDCG} = \frac{\text{DCG}}{\text{IDCG}}$$

## 4. Hasil Eksperimen

### RQ1: Perbandingan dengan Baseline SOTA
GraphSearchNet secara signifikan mengungguli semua *baseline* berbasis sekuensial dan GNN murni.

| Model | R@1 (Java) | MRR (Java) | R@1 (Python) | MRR (Python) |
| :--- | :--- | :--- | :--- | :--- |
| SelfAtt Encoder | 42.09 | 52.00 | 57.81 | 67.40 |
| UNIF | 52.79 | 62.20 | 57.47 | 67.10 |
| GGNN | 48.59 | 58.72 | 59.69 | 69.07 |
| **GraphSearchNet** | **56.99** | **65.80** | **65.31** | **73.90** |

*   **Keunggulan Jelas:** Peningkatan ini menunjukkan bahwa dengan menangkap informasi struktural (melalui graf) dan melengkapi ketergantungan global (*Multi-Head Attention*), model dapat mempelajari pemetaan semantik yang jauh lebih baik.
*   **Perbandingan GNN:** GraphSearchNet mengungguli GGNN dan GCN, memvalidasi bahwa penyebaran pesan dua arah (BiGGNN) dan penangkapan ketergantungan global sangat penting.
*   **Dampak Bahasa:** Kinerja pada Python lebih tinggi daripada Java. Ini disebabkan oleh aturan tata bahasa Python yang lebih sederhana dan lebih dekat ke bahasa alami.

### RQ2: Studi Ablasi Komponen
Ablasi menguji kontribusi BiGGNN (informasi lokal) dan Multi-Head Attention (informasi global) pada *encoder*.

| Dataset | Program Encoder | Summary Encoder | MRR | p-value |
| :--- | :--- | :--- | :--- | :--- |
| \multirow{5}{*}{Java} | Multi-Head | Multi-Head | 41.74 | 0.01 |
| | BiGGNN | BiGGNN | 55.65 | 0.09 |
| | BiGGNN | Multi-Head | 58.51 | 0.18 |
| | Multi-Head | BiGGNN | 41.74 | 0.01 |
| | **BiGGNN + Multi-Head** | **BiGGNN + Multi-Head** | **65.80** | |

*   **BiGGNN lebih Krusial:** BiGGNN sendiri (MRR 55.65) memberikan peningkatan kinerja yang jauh lebih besar daripada *Multi-Head Attention* saja (MRR 41.74), yang menunjukkan bahwa **informasi struktural lokal** yang ditangkap oleh GNN adalah yang paling kritis.
*   **Sinergi:** Kinerja optimal dicapai ketika **keduanya digabungkan**, mengkonfirmasi bahwa ketergantungan global adalah suplemen yang efektif untuk GNN.

### RQ3: Analisis Hop & Head
*   **Optimal Hops (K):** Jumlah *hops* optimal adalah **4 untuk Java** dan **3 untuk Python**. Perbedaan ini dikaitkan dengan ukuran graf: graf Java rata-rata lebih besar, sehingga memerlukan lebih banyak *hops* untuk menyebarkan pesan.
*   **Optimal Heads (h):** Nilai optimal untuk *Multi-Head Attention* adalah **2 *heads*** pada kedua dataset. Ini menunjukkan bahwa 2 *heads* sudah cukup untuk menangkap ketergantungan global.

## 5. Kesimpulan

GraphSearchNet berhasil mengatasi tantangan pemetaan semantik dan keterbatasan ketergantungan global pada GNN dalam *code search*. Dengan menggabungkan BiGGNN untuk pemodelan struktur lokal dan *Multi-Head Attention* untuk melengkapi ketergantungan global, GraphSearchNet mencapai kinerja *state-of-the-art* yang signifikan pada dataset Java dan Python. Kerangka kerja ini membuktikan bahwa pemanfaatan struktur baik dari kode maupun kueri, dengan penambahan penangkapan ketergantungan global, adalah kunci untuk *semantic code search* yang efektif.

::: info Dampak Praktis
GraphSearchNet menyediakan model yang **akurat secara semantik** dengan tingkat MRR hingga $73.90$ (Python) dan $65.80$ (Java). Hal ini secara langsung meningkatkan efisiensi pengembang dengan mengembalikan program yang paling relevan (memiliki skor kemiripan kosinus tertinggi) ke kueri bahasa alami mereka. Pendekatan ini juga memvalidasi pentingnya memodelkan struktur bahasa alami kueri menggunakan *dependency parsing* dalam tugas *code search*.
:::