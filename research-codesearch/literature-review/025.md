---
title: Review Paper - C2B Model Hibrida CodeT5 dan Bi-LSTM untuk Code Retrieval
description: Rangkuman paper tentang Model Retrieval Kode Semantik C2B Menggunakan CodeT5 dan Bi-LSTM (Applied Sciences, 2024).
head:
  - - meta
    - name: keywords
      content: CodeT5, Bi-LSTM, CodeSearchNet, semantic matching, code retrieval, code reusability
---

# 025 - C2B: A Semantic Source Code Retrieval Model Using CodeT5 and Bi-LSTM
Tautan (DOI) [10.3390/app14135795](https://doi.org/10.3390/app14135795)

**Penulis:** **Nazia Bibi** ¹, **Ayesha Maqbool** ¹, **Tauseef Rana** ¹*, **Farkhanda Afzal** ¹, **Adnan Ahmed Khan** ¹

**Afiliasi:**
* ¹ Department of Computer Software Engineering, National University of Sciences and Technology, Islamabad 44000, Pakistan

**Kronologi:** Received: 13 May 2024 • Revised: 20 June 2024 • Accepted: 26 June 2024 • Published: 2 July 2024

<a href="https://www.scimagojr.com/journalsearch.php?q=21100829268&tip=sid" target="_blank"><img src="https://www.scimagojr.com/journal_img.php?id=21100829268" alt="SCImago Journal & Country Rank" /></a>

| Resume Eksekutif |
| :--- |
| **Publikasi:**<br>• **Jurnal:** Applied Sciences 2024, 14, 5795<br>• **Topik:** Mengusulkan model hibrida untuk pencarian dan rekomendasi kode sumber semantik guna mengatasi kesenjangan semantik antara kueri bahasa alami dan *snippet* kode.<br><br>**Masalah & Solusi:**<br>• **Masalah:** Alat pencarian kode yang ada (berbasis *keyword* atau sintaksis) kesulitan menangkap semantik dan konteks kode yang kompleks (*semantic gap*) dan tidak dapat sepenuhnya memahami maksud kueri pengguna. Model *deep learning* yang ada sering mengabaikan kesenjangan pengetahuan antara *snippet* kode dan kueri pengguna.<br>• **Solusi:** Mengusulkan model hibrida **C2B** (*CodeT5 and Bi-LSTM hybrid model*) yang mengintegrasikan CodeT5 (*pre-trained transformer* domain-spesifik) untuk pemahaman semantik dan Bi-LSTM (*Bidirectional Long Short-Term Memory*) untuk menangkap ketergantungan sekuensial dan konteks kueri secara efektif.<br><br>**Contoh Penerapan:**<br>• **Pencarian Kode Semantik Skalabel:** Model ini diimplementasikan sebagai alat pencarian kode yang dapat menghasilkan *embedding* kode yang efisien, memungkinkan perbandingan cepat menggunakan *cosine similarity* selama pengambilan (retrieval), menjadikannya skalabel.<br><br>**Metodologi:**<br>• **Arsitektur:** Menggunakan arsitektur hibrida CodeT5 dan Bi-LSTM dengan lapisan *Concatenation* dan *Attention*.<br>• **CodeT5:** Digunakan untuk menghasilkan *embedding* kata (dan posisi) yang menangkap semantik domain kode dan kueri.<br>• **Bi-LSTM:** Digunakan setelah *embedding* kata untuk memproses data sekuensial, menangkap informasi kontekstual dari kueri/kode dalam arah maju dan mundur.<br>• **Mekanisme *Attention*:** Menggunakan **Cosine Similarity Attention** untuk menghitung bobot relevansi antara *embedding* kueri dan *embedding* kode sebelum *loss* dihitung.<br>• **Strategi Pelatihan:** Model di-*fine-tune* menggunakan dua fungsi *loss*:<br>   1. **Cosine Embedding Loss (CEL):** Bertujuan untuk meminimalkan jarak kosinus antara *embedding* *clone pairs*.<br>   2. **Binary Cross-Entropy Loss (BCEL):** Melatih model untuk melakukan deteksi *clone* (klasifikasi biner) dan secara otomatis mempelajari *embedding* kode yang kaya informasi selama proses tersebut. (BCEL terbukti lebih baik).<br>• **Inference yang Efisien:** Selama *code retrieval*, *embedding* kode dihitung dan disimpan di awal (*precompute*). Untuk kueri baru, hanya *embedding* kueri yang dihitung, dan *cosine similarity* digunakan untuk peringkat, menghasilkan kecepatan yang jauh lebih tinggi daripada model *cross-encoder* (seperti CodeT5 asli).<br><br>**Temuan Kunci:**<br>1. **Efektivitas:** Model C2B terbaik (di-*fine-tune* dengan BCEL) mencapai F1-score $\mathbf{0.9023}$ pada tugas deteksi *clone*, menunjukkan kinerja yang memuaskan meskipun menggunakan batas *token* yang lebih rendah ($\mathbf{128}$) dibandingkan SOTA ($\mathbf{400}+$).<br>2. **Keunggulan BCEL:** Pelatihan menggunakan BCEL menghasilkan kinerja yang lebih baik (F1 $\mathbf{0.9023}$) daripada CEL (F1 $\mathbf{0.7485}$).<br>3. **Skalabilitas:** Model C2B $\mathbf{100}$ kali lebih cepat daripada arsitektur CodeT5 asli (*cross-encoder*) dalam menjawab 50 kueri (sekitar $\mathbf{1}$ menit vs $\mathbf{102}$ menit) karena menggunakan *precomputed embeddings* dan *cosine similarity*.<br>4. **Performa *Retrieval*:** Dalam tugas *code retrieval* (diukur dengan MRR), C2B mencapai MRR keseluruhan $\mathbf{0.799}$, bersaing ketat dengan CodeT5 (MRR $\mathbf{0.801}$) dan bahkan **mengungguli CodeT5** di bahasa **PHP** (MRR $\mathbf{0.731}$ vs $\mathbf{0.710}$).<br>5. **Ablasi:** Penghapusan CodeT5 menyebabkan penurunan F1-score dari $\mathbf{0.820}$ menjadi $\mathbf{0.670}$. Penghapusan mekanisme *Attention* menyebabkan penurunan F1-score dari $\mathbf{0.820}$ menjadi $\mathbf{0.800}$.<br><br>**Kontribusi Utama:**<br>• Mengusulkan model hibrida C2B CodeT5/Bi-LSTM untuk representasi dan retrieval kode.<br>• Menunjukkan integrasi CodeT5 dan Bi-LSTM untuk menangkap semantik dan ketergantungan sekuensial secara sinergis.<br>• Membuktikan efisiensi dan skalabilitas model *retrieval* berbasis *embedding* yang di-*fine-tune* dengan BCEL dibandingkan model *cross-encoder* asli.<br><br>**Dampak:**<br>• Model C2B menyediakan solusi *code search* yang **akurat** (memahami semantik) dan **skalabel** (efisien dalam *retrieval*), secara signifikan meningkatkan produktivitas pengembang dan kualitas perangkat lunak. |

## 1. Pendahuluan & Masalah

Pencarian dan penggunaan kembali kode merupakan praktik mendasar dalam pengembangan perangkat lunak untuk meningkatkan **produktivitas** dan konsistensi kualitas. Pengembang sering menggunakan kueri teks bebas untuk mencari *snippet* kode dalam basis kode yang luas. Namun, alat pencarian kode tradisional yang mengandalkan metode berbasis *keyword* atau sintaksis seringkali gagal mengatasi **kesenjangan semantik** antara kueri bahasa alami (NL) dan kode sumber (PL). Mereka tidak mampu sepenuhnya memahami **maksud** pengguna dan konteks kompleks di mana kode beroperasi, termasuk dependensi dan nuansa struktur bahasa pemrograman.

Pendekatan *deep learning* terbaru telah menunjukkan peningkatan dalam pencocokan semantik, namun masih cenderung memprioritaskan pembelajaran pola daripada mengatasi secara eksplisit **kesenjangan pengetahuan** yang ada antara *snippet* kode dan kueri pengguna.

::: tip Solusi yang Diusulkan
Penelitian ini mengusulkan model hibrida baru bernama **C2B** (*CodeT5 and Bi-LSTM hybrid model*) yang menggabungkan kekuatan pemahaman domain CodeT5 (model *transformer* yang dilatih untuk kode) dengan kemampuan pemodelan sekuensial dan kontekstual dari Bi-LSTM (Recurrent Neural Network). Integrasi ini bertujuan untuk meningkatkan representasi kode, menangkap semantik, dan mengatasi keterbatasan model yang ada, terutama dalam hal efisiensi dan skalabilitas *retrieval*.
:::

## 2. Metodologi

Model C2B dirancang sebagai arsitektur *bi-encoder* yang memproses kueri dan kode secara independen untuk menghasilkan *embedding* yang kaya semantik, memungkinkan *retrieval* yang cepat berdasarkan *cosine similarity*.

### A. Data Preprocessing

Langkah-langkah *preprocessing* diterapkan untuk membersihkan dan menormalisasi kode sumber. Ini termasuk:
1.  **Penghapusan Kode Tidak Perlu:** Menghapus *whitespace* dan komentar yang tidak penting, tetapi mempertahankan fitur penting seperti nama fungsi, parameter, dan pengenal (*identifier*) yang kaya semantik.
2.  **Tokenization:** Menggunakan *byte-pair-encoding* (BPE) *tokenizer* CodeT5 yang sudah dilatih, yang efektif untuk tugas pemahaman dan generasi.
3.  **Fitur Spesifik Kode:** Memanfaatkan informasi *token-type* (pengenal) dalam kode sumber untuk ekstraksi fitur spesifik CodeT5.
4.  **Encoding Program:** Menggunakan *token* `[CLS]` di awal urutan, diikuti *token* kode, dan diakhiri dengan *token* `[PAD]` untuk *padding*.

### B. Word Embeddings (CodeT5 dan Bi-LSTM)

Langkah ini menghasilkan *embedding* kueri dan kode secara terpisah untuk CodeT5 dan Bi-LSTM, yang kemudian akan dibandingkan:
*   **CodeT5 Embeddings:** Menggunakan *encoder* CodeT5, yang menggabungkan *token embeddings* ($E$) dengan *positional encodings* ($P$) menggunakan fungsi $sin$ dan $cos$.
    $$ Query_{Embedding}=sum(E_{Q}+P_{Q}) $$
    $$ Code_{Embedding}=sum(E_{C}+P_{C}) $$
    *Transformer* CodeT5 memastikan *embedding* menangkap hubungan kontekstual.
*   **Bi-LSTM Embeddings:** Mengambil *token embeddings* dari CodeT5 dan memasukkannya ke lapisan Bi-LSTM. Bi-LSTM memproses urutan dalam arah maju ($H_{i_{forward}}$) dan mundur ($H_{i_{backward}}$) untuk menangkap konteks sekuensial yang komprehensif.

### C. C2B Layer

Lapisan ini menggabungkan output dari kedua model dan menerapkan mekanisme penekanan.
1.  **Concatenation:** *Embedding* kueri dan kode dari CodeT5 dan Bi-LSTM digabungkan secara horizontal untuk membentuk representasi gabungan.
    $$ Concat_{(Q\_Vec)}=Concat[QEmb_{CodeT5}, QEmb_{BiLSTM}] $$
    $$ Concat_{(C\_Vec)}=Concat[CEmb_{CodeT5}, CEmb_{BiLSTM}] $$
2.  **Attention (Cosine Similarity Attention):** Mekanisme *attention* dihitung antara *embedding* kueri dan *embedding* kode menggunakan **Cosine Similarity** untuk menentukan relevansi.
    $$ \text{Cosine Similarity}(Q_i, C_j) = \frac{(Q_i \cdot C_j)}{||Q_i|| ||C_j||} $$
    $$ A_{ij} = \text{softmax} (\text{Cosine Similarity} (Q_i, C_j)) $$
3.  **Aggregation:** *Embedding* kode yang dibobotkan ($A_{ij} * C_j$) dijumlahkan atau dirata-ratakan untuk mendapatkan representasi kode teragregasi yang berfokus pada bagian kode yang paling relevan dengan kueri.

### D. Training Procedure (BCEL vs. CEL)

Model di-*fine-tune* dengan CodeT5 yang sudah dilatih sebelumnya. Dua strategi *loss* diuji:
1.  **Cosine Embedding Loss (CEL):** Bertujuan meminimalkan sudut kosinus antara *embedding* pasangan *clone* dan memaksimalkan sudut pasangan *non-clone*.
    $$ l_{CE}(z_1, z_2, TL) = \begin{cases} 1 - \cos(z_1, z_2) & \text{if } TL=1 \\ \max(0, \cos(z_1, z_2) - \gamma) & \text{otherwise} \end{cases} $$
    Di mana $TL$ adalah *true label* dan $\gamma$ adalah *margin*.
2.  **Binary Cross-Entropy Loss (BCEL):** Digunakan untuk tugas deteksi *clone* (klasifikasi biner) pada pasangan *embedding* yang digabungkan, yang dimasukkan ke *feedforward neural network* (Classifier). Model secara otomatis mempelajari *embedding* yang optimal selama proses klasifikasi.
    $$ l_{BCE}(y, \hat{y}) = \frac{1}{N} \sum_{i=1}^N [y_i \cdot \log(\hat{y}_i) + (1 - y_i) \cdot \log(1 - \hat{y}_i)] $$
    BCEL terbukti lebih unggul dalam pembelajaran *embedding*.

## 3. Detail Pengujian

### Dataset
**CodeSearchNet:** Dataset skala besar yang berisi pasangan kueri NL dan *snippet* kode (PL) untuk berbagai bahasa (Java, Python, Javascript, Ruby, Go, PHP). Total sampel pelatihan adalah $\mathbf{4,576,757}$.

### Baseline Models
Dibandingkan dengan berbagai model klasik dan *deep learning* SOTA, termasuk:
*   **Klasik:** NBOW, CNN.
*   **Deep Learning:** BiRNN, SelfAtt.
*   **SOTA:** RoBERTa, CodeBERT, FA-AST, GraphCodeBERT, CodeT5 (arsitektur asli).

### Metrik Evaluasi
1.  **Deteksi Clone:** Precision (P), Recall (R), F1-Score.
2.  **Code Retrieval:** Mean Reciprocal Rank (**MRR**).

Precision:
$$ \text{Precison}(P) = TP / (TP + FP) $$

Recall:
$$ \text{Recall}(R) = TP / (TP + FN) $$

F1-Score:
$$ \text{F1-Score} = 2 \times (P \times R) / (P + R) $$

## 4. Hasil Eksperimen

### Perbandingan Strategi Pelatihan (Eksperimen 1.1)
*Model C2B yang di-*fine-tune* dengan **BCEL** mencapai F1-Score **0.9023**, jauh lebih tinggi daripada C2B dengan CEL (0.7485), mengonfirmasi BCEL sebagai strategi *loss* yang lebih baik untuk pembelajaran *embedding* kode.*

### Kinerja Deteksi Clone (Eksperimen 1.2)
Meskipun menggunakan batas *token* kode yang jauh lebih rendah ($\mathbf{128}$) dibandingkan CodeT5 dan SOTA lainnya ($\mathbf{400}+$), model C2B terbaik (BCEL) menunjukkan kinerja yang kompetitif.

| Arsitektur | Precision | Recall | F1 Score |
| :--- | :--- | :--- | :--- |
| CodeT5 [18] (Asli, $\ge 400$ token) | 0.952 | 0.947 | **0.950** |
| GraphCodeBERT [54] | 0.948 | 0.952 | **0.950** |
| Proposed C2B (BCEL, 128 token) | 0.897 | 0.907 | 0.902 |

### Kinerja Code Retrieval (MRR)

| Model | PHP | OVERALL (MRR) |
| :--- | :--- | :--- |
| CodeT5 | 0.710 | **0.801** |
| **Proposed C2B** | **0.731** | 0.799 |

*Model C2B mencapai **MRR keseluruhan 0.799**, yang sangat mendekati SOTA CodeT5 (0.801). Khususnya di bahasa PHP, C2B **mengungguli** CodeT5 (0.731 vs 0.710), menunjukkan kekuatan dan fleksibilitasnya.*

### Waktu Eksekusi (Eksperimen 2)
Model C2B menunjukkan skalabilitas yang superior karena hanya menghitung *embedding* kueri dan membandingkannya dengan *precomputed embeddings* kode menggunakan *cosine similarity*.

| Model | Waktu Eksekusi (50 Queries) | Perbandingan Kecepatan |
| :--- | :--- | :--- |
| CodeT5 (Asli) | $\approx 102$ menit | $\approx 100 \times$ lebih lambat |
| **C2B Hybrid Model** | $\approx 1$ menit | $\mathbf{1 \times}$ |

### Ablation Study

| Model | F1-Score | Akurasi |
| :--- | :--- | :--- |
| Original C2B Model | 0.820 | 0.902 |
| Tanpa CodeT5 | 0.670 | 0.750 |
| Tanpa Mekanisme Attention | 0.800 | 0.850 |

*Analisis ablasi menegaskan bahwa **CodeT5** adalah komponen penting untuk pemahaman NL/PL secara keseluruhan, dan mekanisme **Attention** sangat penting untuk fokus dan akurasi yang lebih tinggi.*

## 5. Kesimpulan

Penelitian ini berhasil memperkenalkan **C2B**, model hibrida CodeT5 dan Bi-LSTM, yang secara efektif mengatasi tantangan kesenjangan semantik dalam *source code retrieval*. Model C2B memanfaatkan CodeT5 untuk pemahaman semantik domain dan Bi-LSTM untuk menangkap konteks sekuensial.

Temuan utama menunjukkan bahwa C2B yang di-*fine-tune* dengan **BCEL** memberikan hasil yang kompetitif dalam hal efektivitas (*retrieval* MRR $0.799$), dan secara dramatis meningkatkan **skalabilitas** dan **efisiensi** (*retrieval* $\approx 100 \times$ lebih cepat daripada CodeT5 asli) dengan menggunakan *precomputed embeddings* dan *cosine similarity*.

::: info Dampak Praktis
Model C2B menyediakan kerangka kerja yang kuat dan efisien untuk **pencarian kode semantik berskala besar**. Dengan akurasi yang sebanding dengan model SOTA (*cross-encoder*) tetapi dengan kecepatan *retrieval* yang jauh lebih tinggi, C2B secara langsung meningkatkan **produktivitas pengembang** dan memfasilitasi penggunaan kembali kode yang berkualitas dalam skenario pengembangan perangkat lunak praktis.
:::