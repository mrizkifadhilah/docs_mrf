---
title: Review Paper - Code Search Python sebagai Translation Retrieval dengan Dual Encoders
description: Rangkuman paper tentang Pendekatan Code Search untuk Python sebagai Masalah Translation Retrieval dengan Dual Encoders (Empirical Software Engineering, 2025).
head:
  - - meta
    - name: keywords
      content: Code search, Deep neural networks, Dual encoders, Translation Retrieval, Unified Language Model, Cosine Similarity Loss, Python
---

# 011 - Approaching code search for python as a translation retrieval problem with dual encoders
[https://doi.org/10.1007/s10664-024-10580-3]

**Penulis:** **Monoshiz Mahbub Khan** ᵃ, **Zhe Yu** ᵃ

**Afiliasi:**
* ᵃ Rochester Institute of Technology, Rochester, NY, USA

**Kronologi:** Received: 21 October 2024 • Revised: • Accepted: 21 October 2024 • Available Online: 30 October 2024

<a href="https://www.scimagojr.com/journalsearch.php?q=18650&tip=sid" target="_blank"><img src="https://www.scimagojr.com/journal_img.php?id=18650" alt="SCImago Journal & Country Rank" /></a>

| Resume Eksekutif |
| :--- |
| **Publikasi:**<br>• **Jurnal:** Empirical Software Engineering, Vol 30, Isu 12, Artikel 12 (2025)<br>• **Penerbit:** Springer Science+Business Media, LLC<br>• **Topik:** Meningkatkan efektivitas dan efisiensi *Code Search* Python menggunakan arsitektur *Dual Encoder* yang disederhanakan dan *Unified Language Model*.<br><br>**Masalah & Solusi:**<br>• **Masalah:** *Code search* tradisional tidak efektif karena kurangnya kesamaan kosakata antara kueri (*Natural Language/NL*) dan kode (*Programming Language/PL*). Model *Dual Encoder* SOTA sebelumnya seringkali mahal (*expensive*) karena memerlukan *multiple encoders*, model *pre-trained* terpisah, fungsi rugi berbasis klasifikasi, atau ekstraksi/pengkodean informasi kontekstual tambahan (seperti AST atau *Data Flow*).<br>• **Solusi:** Mengusulkan pendekatan yang memperlakukan *code search* sebagai masalah *translation retrieval* dengan **Dual Encoders** tetapi menggunakan arsitektur yang disederhanakan, lebih cepat, dan lebih murah: (1) **Unified FastText Language Model** untuk *word embedding* kueri dan kode (memanfaatkan tumpang tindih kosakata); dan (2) **Cosine Similarity Loss Function** alih-alih *classification loss*, memungkinkan model belajar pola mana yang penting untuk membedakan pasangan terkait (*linked*) dan tidak terkait (*non-linked*) dalam ruang *embedding* bersama (*shared embedding space*).<br><br>**Contoh Penerapan:**<br>• **Pencarian Kode Python:** Model dilatih pada dataset *CodeSearchNet Python* dan variasinya (AdvTest, DGMS) untuk memproyeksikan urutan NL dan PL ke ruang *embedding* bersama, di mana kedekatan (*cosine distance*) mencerminkan kesamaan semantik.<br>• **Fungsi Tujuan:** Tujuan pelatihan adalah membuat *embedding* pasangan yang terhubung menjadi lebih dekat dan *embedding* pasangan yang tidak terhubung menjadi lebih jauh. **Analisis:** Setelah pelatihan, skor kesamaan rata-rata pasangan tidak terhubung turun drastis (dari 0.57 menjadi 0.09), sementara pasangan terhubung tetap tinggi (0.79 menjadi 0.72).<br><br>**Metodologi:**<br>• **Arsitektur:** *Dual Encoder* sederhana. Setiap *encoder* memiliki tiga lapisan berulang (Dense, Dropout, Combination) dan dua *pass* (pengulangan umpan balik), diikuti oleh lapisan ReLU dan Normalisasi L2.<br>• **Embedding:** Menggunakan model FastText CBOW terpadu (*unified*) dengan dimensi 300, yang dilatih **sebelum** pelatihan *Dual Encoder* untuk memanfaatkan informasi *sub-word level* dan tumpang tindih kosakata.<br>• **Loss Function:** *Binary Cross-Entropy Loss* yang dimodifikasi berdasarkan *Cosine Similarity* target. Fungsi ini menghilangkan kebutuhan untuk secara eksplisit menghasilkan pasangan negatif (*non-linked*) yang seimbang.<br><br>**Temuan Kunci:**<br>1. **Kinerja SOTA:** Model yang diusulkan mencapai kinerja SOTA pada *CodeSearchNet Python* (Limited MRR 0.9186, Full MRR 0.7616), melampaui TOSS, CodeT5+, CoCoSoDa, dan CodeBERT.<br>2. **Efisiensi:** Arsitektur ini **jauh lebih murah, lebih cepat, dan kurang kompleks** dibandingkan model SOTA yang memerlukan *pre-training* ekstensif atau fitur tambahan (AST, Data Flow).<br>3. **Peran Data:** Tumpang tindih kosakata yang tinggi antara kueri dan kode dalam Python (*docstrings* dan nama fungsi/variabel yang intuitif) sangat **menguntungkan** model ini.<br><br>**Kontribusi Utama:**<br>• Mengembangkan *pipeline Dual Encoder* yang disederhanakan menggunakan *unified FastText embeddings* dan *cosine similarity loss*.<br>• Membuktikan efektivitas arsitektur yang lebih ringan dan cepat ini melebihi model *pre-trained* yang lebih kompleks pada *code search* Python.<br>• Menganalisis peran kritis dari tumpang tindih kosakata dalam data terhadap kinerja model.<br><br>**Dampak:**<br>• Menyediakan alternatif yang lebih murah, lebih cepat, dan lebih baik untuk tugas *code search* Python, meningkatkan efisiensi pemeliharaan dan ekstensi sistem perangkat lunak. |

## 1. Pendahuluan & Masalah

*Code search* semantik adalah tugas krusial dalam pemeliharaan dan perluasan sistem perangkat lunak, bertujuan untuk mengambil artefak kode yang relevan berdasarkan kueri bahasa alami. Tantangan utama dalam *code search* adalah kurangnya kesamaan kosakata (*shared vocabulary*) antara kueri bahasa alami dan kode, yang membuat proses *Information Retrieval* (IR) tradisional tidak efektif.

Karya-karya kontemporer telah membingkai *code search* sebagai tugas *translation retrieval*, seringkali menggunakan arsitektur *dual encoder* untuk memproyeksikan kueri dan kode ke ruang *embedding* bersama. Namun, model *dual encoder* SOTA sebelumnya cenderung kompleks, mengandalkan:
1.  Model bahasa terpisah untuk NL dan PL.
2.  *Pre-training* ekstensif pada berbagai tugas.
3.  Fungsi rugi berbasis klasifikasi yang memerlukan pembuatan pasangan negatif (*non-linked*) yang seimbang secara terpisah.
4.  Informasi kontekstual tambahan yang mahal (*resource-intensive*) seperti *Abstract Syntax Tree* (AST) atau *Data Flow*.

Ketergantungan pada fitur tambahan dan proses *pre-training* yang intensif sumber daya membuat model-model ini mahal dan memakan waktu, misalnya CodeBERT membutuhkan 12 jam dan GraphCodeBERT 83 jam untuk *pre-training*.

::: tip Solusi yang Diusulkan
Kami mendekati *code search* untuk Python sebagai masalah *translation retrieval* dengan **Dual Encoders** yang disederhanakan. Solusi ini menggunakan **Unified FastText Language Model** untuk *word embedding* dari NL dan PL, memanfaatkan tumpang tindih kosakata alami dalam data Python. Selain itu, kami menggunakan **Cosine Similarity Loss Function** yang langsung mengajarkan model untuk membedakan pasangan yang terhubung dan tidak terhubung, tanpa perlu membuat pasangan negatif secara terpisah, menghasilkan model yang **lebih murah, lebih cepat, dan lebih efektif**.
:::

## 2. Metodologi

Pendekatan yang diusulkan membangun *pipeline* di sekitar arsitektur *dual encoder* sederhana yang dilatih dengan *word embeddings* FastText terpadu dan *cosine similarity loss function*.

### A. FastText Unified Embedding

*   **Pemrosesan Awal:** Data NL dan PL difilter dari simbol non-ASCII dan non-alfanumerik, dan nama variabel/fungsi dipecah menjadi kata-kata terpisah.
*   **Model Bahasa Terpadu:** Model bahasa **FastText CBOW** terpadu (*unified*) dengan dimensi 300 dilatih dari awal pada **semua** teks (NL dan PL).
*   **Keuntungan:** Karena data Python memiliki tumpang tindih kosakata yang signifikan antara NL (*docstrings*) dan PL (*function/variable names*), model terpadu menghasilkan *embedding* yang serupa untuk artefak terkait. FastText juga memanfaatkan informasi *sub-word level*.

### B. Dual Encoder Architecture

*   **Struktur:** Dua *encoder* terpisah (satu untuk teks NL, satu untuk kode PL) memproyeksikan *embedding* ke ruang *embedding* bersama.
*   **Lapisan Encoder:** Setiap *encoder* terdiri dari tiga lapisan berulang: *Dense Layer*, *Dropout Layer* (mengatasi *overfitting* dan memaksa pembelajaran dari seluruh urutan), dan *Combination Layer* (menggabungkan *output* dengan *input pass* saat ini).
*   **Multiple Pass:** Urutan *input* melalui **dua *pass*** dari tiga lapisan berulang ini.
*   **Lapisan Akhir:** *Output* dari *pass* kedua diteruskan ke lapisan **ReLU** (memastikan nilai non-negatif) dan lapisan **Normalisasi L2** (menyederhanakan perhitungan kesamaan kosinus), menghasilkan urutan kode/teks yang dikodekan.

### C. Cosine Similarity Loss Function

*   **Fungsi Rugi:** Berbeda dengan *classification loss* yang digunakan oleh banyak karya sebelumnya, arsitektur ini menggunakan *Binary Cross-Entropy Loss* yang dimodifikasi berdasarkan kesamaan kosinus, di mana skor kesamaan yang diprediksi dibandingkan dengan target kesamaan.
*   **Target Kesamaan:** Untuk pasangan artefak $(X_i, Y_j)$, kesamaan kosinus targetnya adalah:

$$S(X_{i},Y_{j})=(X_{i}\cdot X_{j}+Y_{j}\cdot Y_{i})/2$$

*   **Loss Calculation:** *Binary cross-entropy loss* kemudian dihitung:

$$L(X, Y) = S(X, Y) \log(X_i Y_j) (1- S(X, Y)) \log(1- X_i Y_j)$$

Penggunaan *similarity loss* ini secara langsung memungkinkan model untuk belajar pola yang membedakan pasangan yang terhubung dan tidak terhubung, sekaligus menghilangkan langkah yang mahal untuk menghasilkan pasangan negatif (*non-linked*) yang seimbang secara eksplisit.

## 3. Detail Pengujian

### Dataset
Model dievaluasi pada tiga variasi dataset berbasis Python:
*   **CodeSearchNet Python (Limited/Full):** Dataset dasar dengan pasangan dokumentasi-fungsi. *Limited* menguji dengan 1000 artefak (*distractors*), sementara *Full* menggunakan seluruh *testing set* (22,176 artefak).
*   **AdvTest:** Versi CodeSearchNet yang dibatasi, di mana nama fungsi dan variabel diganti dengan *token* khusus ("func", "argi") untuk menguji kemampuan model tanpa bergantung pada nama unik.
*   **DGMS:** Versi CodeSearchNet di mana *docstrings* dari kode digunakan sebagai artefak NL, dan *testing set* hanya 1000 pasangan.

### Baseline
Model dibandingkan dengan SOTA relevan, termasuk: CodeBERT, GraphCodeBERT, Uni-LCRS, SynCoBERT, CoCoSoDa, CodeT5+, TOSS, dan DGMS.

### Metrik Evaluasi
Metrik utama yang digunakan adalah **Mean Reciprocal Rank (MRR)**, karena digunakan oleh semua *baseline*.

$$MRR = \frac{1}{Q} \sum_{i=1}^{Q} \frac{1}{Rank_i}$$
Di mana $Q$ adalah jumlah kueri dan $Rank_i$ adalah peringkat dari artefak kode terkait yang benar untuk kueri $i$. Metrik tambahan: *Accuracy*, *Mean Average Precision* (*MAP*@1), dan *Mean Average Accuracy* (*MAA*@1).

## 4. Hasil Eksperimen

### RQ1: Efektivitas Model Sederhana
Model yang diusulkan mencapai kinerja SOTA pada semua dataset Python yang diuji.

| Model | CodeSearchNet (Limited) | CodeSearchNet (Full) | AdvTest | DGMS |
| :--- | :--- | :--- | :--- | :--- |
| **Pendekatan Kami** | **0.9186** | **0.7616** | **0.5967** | **0.9385** |
| TOSS (Hu et al. 2023) | - | 0.759 | - | - |
| CodeT5+ (Wang et al. 2023) | - | 0.758 | 0.447 | - |
| CoCoSoDa (Shi et al. 2023) | - | 0.757 | - | - |
| DGMS (Ling et al. 2021) | - | - | - | 0.922 |
| GraphCodeBERT (Guo et al. 2021) | 0.879 | 0.692 | - | - |
| CodeBERT (Feng et al. 2020) | 0.8685 | 0.672 | 0.507 | - |

**Analisis:** Arsitektur *dual encoder* sederhana ini melampaui SOTA, termasuk model yang menggunakan *pre-training* ekstensif dan fitur kontekstual (AST, Data Flow). Hal ini memvalidasi hipotesis bahwa penggunaan **model bahasa terpadu** (memanfaatkan tumpang tindih kosakata) dan **loss fungsi berbasis kesamaan kosinus** yang tepat menawarkan alternatif yang lebih cepat dan lebih efisien. Pelatihan model ini hanya membutuhkan waktu sekitar 2 jam, jauh lebih cepat daripada *pre-training* model *baseline* seperti CodeBERT (10 jam) dan GraphCodeBERT (83 jam).

### RQ2: Peran Data dan Model
*   **Tumpang Tindih Kosakata Menguntungkan:** Model menunjukkan kinerja terbaik pada *CodeSearchNet Python* di mana tumpang tindih kata antara kueri dan kode (melalui *docstrings* dan nama fungsi/variabel yang umum) sangat tinggi. Sebelum pelatihan, skor kesamaan rata-rata pasangan terhubung adalah 0.79 dan pasangan tidak terhubung adalah 0.57.
*   **Diferensiasi Semantik:** Setelah pelatihan, model berhasil menurunkan skor kesamaan rata-rata pasangan tidak terhubung secara signifikan menjadi **0.09** (dari 0.57), sementara mempertahankan skor pasangan terhubung pada **0.72**. Hal ini menunjukkan bahwa model tidak hanya mengandalkan tumpang tindih kata tetapi **belajar membedakan** relevansi kontekstual.
*   **Kapasitas Kontekstual:** Pada dataset AdvTest, di mana nama fungsi dan variabel diganti dengan *token* generik, skor MRR turun (dari 0.7616 menjadi 0.5967). Namun, model masih mengungguli *baseline* di AdvTest (seperti CodeBERT 0.507), membuktikan bahwa ia mampu mengekstrak informasi kontekstual dari pasangan NL/PL meskipun informasi unik (seperti nama variabel) dihilangkan.

## 5. Kesimpulan

Pekerjaan ini berhasil membingkai ulang *code search* Python sebagai masalah *translation retrieval* dengan arsitektur *dual encoder* yang efisien. Dengan menggunakan *unified FastText word embeddings* dan *cosine similarity loss*, model ini mencapai kinerja SOTA di *CodeSearchNet Python* sambil secara drastis mengurangi kompleksitas komputasi dan waktu pelatihan. Analisis menunjukkan bahwa kinerja model ini sangat diuntungkan oleh tumpang tindih kosakata alami dalam data Python, namun model tersebut mampu belajar diferensiasi semantik yang kuat, bahkan ketika nama-nama unik dihilangkan.

::: info Dampak Praktis
Model ini menawarkan **solusi *code search* yang cepat, murah, dan efektif** untuk Python, menjadikannya sangat praktis untuk digunakan dalam organisasi yang memelihara *codebase* besar. Keberhasilan dalam memanfaatkan *unified language model* dan *similarity loss* membuka jalan bagi aplikasi *dual encoder* yang efisien di masa depan pada tugas-tugas *multi-language translation* serupa di mana tumpang tindih kosakata data tinggi.
:::