---
title: Review Paper - SECON Semantic Consistency in Data Augmentation for Code Search
description: Rangkuman paper tentang Mempertahankan Konsistensi Semantik dalam Augmentasi Data untuk Pencarian Kode (ACM Transactions on Information Systems, 2025).
head:
  - - meta
    - name: keywords
      content: code search, data augmentation, semantic consistency, pre-trained models, dual-encoder, contrastive learning
---

# 012 - SECON: Maintaining Semantic Consistency in Data Augmentation for Code Search
[https://doi.org/10.1145/3686151]

**Penulis:** **Xu Zhang** ᵃ, **Zexu Lin** ᵃ, **Xiaoyu Hu** ᵃ, **Jianlei Wang** ᵇ, **Wenpeng Lu** ᵇ, **Deyu Zhou** ᵃ*

**Afiliasi:**
* ᵃ School of Computer Science and Engineering, Key Laboratory of New Generation Artificial Intelligence Technology and Its Interdisciplinary Applications, Ministry of Education, Southeast University, Nanjing, China
* ᵇ School of Computer Science and Technology, Key Laboratory of Computing Power Network and Information Security, Ministry of Education, Shandong Computer Science Center (National Supercomputer Center in Jinan), Qilu University of Technology (Shandong Academy of Sciences), Jinan, China

**Kronologi:** Received: 31 January 2024 • Revised: 22 June 2024 • Accepted: 26 July 2024 • Available Online: January 2025

<a href="https://www.scimagojr.com/journalsearch.php?q=18997&tip=sid" target="_blank"><img src="https://www.scimagojr.com/journal_img.php?id=18997" alt="SCImago Journal & Country Rank" /></a>

| Resume Eksekutif |
| :--- |
| **Publikasi:**<br>• **Jurnal:** ACM Transactions on Information Systems, Vol 43, No. 2, Article 36 (2025)<br>• **Topik:** Peningkatan *Code Search* melalui *Data Augmentation* berbasis *Representation* dengan menjaga Konsistensi Semantik (*SECON*).<br><br>**Masalah & Solusi:**<br>• **Masalah:** *Code search* berbasis *Pre-Trained Model* (*PTM*) yang menggunakan *Contrastive Learning* sangat bergantung pada *Data Augmentation* (*DA*). Metode DA tradisional (*raw data level*) sering **mengabaikan Konsistensi Semantik** (*SECON*), menghasilkan sampel yang tidak realistis (seperti templat kode dengan *placeholder*) dan memperkuat bias dataset, serta memerlukan biaya *pre-processing* dan pelatihan yang tinggi.<br>• **Solusi:** Mengusulkan **SECON**, sebuah metodologi DA inovatif yang beroperasi pada **tingkat representasi (*representation level*)** bukan pada data mentah. SECON menggunakan: (1) **Pembekuan Lapisan Jaringan (*Network Freezing*)** pada PTM untuk menghasilkan sampel positif tanpa bias; dan (2) **Mekanisme Interaksi (*Interaction Mechanism*)** yang diperkuat *Self-Attention* untuk memasukkan fitur interaksi kode-kueri ke dalam sampel yang diaugmentasi.<br><br>**Contoh Penerapan:**<br>• **Peningkatan *Code Search* PTM:** SECON diterapkan untuk meningkatkan kinerja model PTM yang sudah ada (CoCoSoDa, UniXcoder, GraphCodeBERT) dalam tugas *code search* di dataset CSN multi-bahasa (Ruby, JavaScript, Go, Python, Java, PHP).<br>• **Hasil Terbaik:** SECON (dengan *GCBS*) mencapai MRR keseluruhan **81.1**, mengungguli *baseline* SOTA seperti CoCoSoDa (78.8) dan CPLCS (78.9).<br><br>**Metodologi:**<br>• **Data Augmentation Level Representasi:** Membekukan lapisan spesifik di PTM (*Encoder***) untuk mendapatkan representasi yang stabil dan kaya semantik sebagai sampel positif ($q_i^*, c_i^*$). Ini menghindari komputasi *gradient* dan menghemat sumber daya.<br>• **Mekanisme Interaksi:** Menggunakan *Self-Attention* (*Poly-encoder* terinspirasi) untuk menghitung korelasi antara *embedding* kueri ($q_i^*$) dan *set* fitur kode ($y_{code}^r$). Hal ini menghasilkan representasi kode yang diaugmentasi ($c'$) yang telah terintegrasi dengan perhatian *query embedding*, memungkinkan interaksi yang lebih mendalam.<br>• **Optimization Objective:** Menggunakan *Contrastive Loss* dengan kerangka pembobotan umum (*generalized weighting framework*) untuk *cross-modal matching* guna memaksimalkan jarak antara pasangan positif/negatif, termasuk sampel yang diaugmentasi interaktif.<br><br>**Temuan Kunci:**<br>1. **Kinerja Unggul:** SECON (dengan GCBS) mencapai MRR keseluruhan 81.1, menetapkan SOTA baru pada dataset CSN.<br>2. **Efek Pembekuan Lapisan:** Pembekuan semua lapisan (*Full Layer Frozen*) pada PTM terbukti menjadi strategi terbaik untuk mendapatkan sampel representasi yang konsisten secara semantik dan paling efektif untuk DA pada tingkat representasi.<br>3. **Keunggulan Augmentasi Sisi Kode:** Augmentasi yang diterapkan pada sisi kode (*code-side augmentation*) cenderung mencapai hasil yang lebih menonjol karena kode seringkali lebih panjang dan mengandung informasi berlebihan (*redundancy*), memungkinkan interaksi yang lebih baik dengan kueri dibandingkan augmentasi sisi kueri.<br><br>**Kontribusi Utama:**<br>• Mengusulkan SECON, metodologi *Data Augmentation* tingkat representasi yang menjaga Konsistensi Semantik melalui *network freezing*.<br>• Memperkenalkan mekanisme interaksi *Self-Attention* ke dalam sampel augmentasi untuk meningkatkan keselarasan semantik kode-kueri.<br>• Menunjukkan efisiensi komputasi yang lebih baik dan kinerja *code search* yang unggul pada dataset multi-bahasa.<br><br>**Dampak:**<br>• Menyediakan teknik *Data Augmentation* yang lebih andal dan efisien untuk *code search* modern, meningkatkan akurasi dan efisiensi dalam penemuan kode dan pemahaman fungsionalitas bagi pengembang. |

## 1. Pendahuluan & Masalah

Teknik *code search* sangat penting dalam rekayasa perangkat lunak modern, bertujuan untuk membantu pengembang menemukan *code snippet* yang relevan dan memahami fungsionalitas kode secara efisien dalam repositori besar. Penelitian dan pengembangan mesin *code search* telah berkembang dari pendekatan berbasis *Information Retrieval* (IR), *Deep Learning* (DL), hingga yang paling dominan saat ini, berbasis *Pre-Trained Model* (PTM).

Pendekatan PTM, seringkali dikombinasikan dengan *Contrastive Learning*, memproyeksikan kode dan kueri bahasa alami ke ruang vektor bersama, memaksimalkan skor korelasi antara pasangan yang terhubung. Kunci efektivitas dalam *Contrastive Learning* adalah *Data Augmentation* (DA) yang menghasilkan sampel positif dan negatif untuk pembelajaran diskriminatif.

Masalah utama muncul dari metode DA konvensional yang beroperasi pada **tingkat data mentah (*raw data level*)**. Contohnya adalah mengganti variabel dengan *placeholder* generik, seperti pada Listing 2. Sampel yang diaugmentasi ini seringkali:
1.  **Mengganggu Konsistensi Semantik (SECON):** Menghasilkan sampel yang *spurious* atau tidak realistis (seperti templat yang tidak dapat dieksekusi), yang menghambat pemahaman model.
2.  **Memperkuat Bias Dataset:** Secara tidak sengaja memperkuat bias yang ada dalam data.
3.  **Memerlukan Biaya Tinggi:** Membutuhkan *pre-processing* tambahan dan meningkatkan biaya pelatihan.

::: tip Solusi yang Diusulkan
Kami memperkenalkan **SECON** (*Maintaining Semantic Consistency*), sebuah metodologi *Data Augmentation* inovatif yang beroperasi pada **tingkat representasi (*representation level*)**. SECON menggunakan **pembekuan lapisan (*layer freezing*)** pada *pre-trained model* untuk menghasilkan sampel positif dengan Konsistensi Semantik yang tinggi, dan mengintegrasikan **mekanisme interaksi *self-attention*** untuk meningkatkan keselarasan semantik antara kode dan kueri dalam sampel augmentasi. Ini menghindari modifikasi data mentah dan mengurangi biaya komputasi.
:::

## 2. Metodologi

Metodologi SECON beroperasi dalam empat bagian utama: *Query and Code Embedding*, *Representation Augmentation Method*, *Interaction Mechanism*, dan *Optimization Objective*.

### A. Query and Code Embeddings

Proses ini mengubah pasangan kueri-kode $(q_i, c_i)$ menjadi representasi vektor dalam ruang *embedding* terpadu menggunakan PTM yang sudah mapan (misalnya, CoCoSoDa, UniXcoder).
1.  **Tokenisasi dan Embedding:** Kueri dan kode di-*tokenize* dan dipetakan ke *word embedding* ($E_q, E_c$).
2.  **Contextualisasi:** *Embedding* dikontekstualisasi menggunakan *transformer encoder blocks* untuk menghasilkan representasi kontekstual ($q_i, c_i$).

$$q_{i}=\text{transformer\_block}(E_{q}+P_{q})$$
$$c_{i}=\text{transformer\_block}(E_{c}+P_{c})$$

### B. Representation Augmentation Method

SECON mencapai *data augmentation* pada tingkat representasi dengan membekukan lapisan tertentu dari PTM, yang disebut *Encoder***. Strategi ini didasarkan pada asumsi bahwa *Encoder*** yang tidak diperbarui selama *fine-tuning* akan mempertahankan fitur linguistik kaya yang dipelajari selama *pre-training* (SECON) dan menghasilkan sampel positif yang stabil.
$$q_{i}^{*}=\text{Encoder*}(q_{i})$$
$$c_{i}^{*}=\text{Encoder*}(c_{i})$$
Pembekuan lapisan menghindari komputasi *gradient* dan penghematan sumber daya komputasi. Strategi *full layer frozen* dipilih untuk konstruksi sampel augmentasi.

### C. Interaction Mechanism

Mekanisme ini, yang terinspirasi oleh arsitektur *Poly-encoder*, bertujuan untuk memperkaya interaksi semantik antara kode dan kueri dalam sampel augmentasi.

1.  **Ekstraksi Fitur Global Kode:** Kode ($c$) direpresentasikan sebagai satu set $d$ vektor global $(y_{code}^{1},...,y_{code}^{d})$, di mana setiap vektor diekstraksi melalui *self-attention* menggunakan *konteks codes* $(c_1,...,c_d)$ yang dipelajari.

2.  **Augmentasi Interaksi Kode:** Representasi kode yang diaugmentasi ($c'$) diperoleh dengan menggabungkan vektor global kode, dibobot berdasarkan korelasi (*inner product*) dengan *query embedding* yang dibekukan ($q_i^*$).
$$c'=\sum_{r}w_{r}y_{code}^{r}$$
Di mana bobot $w_r$ ditentukan oleh:
$$\text{softmax}(q_{i}^{*}\cdot y_{code}^{1},...,q_{i}^{*}\cdot y_{code}^{d})$$

3.  **Augmentasi Interaksi Kueri:** Representasi kueri yang diaugmentasi ($q'$) diperoleh melalui prosedur paralel.

### D. Optimization Objective

SECON menggunakan *Contrastive Learning* untuk meminimalkan divergensi antara pasangan positif dan memaksimalkan jarak dari pasangan negatif. Kerangka umum *triplet loss* digunakan:
$$L_{triplet}=\sum_{i=1}^{N}\{G_{Pos}S_{ii}+\sum(G_{Neg}S_{ij,i\ne j})+\lambda\}$$
Di mana $S_{ii}$ adalah skor kesamaan pasangan positif $(q_i, c_i)$, dan $S_{ij}$ adalah skor kesamaan pasangan negatif. $G_{Pos}$ dan $G_{Neg}$ adalah bobot polinomial yang memungkinkan model untuk secara selektif mempertahankan pasangan informatif dari set yang berlebihan (*redundant sets*). Pasangan positif dalam pelatihan SECON meliputi: $(q_i, c_i)$, $(q_i, c_i')$, dan $(q_i', c_i)$.

## 3. Detail Pengujian

### Dataset
Dataset yang digunakan adalah **CSN** (*CodeSearchNet*) yang mencakup enam bahasa pemrograman: Ruby, JavaScript, Go, Python, Java, dan PHP.

| Language | Training | Validation | Test | Candidate Code |
| :--- | :--- | :--- | :--- | :--- |
| **Ruby** | 24,927 | 1,400 | 1,261 | 4,360 |
| **Python** | 251,820 | 13,914 | 14,918 | 43,827 |
| **Java** | 164,923 | 5,183 | 10,955 | 40,347 |
| **PHP** | 241,241 | 12,982 | 14,014 | 52,660 |

### Baseline
Model dibandingkan dengan model *code search* dari tiga kategori:
*   **IR-based:** BOW, TF-IDF, Jaccard.
*   **DL-based:** NBow, CNN, BiRNN, SelfAtt.
*   **PTM-based:** RoBERTa, CodeBERT, GraphCodeBERT, CoCoSoDa, UniXcoder, CPLCS, dll.

### Metrik Evaluasi
Metrik utama untuk mengevaluasi kinerja sistem *code search* adalah **Mean Reciprocal Rank (MRR)**.
$$MRR = \frac{1}{Q} \sum_{i=1}^{Q} \frac{1}{rank_{i}}$$
Di mana $Q$ adalah jumlah kueri, dan $rank_{i}$ adalah posisi peringkat item pertama dalam hasil *ground-truth* untuk kueri ke-$i$.

## 4. Hasil Eksperimen

### RQ3: Efektivitas SECON dalam Code Search
SECON secara signifikan mengungguli model *code search* SOTA lainnya pada dataset CSN.

| Model | Ruby | Javascript | Go | Python | Java | PHP | Overall |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| CoCoSoDa | 81.8 | 76.4 | 92.1 | 75.7 | 76.3 | 70.3 | 78.8 |
| CPLCS | 81.2 | 75.7 | 92.8 | 76.4 | 76.0 | 71.5 | 78.9 |
| **SECON** | 83.0 | 78.5 | 92.7 | 76.9 | 77.1 | 72.0 | 80.0 |
| **SECON with GCBS** | **83.6** | **79.6** | **92.7** | **78.3** | **79.1** | **73.3** | **81.1** |

**Analisis:** Peningkatan keseluruhan 2.3 poin MRR (81.1 vs 78.8) dicapai oleh SECON dengan GCBS (sebuah metode untuk *hard negative mining*). Ini menunjukkan bahwa augmentasi representasi yang menjaga Konsistensi Semantik, dikombinasikan dengan fokus pada pasangan negatif yang menantang, adalah strategi yang sangat efektif.

### RQ2: Dampak Augmentasi pada Lapisan Berbeda
Eksperimen menunjukkan bahwa membekukan lapisan model yang berbeda menghasilkan kinerja yang bervariasi.

| Trainable Layer | Code-side Aug. | Query-side Aug. | SECON |
| :--- | :--- | :--- | :--- |
| Full layer frozen | 80.6 | 80.7 | 80.8 |
| Layer 1 trainable | 81.1 | 80.8 | 80.9 |
| Layer 8 trainable | 80.7 | 80.9 | 80.7 |
| Layer 11 trainable | 81.1 | 80.8 | 81.3 |

**Analisis:** Meskipun *Layer 11 trainable* menunjukkan MRR tertinggi (81.3), strategi **Full Layer Frozen** dipilih untuk DA SECON (80.8) karena menghasilkan representasi yang paling stabil dan konsisten secara semantik, serta mengurangi beban komputasi secara signifikan.

### RQ4: Efek Augmentasi Sisi Kode vs Sisi Kueri
Ketika Augmentasi Sisi Kode dan Sisi Kueri dibandingkan (dengan model CoCoSoDa sebagai *baseline*):

| Model Ablation | Ruby | Javascript | Go | Python | Java | PHP | Overall |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| SECON (full layer frozen) | 83.0 | 78.5 | 92.7 | 76.9 | 77.1 | 72.0 | 80.0 |
| **dengan code-side augmentation** | **83.2** | **78.3** | **92.6** | **76.8** | **77.2** | **71.9** | **80.0** |
| dengan query-side augmentation | 82.7 | 78.3 | 92.7 | 76.4 | 77.0 | 71.6 | 79.8 |

**Analisis:** Augmentasi yang diterapkan pada **sisi kode** menunjukkan hasil yang sedikit lebih menonjol atau setara. Hal ini didiskusikan karena kode cenderung lebih panjang dan sarat dengan informasi yang berlebihan (*redundant*). *Code-side augmentation* memungkinkan setiap komponen vektor kode untuk berinteraksi secara mendalam dengan *keseluruhan* vektor kueri. Sebaliknya, *query-side augmentation* memaksa setiap komponen kueri untuk berinteraksi dengan *keseluruhan* vektor kode yang *redundant*, yang menghambat fusi fitur yang efektif.

## 5. Kesimpulan

Studi ini berhasil mengidentifikasi dan mengatasi masalah Konsistensi Semantik dalam *Data Augmentation* untuk *code search* PTM melalui metodologi inovatif **SECON**. SECON, yang beroperasi pada tingkat representasi menggunakan *network freezing* dan mekanisme interaksi *self-attention*, terbukti secara empiris lebih unggul daripada metode *data augmentation* tradisional dan model SOTA. SECON menetapkan tolok ukur kinerja baru di MRR keseluruhan **81.1** (dengan GCBS). Temuan ini menegaskan pentingnya mempertahankan stabilitas semantik melalui DA tingkat representasi dan menunjukkan bahwa augmentasi sisi kode lebih efektif dalam mengatasi redudansi bawaan kode.

::: info Dampak Praktis
SECON menyediakan kerangka kerja yang **efisien secara komputasi** dan **andal** untuk *Data Augmentation* dalam *code search*. Ini membantu pengembang menemukan fungsionalitas kode yang tepat lebih cepat, karena model mampu memahami niat kueri dan kode secara lebih koheren, terlepas dari redudansi leksikal. Penelitian masa depan akan difokuskan untuk meningkatkan interpretasi hasil pencarian, menjembatani kesenjangan antara masukan kueri yang kompleks dan ekspektasi pengguna.
:::