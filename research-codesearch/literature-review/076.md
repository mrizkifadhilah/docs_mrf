---
title: Review Paper - Pendekatan Code Search Cerdas dengan Hybrid Encoders (RETRACTED)
description: Rangkuman teknis dari artikel yang ditarik kembali mengenai model At-CodeSM untuk pencarian kode menggunakan Hybrid Encoders (Wireless Communications and Mobile Computing, 2021).
head:
  - - meta
    - name: keywords
      content: Code Search, Deep Learning, Hybrid Encoders, Tree-LSTM, Self-Attention, Abstract Syntax Tree, At-CodeSM
---

# 076 - An Intelligent Code Search Approach Using Hybrid Encoders (RETRACTED)
Tautan (DOI) [https://doi.org/10.1155/2021/9990988]

**Penulis:** **Yao Meng** $^{\text{a)*}}$

**Afiliasi:**
* $^{\text{a)}$ State Key Laboratory of Mathematical Engineering and Advanced Computing, Zhengzhou 450001, China

**Kronologi (Artikel 2021):** Received: 23 Maret 2021 • Revised: 2 Juni 2021 • Accepted: 18 Juni 2021 • Published: 1 Juli 2021
**Kronologi (Penarikan 2023):** Received: 28 November 2023 • Accepted: 28 November 2023 • Published: 29 November 2023

<a href="https://www.scimagojr.com/journalsearch.php?q=17543&tip=sid" target="_blank"><img src="https://www.scimagojr.com/journal_img.php?id=17543" alt="SCImago Journal & Country Rank" /></a>

| Resume Eksekutif |
| :--- |
| **Publikasi:**<br>• **Jurnal:** Wireless Communications and Mobile Computing, Vol. 2021, Article ID 9990988 (16 pages), 2021.<br>• **STATUS:** **DITARIK KEMBALI** oleh Hindawi/Wiley pada 29 November 2023 karena temuan manipulasi sistematis terhadap proses publikasi dan *peer-review*.<br>• **Topik:** Mengusulkan model *code search* berbasis *deep learning* baru yang disebut At-CodeSM untuk meningkatkan kinerja dengan menggabungkan fitur leksikal dan struktural kode.<br><br>**Masalah & Solusi:**<br>• **Masalah:** Algoritma *code search* tradisional berbasis *Information Retrieval* (IR) sering gagal karena mengabaikan informasi struktural dan semantik yang mendalam dalam kode, serta heterogenitas antara kueri bahasa alami dan kode sumber.<br>• **Solusi:** Mengusulkan **At-CodeSM** (*Attention-based Code Search Model*), sebuah kerangka kerja *deep learning* yang menggunakan *Hybrid Encoders* (gabungan tiga *encoder* independen) untuk memvektorisasi kode: **Lexical Encoder** (dengan *Self-Attention*), **Method Name Encoder** (Bi-LSTM), dan **Structural Encoder** (Tree-LSTM berbasis AST).<br><br>**Contoh Penerapan:**<br>• Diuji pada *dataset* skala besar **CodeSearchNet** (subset Java) dan **CodeSearchNet Challenge** (untuk evaluasi manual).<br><br>**Metodologi:**<br>• **Lexical Encoder:** Menggunakan **Bi-LSTM** yang diperkuat dengan lapisan **Self-Attention** untuk memberi bobot lebih tinggi pada token inti (kata kunci) kode. Ini adalah klaim pertama pengenalan *self-attention* dalam *code search* (meskipun ini terbantahkan oleh penelitian lain yang lebih dulu).<br>• **Structural Encoder:** Mengubah kode menjadi **Abstract Syntax Tree (AST)**, kemudian menggunakan algoritma **Tree-LSTM** (yang dimodifikasi untuk pohon biner) untuk menangkap fitur struktural secara rekursif.<br>• **Fusion Layer:** Menggabungkan vektor output dari ketiga *encoder* ($V_{name}, V_{token}, V_{ast}$) menjadi vektor kode terpadu $v_c$ melalui lapisan *fully connected*.<br>• **Loss Function:** Menggunakan fungsi **Ranking Loss** untuk meminimalkan jarak kosinus antara vektor kode dan deskripsi positif ($D^+$) sambil memaksimalkan jarak dari deskripsi negatif ($D^-$).<br><br>**Temuan Kunci (Klaim dalam Artikel):**<br>1. **Kinerja Unggul:** At-CodeSM diklaim mengungguli model *baseline* seperti NBoW, NCS, dan CODEnn (DeepCS) dalam metrik MRR dan NDCG pada *dataset* CodeSearchNet.<br>2. **Pentingnya Struktural:** Penambahan *Structural Encoder* (Tree-LSTM pada AST) meningkatkan kinerja secara signifikan dibandingkan model yang hanya mengandalkan fitur leksikal.<br>3. **Pentingnya Self-Attention:** Lapisan *Self-Attention* pada *Lexical Encoder* terbukti sangat meningkatkan kinerja karena memperkuat kontribusi kata-kata inti kode.<br><br>**Kontribusi Utama (Klaim dalam Artikel):**<br>• Mengusulkan At-CodeSM, model representasi kode baru dengan tiga *encoder* independen yang mempertahankan karakteristik leksikal dan semantik/sintaksis kode.<br>• Mengklaim sebagai yang pertama memperkenalkan mekanisme *Self-Attention* ke dalam *code search*.<br>• Mendemonstrasikan kinerja superior pada *dataset* CodeSearchNet.<br><br>**Dampak:**<br>• Jika hasil klaimnya valid, At-CodeSM akan menjadi model *code search* berbasis *deep learning* yang sangat efektif dengan memanfaatkan fitur hibrida leksikal dan struktural. **Namun, karena penarikan resmi, validitas temuan ini diragukan.** |

## 1. Pendahuluan & Masalah

Pencarian kode yang cerdas menggunakan kueri bahasa alami merupakan bidang penelitian yang penting dalam rekayasa perangkat lunak. Programmers sering mencari dan menggunakan kembali potongan kode untuk meningkatkan kualitas dan efisiensi. Metode pencarian kode tradisional yang mengandalkan strategi *Information Retrieval* (IR) berbasis token atau teks seringkali tidak memadai karena kode memiliki informasi **struktural dan semantik** yang kompleks yang diabaikan oleh metode tersebut. Selain itu, terdapat **heterogenitas** mendasar antara kueri bahasa alami dan kode sumber.

Model pembelajaran mendalam (*deep learning*) telah terbukti mampu mengekstrak fitur tersembunyi ini, tetapi ada kebutuhan untuk model yang secara efektif menggabungkan berbagai jenis informasi dalam kode.

::: tip Solusi yang Diusulkan
At-CodeSM diusulkan sebagai kerangka kerja *deep learning* baru untuk *code search*. Model ini menggunakan **tiga *encoder* independen** untuk memvektorisasi kode: *Lexical Encoder* (token, diperkuat dengan *Self-Attention*), *Method Name Encoder* (nama fungsi), dan *Structural Encoder* (berbasis Abstract Syntax Tree/AST dan Tree-LSTM). Kombinasi ini bertujuan untuk mempertahankan karakteristik leksikal dan sintaksis kode secara efektif.
:::

## 2. Metodologi

At-CodeSM adalah model kombinasi yang terdiri dari *Source Code Encoder* ($E_c$) dan *Query Encoder* ($E_q$), yang memetakan kode dan kueri ke ruang vektor yang sama untuk perbandingan kesamaan kosinus.

### A. Source Code Encoder ($E_c$)

$E_c$ adalah model gabungan dari tiga *encoder* independen:

1.  **Lexical Encoder:**
    *   Memproses token inti dari badan metode (setelah menghilangkan kata duplikat, *stop words*, dan *keyword* Java).
    *   Token disematkan menggunakan **Word2vec**.
    *   Representasi sekuensial diperoleh dengan **Bi-LSTM**.
    *   Lapisan **Self-Attention** diterapkan pada output Bi-LSTM untuk menghasilkan vektor representasi $V_{token}$. Mekanisme ini bertujuan untuk memperkuat kata-kata inti dalam kode.
    $$\alpha_{i} = \frac{\exp({h_i}^T K)}{\sum_{j=1}^{n} \exp({h_j}^T K)}$$
    $$V_{token} = \sum \alpha_i h_i$$
    Di mana $h_i$ adalah status tersembunyi Bi-LSTM dan $K$ adalah vektor konteks yang dapat dipelajari.

2.  **Method Name Encoder:**
    *   Memproses kata kunci dari nama metode.
    *   Menggunakan **Bi-LSTM** untuk menghasilkan vektor representasi $V_{name}$ dari urutan token nama metode.

3.  **Structural Encoder:**
    *   Menggunakan `javalang` untuk menghasilkan **Abstract Syntax Tree (AST)**.
    *   AST diubah menjadi **pohon biner** untuk mengatasi variasi jumlah anak, mempermudah pembagian parameter matriks bobot.
    *   Menggunakan **Tree-LSTM** untuk secara rekursif menghitung status tersembunyi dari node daun ke node akar, menghasilkan vektor struktural $V_{ast}$.

4.  **Fusion Layer:**
    *   Vektor output dari ketiga *encoder* ($V_{name}, V_{token}, V_{ast}$) digabungkan menjadi vektor kode terpadu $v_c$ melalui lapisan *fully connected*.

### B. Query Encoder ($E_q$)

*   Memproses urutan kata dalam kueri bahasa alami.
*   Token disematkan (Word2vec) dan diproses menggunakan **Bi-LSTM** untuk menghasilkan vektor kueri akhir $v_{query}$.

### C. Loss Function (Ranking Loss)

Model dilatih menggunakan *supervised learning* dengan meminimalkan **Ranking Loss** pada triple $\langle C, D^+, D^- \rangle$ (Kode, Deskripsi Positif, Deskripsi Negatif).
$$L(\theta) = \sum_{|C, D^+, D^-| \in P} \max(0, \epsilon - \cos(c, d^+) + \cos(c, d^-))$$
Di mana $\epsilon$ adalah margin konstan, $c$ adalah vektor kode, $d^+$ adalah vektor deskripsi positif, dan $d^-$ adalah vektor deskripsi negatif.

## 3. Detail Pengujian

### Dataset
1.  **CodeSearchNet (CSN):** *Dataset* Java skala besar yang berisi lebih dari 542.991 pasangan fungsi-deskripsi (dari *docstring*) untuk pelatihan dan pengujian otomatis (26.909 pasangan).
2.  **CodeSearchNet Challenge (CSC):** *Dataset* kecil (99 kueri) yang memerlukan penilaian manual oleh ahli untuk mengevaluasi NDCG (memberikan skor relevansi 0-3).

### Baseline Model
1.  **NBoW (Neural Bag of Words):** Model dasar IR/NLP yang menggunakan *max-pooling* atas vektor token.
2.  **NCS (Neural Code Search):** Menggabungkan *embedding* token (*fastText*) dengan teknik IR (TF-IDF), dilatih secara *unsupervised*.
3.  **CODEnn (DeepCS):** Model SOTA berbasis *deep learning* (LSTM) yang juga menggunakan tiga *encoder* (Nama Metode, API, Token) tetapi tanpa Tree-LSTM atau *Self-Attention* pada token leksikal.

### Metrik Evaluasi
1.  **MRR (Mean Reciprocal Rank):** Mengukur peringkat hasil relevan pertama. Digunakan untuk evaluasi otomatis pada CSN.
    $$\text{MRR}=\frac{1}{|Q|}\sum_{i=1}^{|Q|}\frac{1}{\text{Index}_{Q_{i}}}$$
2.  **NDCG (Normalized Discounted Cumulative Gain):** Mengukur kualitas daftar hasil, memberi bobot lebih besar pada hasil yang relevan di peringkat teratas. Digunakan untuk evaluasi manual pada CSC.

## 4. Hasil Eksperimen

### A. Perbandingan Kinerja Utama (MRR dan NDCG)

| Model | MRR (Test A, Otomatis) | NDCG (Test B, Manual) |
| :--- | :--- | :--- |
| NBoW | 40.91% | 0.2011 |
| NCS | 41.03% | 0.2563 |
| CODEnn | 49.22% | 0.4635 |
| **At-CodeSM** | **53.37%** | **0.4918** |

*   **Klaim At-CodeSM:** Model ini diklaim mengungguli semua *baseline*, mencapai MRR 53.37% dan NDCG 0.4918. Kinerja ini dikaitkan dengan penangkapan fitur leksikal dan struktural secara komprehensif.

### B. Diskusi Ablasi (AST Traversal Algorithms)

| Descriptions | MRR | NDCG |
| :--- | :--- | :--- |
| Without AST (Model 1) | 46.45% | 0.4149 |
| SBT (Model 2) | 51.03% | 0.4683 |
| **Tree-LSTM (Model 3/At-CodeSM)** | **53.37%** | **0.4918** |

*   **Pentingnya Struktural:** Model yang mengabaikan AST (Model 1) menunjukkan kinerja terburuk. Penggunaan **Tree-LSTM** (Model 3) pada AST menghasilkan kinerja terbaik, mengkonfirmasi bahwa *encoder* struktural adalah komponen kunci.

### C. Diskusi Ablasi (Self-Attention)

| Model | MRR | NDCG |
| :--- | :--- | :--- |
| NoAtt (Tanpa Attention) | 41.97% | 0.3687 |
| MNAtt (Attention di Nama Metode) | 44.40% | 0.3980 |
| StrAtt (Attention di AST) | 53.07% | 0.4851 |
| **At-CodeSM (Attention di Lexical)** | **53.37%** | **0.4918** |
| At-CodeSM2 (Attention tanpa parameter K) | 51.64% | 0.4735 |

*   **Pentingnya Self-Attention:** Model tanpa *attention* (NoAtt) tertinggal jauh. Kinerja tertinggi dicapai ketika *Self-Attention* diterapkan pada *Lexical Encoder* (token), yang menunjukkan bahwa mekanisme ini efektif dalam memperkuat kata-kata inti yang mengimplementasikan fungsionalitas.

## 5. Kesimpulan

At-CodeSM diklaim sebagai model *code search* yang efektif dengan memanfaatkan *hybrid encoders* (Lexical, Method Name, Structural) dan mekanisme *Self-Attention*. Klaim utama adalah bahwa model ini berhasil menangkap informasi semantik dan sintaksis yang kaya dalam kode. Hasil eksperimen (yang kini **diragukan** karena penarikan artikel) menunjukkan kinerja superior dibandingkan *baseline* SOTA pada *dataset* CodeSearchNet.

::: info Dampak Praktis (Berdasarkan Klaim)
Jika klaimnya benar, At-CodeSM akan memberikan alat *code search* berbasis *deep learning* yang sangat akurat, secara efektif membantu pengembang dalam menemukan kode yang relevan dengan cepat. Fokus pada informasi struktural (AST) dan mekanisme *attention* pada token inti menunjukkan jalur yang menjanjikan untuk mengatasi kesenjangan semantik antara NL dan kode. **(Penting: Pembaca diwajibkan untuk memperhatikan status RETRACTED dari artikel ini.)**
:::