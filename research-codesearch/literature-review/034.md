---
title: Review Paper - TCS Pencarian Kode Berbasis Transformer
description: Rangkuman paper tentang pencarian kode berbasis Transformer untuk situs Q&A perangkat lunak (Journal of Software Evolution and Process, 2024).
head:
  - - meta
    - name: keywords
      content: aligned attention, code search, neural network, structural code information, transformer, software Q&A sites
---

# 034 - Transformer-based code search for software Q&A sites
Tautan (DOI) [10.1002/smr.2517](https://doi.org/10.1002/smr.2517)

**Penulis:** **Yaohui Peng** $^{a}$, **Jing Xie** $^{a}$, **Gang Hu** $^{a}$, **Mengting Yuan** $^{a}$*

**Afiliasi:**
* $^a$ School of Computer Science, Wuhan University, Hubei, China

**Kronologi:** Received: 15 December 2021 • Revised: 29 August 2022 • Accepted: 26 September 2022 • Available Online: October 2022 (As per document; *Journal of Software* 36(2), 2024)

<a href="https://www.scimagojr.com/journalsearch.php?q=21100205112&tip=sid" target="_blank"><img src="https://www.scimagojr.com/journal_img.php?id=21100205112" alt="SCImago Journal & Country Rank" /></a>

| Resume Eksekutif |
| :--- |
| **Publikasi:**<br>• **Jurnal:** Journal of Software: Evolution and Process, Vol. 36, Issue 2, e2517 (2024)<br>• **Topik:** Meningkatkan pencarian kode sumber (*code search*) di situs Tanya Jawab (Q&A) perangkat lunak dengan menangkap informasi struktural kode dan interaktivitasnya dengan kueri bahasa alami (NL).<br><br>**Masalah & Solusi:**<br>• **Masalah 1 (Mengabaikan Struktur Kode):** Pendekatan pencarian kode sebelumnya (terutama yang berbasis RNN/LSTM) langsung memperlakukan kode sumber sebagai urutan teks sederhana, mengabaikan informasi **sintaksis dan struktural** yang penting di dalamnya. Sementara yang mempertimbangkan struktur (seperti yang berbasis *pre-training* atau *multi-stage*) cenderung terlalu kompleks dan tidak praktis (*end-to-end*).<br>• **Masalah 2 (Kurangnya Interaktivitas):** Model sebelumnya gagal menangkap hubungan interaktif (*interactivity*) yang akurat antara kueri bahasa alami dan kode sumber yang bersangkutan, terutama karena kueri di situs Q&A sering kali mengandung ***noise* dan tidak spesifik**.<br>• **Solusi:** Mengusulkan **TCS** (*Transformer-based Code Search*), model jaringan saraf dalam novel yang merupakan model **end-to-end** pertama yang: (1) Menggunakan arsitektur **Transformer** (dengan *Multi-Head Attention*) untuk mengekstrak fitur dan **memasukkan struktur kode** secara integral ke dalam representasi vektor. (2) Menggunakan **Aligned/Joint Attention Matrix** untuk menangkap korelasi interaktif yang mendalam antara kode dan kueri (QC pairs).<br><br>**Contoh Penerapan:**<br>• Dievaluasi pada dua *dataset* buatan sendiri dari StackOverflow: **MucQC** (kueri satu-ke-banyak, lebih menantang) dan **SicQC** (kueri satu-ke-satu). *Dataset* mencakup bahasa **SQL, Java, dan Python**.<br><br>**Metodologi:**<br>• **Arsitektur:** Terdiri dari *Language Embedding*, *Transformer Encoding*, *Joint Attention*, dan *Similarity Calculation Module*.<br>• **Transformer Encoding:** Menggunakan *Multi-Head Attention* dan *Feed Forward Network* (FFN) untuk menghasilkan representasi vektor yang menangkap hubungan struktural internal dalam kueri dan kode, terlepas dari arah atau jarak kata.<br>• **Joint Attention Module:** Membuat **Matriks Interaksi Atensi** antara $Q_{query}$ dan $K_{code}$ (dan sebaliknya) untuk memodelkan hubungan korespondensi yang akurat antara kode dan kueri. Ini menghasilkan vektor atensi interaktif $V_Q$ dan $V_C$.<br>• **Fungsi Rugi & Kesamaan:** Menggunakan **Triplet Loss** ($L$) selama pelatihan untuk mendekatkan vektor kueri $V_Q$ dengan sampel kode positif $V_C^+$ dan menjauhkannya dari sampel negatif $V_C^-$. Selama inferensi, **Cosine Similarity** digunakan untuk pengukuran akhir.<br><br>**Temuan Kunci:**<br>1. **Kinerja SOTA:** TCS mencapai kinerja *state-of-the-art* (SOTA) di hampir semua *benchmark* (MucQC dan SicQC), khususnya mengungguli model-model SOTA sebelumnya seperti NJACS di sebagian besar metrik.<br>2. **Stabilitas Lintas-Bahasa:** Kinerja TCS sangat stabil di berbagai bahasa (SQL, Java, Python) berkat sifat universal *Transformer* dalam mengekstrak fitur, berbeda dengan beberapa *baseline* yang memiliki ketergantungan bahasa (misalnya, NJACS yang didesain khusus untuk bahasa terstruktur tinggi).<br>3. **Keunggulan Transformer:** Penggunaan *Transformer* terbukti lebih efektif daripada mekanisme *attention* sederhana lainnya (*CodeATT, SelfATT*) dan arsitektur non-*Transformer* SOTA (*NJACS*) dalam menangkap informasi struktural kode.<br>4. **Peningkatan MucQC:** TCS menunjukkan peningkatan terbesar pada *dataset* **MucQC** (lebih menantang), dengan peningkatan Recall@1 hingga 22.7% (SQL) dan 29.00% (Python) dibandingkan *baseline* terburuk.<br><br>**Kontribusi Utama:**<br>• Model **TCS** novel, jaringan saraf *end-to-end* berbasis *Transformer* pertama yang mengintegrasikan penangkapan struktur kode dan *joint attention* untuk *code search*.<br>• Demonstrasi bahwa *Multi-Head Attention* *Transformer* secara efektif mengekstrak fitur struktural kode secara integral.<br>• Validasi empiris TCS pada *dataset* Q&A nyata (StackOverflow) untuk tiga bahasa pemrograman berbeda.<br><br>**Dampak:**<br>• TCS menawarkan metode yang lebih akurat dan praktis untuk pencarian kode di situs Q&A, yang secara langsung **meningkatkan produktivitas pengembang** dengan mengurangi waktu dan kesalahan dalam menemukan kode yang dapat digunakan kembali. Model ini berpotensi diterapkan pada tugas lain yang bertujuan menangkap kesamaan semantik, seperti rekomendasi kode. |

## 1. Pendahuluan & Masalah

Contoh kode sumber sangat penting dalam pengembangan perangkat lunak untuk pemahaman konsep dan penggunaan kembali. Situs Tanya Jawab (Q&A) perangkat lunak mengandung banyak contoh solusi kode dengan deskripsi bahasa alami (NL), yang berharga untuk pencarian kode. Peningkatan efisiensi pencarian kode di situs Q&A adalah masalah kunci yang perlu dieksplorasi.

Studi sebelumnya mengenai pencarian kode sering memperlakukan kode sumber sebagai urutan teks sederhana, mengabaikan informasi **sintaksis dan struktural** di luar representasi tekstual. Meskipun ada upaya yang mempertimbangkan struktur (seperti yang berbasis *pre-training* atau *multi-stage*), strategi ini cenderung rumit, memakan waktu, dan tidak praktis sebagai model *end-to-end*.

Selain itu, kualitas *dataset* pencocokan sangat memengaruhi hasil. Kueri di situs Q&A seringkali mengandung *noise*, dan model yang ada berjuang untuk secara akurat menangkap hubungan **interaktif** antara kueri NL dan kode sumber. Kebutuhan untuk menemukan pendekatan umum yang bekerja dengan baik untuk berbagai bahasa pemrograman juga merupakan tantangan besar.

::: tip Solusi yang Diusulkan
Paper ini mengusulkan **TCS** (*Transformer-based Code Search*), model jaringan saraf dalam *end-to-end* novel yang memanfaatkan mekanisme **Multi-Head Attention** pada arsitektur **Transformer** untuk secara integral mengekstrak informasi struktural dan semantik dari kode dan kueri. Selain itu, TCS menggunakan **Joint Attention Module** untuk menangkap korelasi interaktif antara kode dan kueri (QC pairs) secara lebih akurat, memastikan kinerja yang stabil di berbagai bahasa pemrograman.
:::

## 2. Metodologi

TCS adalah model *end-to-end* yang terdiri dari empat modul: *Language Embedding*, *Transformer Encoding*, *Joint Attention*, dan *Similarity Calculation Module*.

### A. Language Embedding Module

Modul ini bertanggung jawab untuk mengubah kode dan kueri menjadi fitur informatif. TCS menggunakan model SGNS (*Skip-gram with Negative Sampling*) dari FastText untuk mempelajari *embedding* kata secara terpisah untuk kode dan kueri. Representasi kata-kata ini kemudian dicari dalam kamus *embedding* (*lookup*) untuk menjadi masukan bagi lapisan berikutnya.

### B. Transformer Encoding Module

Modul ini menggunakan arsitektur Transformer untuk menangkap informasi struktural internal. Ia terdiri dari lapisan *Multi-Head Attention* dan *Feed Forward Network* (FFN).

1.  ***Multi-Head Attention*:** Mekanisme ini memetakan *Query* ($Q$), *Key* ($K$), dan *Value* ($V$) ke sub-ruang yang berbeda, memungkinkan model untuk fokus pada hubungan asosiasi dalam urutan dari berbagai sudut pandang.
    $$ x=\text{MultiHead}(Q,K,V)=\text{Concat}(\text{head}_1,...,\text{head}_h)W $$
    Di mana $\text{head}_i$ adalah *Scaled Dot-Product Attention*:
    $$ \text{head}_i = \text{softmax}\left(\frac{Q K^T}{\sqrt{d_k}}\right)V $$
2.  ***Feed Forward Network* (FFN):** Terdiri dari dua transformasi linear dengan aktivasi ReLU, FFN melakukan fusi berbobot dari fitur-fitur yang diekstrak.

Dengan Transformer, setiap kata dapat di-*encode* terlepas dari arah dan jarak, sehingga TCS efektif dalam mengekstrak fitur struktural dan semantik yang integral dari kode dan kueri.

### C. Joint Attention Module

Modul ini memodelkan hubungan korespondensi yang penting antara kueri dan kode. Ia memperkenalkan mekanisme *attention* antara keduanya, bukan hanya fokus pada representasi internal masing-masing. *Aligned/Joint Attention Matrix* dihitung untuk menghasilkan vektor atensi interaktif $V_Q$ dan $V_C$.
$$ V_Q=\text{softmax}\left(\frac{Q_{\text{query}}K_{\text{code}}^T}{\sqrt{d_k}}\right)V_{\text{query}} $$
$$ V_C=\text{softmax}\left(\frac{Q_{\text{code}}K_{\text{query}}^T}{\sqrt{d_k}}\right)V_{\text{code}} $$
Matriks ini menangkap korelasi fitur, di mana nilai yang lebih besar menunjukkan relevansi yang lebih tinggi antara kata kode yang bersangkutan dan kueri.

### D. Similarity Calculation Module

1.  **Pelatihan (Training):** Tugas *code search* diubah menjadi tugas auto-regresif dengan menggunakan sampel positif ($V_C^+$) dan negatif ($V_C^-$) dari kode. **Triplet Loss** digunakan sebagai fungsi rugi untuk meminimalkan jarak Euclidean $d$ antara $V_Q$ dan $V_C^+$, serta memaksimalkan jarak dengan $V_C^-$.
    $$ L=\max(d(V_Q,V_C^+)-d(V_Q,V_C^-)+\text{margin}, 0) $$
2.  **Inferensi (Inference):** **Cosine Similarity** digunakan untuk mengukur korelasi semantik antara vektor fitur kueri ($V_Q$) dan kode ($V_C$).
    $$ \cos(V_Q,V_C)=\frac{V_Q^T V_C}{|V_Q||V_C|} $$

## 3. Detail Pengujian

### Dataset
Data dikumpulkan dari Stack Overflow dan dibagi menjadi dua korpus:
*   **MucQC (Multiple Query Code):** Pasangan kueri satu-ke-banyak, lebih menantang.
*   **SicQC (Single Query Code):** Pasangan kueri satu-ke-satu.
Kedua korpus mencakup data untuk bahasa **SQL, Java, dan Python**. Rasio sampel positif sekitar 33.33% dalam set pengujian.

### Baseline
Model TCS dibandingkan dengan model-model yang representatif, termasuk:
*   **Keyword-based:** QECK, $QECK_{CodeMF}$.
*   **Embedding/RNN-based:** NCS, UNIF, DeepCS, CodeLSTM, CodeCNN, CodeRCNN.
*   **Attention/Joint Attention-based:** SelfATT, CodeATT, NJACS.

### Metrik Evaluasi
Dua metrik umum untuk masalah *ranking* digunakan:

*   **Recall@1:** Mengukur persentase kueri di mana fragmen kode yang relevan diperingkat pada posisi #1.
    $$ \text{Recall}@1=\frac{1}{R}\sum_{r=1}^{R}\delta(\text{Rank}_{C(r)}\le1) $$
*   **Mean Reciprocal Rank (MRR):** Rata-rata dari kebalikan peringkat hasil yang cocok. MRR yang lebih tinggi menunjukkan bahwa hasil yang akurat berada di dekat bagian atas urutan pencocokan.
    $$ \text{MRR}=\frac{1}{R}\sum_{r=1}^{R}\frac{1}{\text{Rank}_{C(r)}} $$

## 4. Hasil Eksperimen

### RQ1: Kinerja TCS vs. Model SOTA

Hasil membandingkan kinerja TCS dengan *baseline* di set pengujian MucQC dan SicQC. Angka yang dicetak tebal menunjukkan hasil terbaik.

#### MucQC (Kueri satu-ke-banyak, lebih menantang)

| Model | SQL (Recall@1) | SQL (MRR) | Java (Recall@1) | Java (MRR) | Python (Recall@1) | Python (MRR) |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| SelfATT | 0.3750 | 0.5625 | 0.2750 | 0.4884 | 0.2150 | 0.4494 |
| NJACS | 0.3550 | 0.5620 | 0.2750 | 0.4804 | **0.2900** | **0.4812** |
| **TCS** | **0.3850** | **0.5637** | **0.2850** | **0.4938** | **0.2900** | 0.4779 |

*   TCS mencapai Recall@1 dan MRR terbaik di SQL dan Java.
*   Untuk Python, TCS mencapai Recall@1 terbaik (bersama NJACS) dan MRR-nya hanya 0.33% lebih rendah dari NJACS.
*   Peningkatan tertinggi dibandingkan model sebelumnya: 22.7% (Recall@1 SQL) dan 19.15% (MRR Java) dibandingkan *baseline* terburuk.

#### SicQC (Kueri satu-ke-satu)

| Model | SQL (Recall@1) | SQL (MRR) | Java (Recall@1) | Java (MRR) | Python (Recall@1) | Python (MRR) |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| CodeRCNN | 0.2100 | 0.4175 | 0.2150 | 0.4332 | 0.1500 | 0.3620 |
| NJACS | 0.1900 | **0.4348** | **0.2350** | **0.4655** | 0.2150 | 0.4128 |
| **TCS** | **0.2200** | 0.4265 | 0.2200 | 0.4284 | **0.2400** | **0.4417** |

*   TCS mencapai Recall@1 terbaik di SQL dan Python, serta MRR terbaik di Python.
*   Kinerja TCS pada Java SicQC kurang kompetitif (1.5% lebih rendah pada Recall@1 dan 3.7% lebih rendah pada MRR dibandingkan NJACS). Ini dikaitkan dengan kompleksitas struktur Java dan *noise* data.
*   Secara keseluruhan, TCS dapat disimpulkan mencapai kinerja SOTA di hampir semua *benchmark*.

### RQ2: Efek dari Pengenalan Transformer

*   **TCS vs. Attention Sederhana (CodeATT & SelfATT):** TCS secara konsisten mengungguli CodeATT dan SelfATT di hampir semua kasus. Ini mengkonfirmasi bahwa lapisan *self-attention* dan FFN yang berulang dalam arsitektur Transformer lebih efektif daripada mekanisme *attention* yang lebih sederhana.
*   **TCS vs. NJACS (Joint Attention non-Transformer):** TCS umumnya mengungguli NJACS (yang juga menggunakan *joint attention* tetapi dengan *structural embedding* terpisah) di MucQC. Performa yang lebih baik ini dikaitkan dengan kemampuan Transformer untuk mempelajari informasi struktural kode secara integral. Meskipun NJACS unggul dalam Java SicQC (kemungkinan karena *structural embedding* khususnya lebih efektif untuk bahasa yang sangat terstruktur), kinerja TCS secara keseluruhan lebih **seimbang** dan **serbaguna** di berbagai bahasa.

## 5. Kesimpulan

Paper ini mengusulkan **TCS**, sebuah jaringan saraf baru berbasis **Transformer** dan **Joint Attention** untuk *code search* di situs Q&A perangkat lunak. TCS adalah model *end-to-end* pertama yang secara efektif memanfaatkan Transformer untuk menangkap informasi struktural kode yang vital sekaligus memodelkan hubungan interaktif antara kode dan kueri.

Hasil eksperimen menunjukkan bahwa TCS mencapai kinerja *state-of-the-art* di hampir semua *benchmark* (MucQC dan SicQC) untuk bahasa SQL, Java, dan Python, sekaligus mempertahankan kinerja yang stabil di berbagai bahasa pemrograman. Keunggulan ini menegaskan bahwa pengenalan Transformer, terutama mekanisme *multi-head attention*-nya, secara signifikan meningkatkan kemampuan model untuk menemukan *snippet* kode yang paling relevan.

::: info Dampak Praktis
Modul *semantic feature embedding* dan *joint attention* TCS berpotensi diterapkan pada bidang lain yang bertujuan menangkap kesamaan semantik, seperti rekomendasi kode dan pencarian kontekstual. Dalam pekerjaan di masa mendatang, anotasi *dataset* secara manual akan dipertimbangkan untuk mengatasi masalah *imbalance* dan *noise* yang umum di situs Q&A, serta untuk lebih meningkatkan pembelajaran informasi struktural kode.
:::